{
  "chunk-fa46ef3acf35b900d9b09945f5135ac3": {
    "tokens": 31466,
    "content": "--- Page 1 ---\n\nLEGO-GraphRAG: Modularizing Graph-based\nRetrieval-Augmented Generation for Design Space Exploration\nYukun Cao\nUniversity of Science and Technology\nof China\nykcho@mail.ustc.edu.cn\nZengyi Gao\nUniversity of Science and Technology\nof China\ngzy02@mail.ustc.edu.cn\nZhiyang Li\nUniversity of Science and Technology\nof China\nlizhiyang215@gmail.com\nXike Xie\nUniversity of Science and Technology\nof China\nxkxie@ustc.edu.cn\nS. Kevin Zhou\nUniversity of Science and Technology\nof China\ns.kevin.zhou@gmail.com\nJianliang Xu\nHong Kong Baptist University\nxujl@comp.hkbu.edu.hk\nABSTRACT\nGraphRAG integrates (knowledge) graphs with large language mod-\nels (LLMs) to improve reasoning accuracy and contextual relevance.\nDespite its promising applications and strong relevance to multi-\nple research communities, such as databases and natural language\nprocessing, GraphRAG currently lacks modular workflow analysis,\nsystematic solution frameworks, and insightful empirical studies.\nTo bridge these gaps, we propose LEGO-GraphRAG, a modu-\nlar framework that enables: 1) fine-grained decomposition of the\nGraphRAG workflow, 2) systematic classification of existing tech-\nniques and implemented GraphRAG instances, and 3) creation\nof new GraphRAG instances. Our framework facilitates compre-\nhensive empirical studies of GraphRAG on large-scale real-world\ngraphs and diverse query sets, revealing insights into balancing\nreasoning quality, runtime efficiency, and token or GPU cost, that\nare essential for building advanced GraphRAG systems.\nPVLDB Reference Format:\nYukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, S. Kevin Zhou, and Jianliang\nXu. LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented\nGeneration for Design Space Exploration. PVLDB, 18(10): 3269 - 3283, 2025.\ndoi:10.14778/3748191.3748194\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/gzy02/LEGO-GraphRAG.\n1\nINTRODUCTION\nRecent advancements in large language models (LLMs) have high-\nlighted their strengths in semantic understanding and contextual\nreasoning, enabled by extensive pre-training on vast corpora. De-\nspite these strengths, LLMs often struggle with domain-specific\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 18, No. 10 ISSN 2150-8097.\ndoi:10.14778/3748191.3748194\nqueries and complex contexts, frequently generating “hallucina-\ntions”, in which outputs appear credible but lack factual accu-\nracy. Retrieval-augmented generation (RAG) addresses these limi-\ntations by integrating external knowledge to enhance factual accu-\nracy and contextual relevance. Early RAG implementations, how-\never, often relied on document retrieval methods that introduced\nnoise and excessive context [26, 55, 60, 122]. This limitation has\nled to a growing focus on graph-based RAG systems, known as\nGraphRAG [22, 27, 42, 61].\nIn GraphRAG, conventional document retrieval is replaced by\ngraph-based retrieval, leveraging the structured and relational na-\nture of graphs to extract query-specific reasoning paths that provide\nmore precise and contextually relevant support for LLM reasoning.\nTypically, the GraphRAG workflow consists of two primary phases:\n• Retrieval: This phase retrieves knowledge from graphs by iden-\ntifying reasoning paths relevant to the query.\n• Augmented generation: The retrieved reasoning paths are\nused to augment the LLM prompt, enhancing its ability in reason-\ning and generating accurate and contextually relevant outputs.\nExisting GraphRAG studies [22, 45, 50, 65, 71, 72, 74, 98] have\ndemonstrated the potential of integrating graph data management\ntechniques with GraphRAG. However, despite their promise, these\nstudies are still in their early stages and face two significant chal-\nlenges. First, they lack foundational support in addressing the scala-\nbility of graph algorithms, which is crucial for managing large-scale\ngraphs and reducing query latency in real-world applications. Sec-\nond, there is a knowledge gap regarding the impact of semantic\ninformation on graphs on the performance of GraphRAG. Specifi-\ncally, it is unclear which strategies are most effective for leveraging\nsemantic information from both the query and the graph, and which\ntype of semantic model is best suited for different query scenarios.\nResolving these uncertainties is essential to improving the quality,\nefficiency, and cost of GraphRAG across various query scenarios.\nOur Motivation. Building on the strengths of database research,\nwe aim to address key challenges in GraphRAG and lay the ground-\nwork for future advancements. While efforts like Modular RAG [28]\nhave modularized general RAG, they fall short of addressing the\nGraphRAG’s workflows and needs. We identify three critical gaps:\n• Need for a Unified Framework for Categorizing and An-\nalyzing GraphRAG Solutions. GraphRAG solutions consist\nof diverse technologies, including algorithmic approaches (e.g.,\nrandom walk, PageRank) and neural network-based methods\n\n\n--- Page 2 ---\n\n(e.g., end-to-end and LLM models), each playing a distinct role\nin their functionality. The lack of a unified categorization for\nsystematically analyzing these techniques—despite prior efforts\nsuch as Modular RAG [28] and other surveys [24, 35, 86, 126] on\nLLMs with graphs—hinders effective research summarization\nand the identification of promising candidates for further study.\n• Need for Modular Retrieval in GraphRAG. Current imple-\nmentations of GraphRAG often treat the retrieval phase as a\nsingle, monolithic process, making it difficult to isolate, analyze,\nand optimize individual components. A modular approach is\ndesirable to facilitate the development of more efficient and effec-\ntive retrieval mechanisms. Moreover, while Modular RAG [28]\nmodularizes general RAG workflows, it neither dissects the core\nretrieval process of GraphRAG nor clarifies its distinction from\ngeneral RAG.\n• Absence of a GraphRAG Testbed for New Instance Design\nand Evaluation. Optimizing GraphRAG performance requires\nnavigating a complex trade-off between runtime efficiency and\nreasoning accuracy, influenced by various design factors. A\nGraphRAG testbed is needed to generate diverse implementa-\ntion instances, allowing users to evaluate, compare, and apply\ndifferent approaches while providing clear guidelines and trade-\noffs for constructing optimal GraphRAG instances tailored to\nspecific scenarios. However, Modular RAG [28] and related sur-\nveys [24, 35, 86, 126] focus on conceptual overviews and lack\nimplementation or evaluation support for GraphRAG.\nOur Contributions. We presents the first empirical study on\nGraphRAG and introduces LEGO-GraphRAG, a unified and modular\nframework that provides insights to guide future research through\ncontributions in both framework design and empirical analysis.\nFor Framework Design:\n• We propose LEGO-GraphRAG, a modular framework dividing\nthe retrieval phase into two flexible modules: subgraph-extraction\nand path-retrieval, and classifies the techniques into structure-\nbased and semantic-augmented methods for each module.\n• LEGO-GraphRAG’s modular framework, with categorized tech-\nniques, supports implementing all existing GraphRAG instances\nwhile enabling the creation of new ones, promoting both stan-\ndardization and innovation.\n• We identify essential design factors, including reasoning quality,\nefficiency, and cost, providing a structured trade-off analysis to\nguide the development of GraphRAG instances.\nFor Empirical Research:\n• We study a comprehensive set of GraphRAG instances, including\n7 existing implementations and 16 new instances generated by\nLEGO-GraphRAG. These instances were extensively evaluated\non large-scale real graphs (i.e., Freebase) and 5 commonly used\nGraphRAG query sets, covering various query scenarios.\n• We suggest several modular configurations and a number of im-\nprovements to existing implementations for improved reasoning\nquality and balancing efficiency and cost.\n1.1\nRelated Works\n1.1.1\nRAG and GraphRAG. Early text-based RAG systems rely\non basic retrieval techniques, such as text chunking and cosine\nsimilarity for ranking [60], which are prone to retrieving noisy\nand irrelevant information, leading to lower-quality results [26].\nTo address this, recent works have introduced both pre- and post-\nretrieval improvements. Pre-retrieval methods [128] enhance the\ninput query, while post-retrieval techniques [21, 124] refine the\nranking and filtering of retrieved results. Toolkits like LlamaIndex\nand LangChain [10, 66] offer modular control over these stages, im-\nproving overall retrieval precision and system interpretability [26].\nHowever, refining the retrieval stage to reliably suppress irrelevant\ncontent remains a key challenge [22].\nGraphRAG has recently emerged as a promising direction by\nrepresenting knowledge in graph form, enabling more structured re-\ntrieval, multi-hop reasoning with better interpretability [22, 34, 123].\nCompared to unstructured text retrieval, graph-based retrieval of-\nfers reduced noise, better coverage of entity relations, and lowered\ntoken overhead during inference [22, 75, 93, 101].1 Existing in-\nstances (or implementations) in GraphRAG have drawn heavily\nfrom knowledge-base question answering (KBQA) techniques, uti-\nlizing both information retrieval methods [78, 92] and semantic\nparsing models [14, 59] to identify relevant subgraphs or reasoning\npaths. Microsoft GraphRAG [22] was an early effort exploring the\nadvantages of graph-based over text-based retrieval, focusing on\ngraph construction from text and precomputation. GCR [72] com-\nbines LLMs and beam search to refine reasoning paths, improving\nretrieval alignment with queries. RoG [71] and GSR [45] employ\nPersonalized PageRank (PPR) to retrieve query-specific subgraphs,\nwhile KELP [65] refines reasoning paths using fine-tuned models\nlike BERT [20]. These studies highlight the diverse graph-based and\nneural network-based techniques employed in GraphRAG, show-\ncasing the extensive potential of graphs in enhancing the reasoning\nquality of LLMs.\n1.1.2\nLLMs with (Knowledge) Graphs. GraphRAG aligns with the\nbroader research to integrate LLMs with structured knowledge,\nsuch as KGs. Surveys [56, 84] outline three paradigms: KG-enhanced\nLLMs, LLM-augmented KGs, and their joint integration. More-\nover, [41] reviews knowledge-enhanced pre-trained language mod-\nels (e.g., LLMs) for language understanding and generation, and\n[52] examines LLMs as interfaces for data pipelines, highlighting\ntheir integration with KGs in AI systems. Our work fits within the\nKG-enhanced LLMs paradigm and aims to contribute a modular\nGraphRAG framework to support systematic design and evaluation\nof reasoning-augmented LLMs with graphs.\n2\nPRELIMINARIES\n2.1\nProblem Formalization\nGraphRAG is designed to integrate structured knowledge into the\nreasoning process of LLMs, enhancing their ability to generate con-\ntent that is both accurate and contextually relevant. In GraphRAG,\nknowledge is typically represented in the form of Text-Attributed\nGraphs (TAGs), where both nodes and edges are enriched with\ntextual information, with knowledge graphs serving as a typical ex-\nample. For clarity and without loss of generality, the graphs referred\nto in this paper will follow the structure outlined in Definition 1.\n1A detailed comparison between GraphRAG and text-based RAG is provided in our\ntechnical report B.10.\n\n\n--- Page 3 ---\n\nDefinition 1 (Graph in GraphRAG). In GraphRAG, a graph\nis defined as a directed labeled graph 𝐺= (𝑉, 𝐸), where 𝑉represents\nthe set of nodes (entities), and 𝐸denotes the set of directed edges that\nsignify relations between those entities. Each node 𝑣∈𝑉and each\nedge 𝑒∈𝐸carry semantic information. Specifically, nodes represent\nentities with associated semantic attributes (e.g., descriptions), while\nedges represent relations with semantic contexts (e.g., relation types).\nFormally, the graph in GraphRAG is represented as a collection\nof triples: 𝐺= {𝜏𝑖= (𝑠𝑖,𝑟𝑖,𝑡𝑖) | 𝑠𝑖,𝑡𝑖∈𝑉,𝑟𝑖∈𝐸} , where each 𝜏𝑖=\n(𝑠𝑖,𝑟𝑖,𝑡𝑖) is a triple representing that the source entity 𝑠𝑖is connected\nto the target entity 𝑡𝑖by the relation 𝑟𝑖, encapsulating structured,\ndomain-specific knowledge. While a single triple 𝜏𝑖is a basic unit of\nknowledge, a sequence of connected triples forms reasoning paths that\nsupport more advanced inferences.\nIn practice, graphs are built from domain knowledge or text (e.g.,\nMicrosoft GraphRAG [22] with auxiliary summaries; see Defini-\ntion 2). As text-to-graph construction is well-studied [11, 22, 32,\n33, 64] and not central to our framework, we treat it as optional\nbut include a GraphRAG instance with textual information in our\nanalysis (Section 4.2 e).\nDefinition 2 (Graph Construction (Optional)). In GraphRAG,\ngraph construction is an optional step that builds a text-attributed\ngraph from large-scale text, and may also precompute subgraphs and\nsummaries to enhance GraphRAG. Specifically:\n• Construction: Identify entities and relations from text and repre-\nsent them as nodes and edges in the graph.\n• Precomputation (Optional): Compute subgraphs and relevant\ntextual summaries from the constructed graph, such as via graph\nclustering or community detection.\nWith text-attributed auxiliary graphs, GraphRAG operates in\ntwo main phases: retrieval and augmented generation phases. Dur-\ning the retrieval phase (Definition 3), the system extracts relevant\nentities and reasoning paths from the graph, aligning them with the\nquery context. In the augmented generation phase (Definition 4),\nthese retrieved reasoning paths are incorporated to enhance LLM’s\ncontent generation capabilities.\nDefinition 3 (Retrieval Phase). This phase starts by process-\ning the query 𝑞to extract relevant entities and relations that cor-\nrespond to nodes and edges in the graph 𝐺: Extract(𝑞,𝐺) →𝜖𝑞=\n({𝑣(𝑞)\n𝑖\n}, {𝑒(𝑞)\n𝑖\n}), where𝜖𝑞is the set of entities and relations extracted.2\nUsing the extracted entities {𝑣(𝑞)\n𝑖\n} and relations {𝑒(𝑞)\n𝑖\n} in 𝜖𝑞, a\nvariety of methods (e.g., search algorithms, neural network-based re-\ntrieval/ranking models) are applied to retrieve reasoning paths P𝑞that\nconnect 𝜖𝑞to potential target answers, potentially spanning multiple\nhops. The retrieval process is thus formalized by: Retrieve(𝜖𝑞,𝐺) →\nP𝑞= {𝑃(𝑞)\n𝑖\n} where each reasoning path 𝑃(𝑞)\n𝑖\n∈P𝑞of length 𝑘is\nrepresented as a sequence of triples ⟨𝜏1,𝜏2, . . . ,𝜏𝑘⟩:\n𝑃(𝑞)\n𝑖\n= ⟨𝜏1,𝜏2, . . . ,𝜏𝑘⟩= ⟨(𝑠1,𝑟1,𝑡1), (𝑡1,𝑟2,𝑡2), . . . , (𝑡𝑘−1,𝑟𝑘,𝑡𝑘)⟩\n2Extracting entities/relations from the semantic context of 𝑞has been well-studied [38,\n50, 92] and is beyond the scope of this paper. Following prior works on reasoning\ntasks in graphs [4, 16, 79, 92, 119], queries are typically categorized by the minimum\nnumber of reasoning hops required to reach the answer from the query entities or\nrelations (e.g., one-hop queries or multi-hop queries). In addition, queries can also be\ncategorized by the number of answers and the number of query entities involved.\nGraph\nQuery Context\nSemantic Models\nLLMs\nQuery Entity\nLarge \nScale\nSmall \nScale\nSubgraph\nReasoning\nPaths\nGraph-based Algorithms\nPPR, \nRW,KSE\nBFS,DFS,\nDijkstra\nNode/Edge/\nTriple pruning\nOne-way/\nInteractive retrieval\nRetrieval Phase\nAugmented \nGeneration Phase\nGeneration\nQuery \nContext\nAugmented \nPrompt\nFigure 1: The GraphRAG Workflow\nDefinition 4 (Augmented Generation Phase). In this phase,\nthe retrieved reasoning paths P𝑞are merged with the original query\n𝑞, forming an augmented prompt 𝑞′: Augment(𝑞, P𝑞) →𝑞′. This\naugmented prompt is then input to the LLMs, enhancing their ability\nto generate more accurate and contextually relevant content for the\nfinal answer: Generate(𝑞′, LLM) →final answer.\nFigure 1 illustrates the GraphRAG workflow, where retrieval\nforms the foundation by supplying key reasoning paths for LLM\naugmentation and generation. As these stages hinge on retrieval\nquality and LLM strength, this work focuses on analyzing the mod-\nular design and solution space of the retrieval phase.\n3\nLEGO-GRAPHRAG FRAMEWORK\n3.1\nOverview\nTable 1 outlines the design framework of LEGO-GraphRAG, struc-\ntured along two key dimensions: modules and method types.\nSpecifically, the core retrieval phase is divided into two modules:\nthe subgraph-extraction module and the path-retrieval module. Each\nmodule incorporates two solution types: structure-based methods\nand semantic-augmented methods. Accordingly, GraphRAG instan\nces within the LEGO-GraphRAG framework are classified into five\ndistinct groups based on the combinations of modules and solution\ntypes used. Table 2 provides a detailed breakdown of these groups,\nencompassing both existing methods and new instances developed\nin our empirical study. In general, the LEGO-GraphRAG framework\nis materialized by two key considerations.\n• Natural Segmentation of the GraphRAG Process. The rea-\nsoning path retrieval process is naturally segmented into two\nphases: a) extracting a query-relevant subgraph to scale down\nthe search space; and b) meticulously retrieving reasoning paths\nfrom the extracted subgraph.\n• Facilitation of GraphRAG Research Analysis. The frame-\nwork enables a comprehensive analysis of recent GraphRAG\nadvancements, which predominantly focus on enhancing one or\nboth modules, as summarized in Table 2.\n3.1.1\nSubgraph-Extraction vs. Path-Retrieval Modules. The\nsubgraph-extraction (SE) module aims to enhance the effective-\nness and efficiency of reasoning path retrieval by reducing the\nsearch space from the full graph to a smaller set of query-relevant\nsubgraphs. The path-retrieve (PR) module then operates on these\nsubgraphs to retrieve reasoning paths using specific methods. Based\non the retrieval phase of GraphRAG formalized in Definition 3, the\ntwo modules are formalized as follows.\n\n\n--- Page 4 ---\n\nDefinition 5 (Subgraph-Extraction (SE)). Given a query\n𝑞, a graph 𝐺= (𝑉, 𝐸), and a set of entities and relations 𝜖𝑞derived\nfrom specified query 𝑞, the SE module aims to extract a query-specific\nsubgraph 𝑔𝑞⊆𝐺, defined as: 𝑆𝐸(𝐺,𝑞,𝜖𝑞) →𝑔𝑞.\nDefinition 6 (Path-Retrieval (PR)). Given an extracted\nsubgraph𝑔𝑞⊆𝐺and the entity and relation set𝜖𝑞derived from query\n𝑞, the process of this module obtains a reasoning path set P𝑞= {𝑃(𝑞)\n𝑖\n}\nwith the following process: 𝑃𝑅(𝐺or 𝑔𝑞,𝑞,𝜖𝑞) →P𝑞= {𝑃(𝑞)\n𝑖\n}.\nOf the two modules, the path-retrieval module is essential for\nany GraphRAG instance, while the subgraph-extraction module is\noptional. The optionality of the subgraph-extraction module de-\npends largely on the graph size, primarily due to computational ef-\nficiency. For smaller graphs, where the node and edge space remain\nmanageable, direct retrieval of reasoning paths is feasible, making\nthe subgraph-extraction module optional. However, for large-scale\ngraphs, the exponential growth in the number of nodes, edges, and\npaths significantly complicates the retrieval [23, 53, 54, 67]. The\nsubgraph-extraction module addresses this by extracting a query-\nrelevant subgraph, reducing the retrieval space and improving com-\nputational efficiency and relevance. After passing through these\nmodules, the refined reasoning paths enhance the reasoning abili-\nties of LLMs during the generation phase, as detailed in Definition 4.\n3.1.2\nStructure-based vs. Semantic-augmented Methods. For\neach module, systematically classifying and summarizing potential\nsolutions is important, as it enables the identification of potential\nstrategies and provides a deeper understanding of existing imple-\nmentations. At their core, the workflows of the subgraph-extraction\nand path-retrieval modules share potential similarities, as both in-\nvolve retrieving a small-scale but query-relevant subset from the\ngraph. Accordingly, the design solutions for these modules can be\ninterwoven and further categorized into two complementary cate-\ngories: structure-based methods, focusing on the graph’s topological\nproperties, and semantic-augmented methods, utilizing the semantic\ninformation of both the graph and the query, as detailed in Table 1.\nStructure-based methods (SBE and SBR) use graph algorithms [25,\n57, 69, 99, 113, 118] to iteratively explore nodes (entities) and edges\n(relations), constructing subgraphs or reasoning paths. Semantic-\naugmented methods (SAE, OSAR, and ISAR) exploit the semantic\nrelevance between the query and the nodes, edges, and triples of\ngraphs, using two types of semantic models: end-to-end models\n(EEMs) [88, 89, 104] and large language models (LLMs) [8, 98, 127].\nThe two types of models are applied differently. SAE/OSAR meth-\nods refine candidate subgraphs/paths for better semantic alignment.\nISAR methods incorporate semantic evaluation directly into the\npath-retrieval process, using EEMs/LLMs to interactively identify\nrelevant reasoning paths. In the sequel, we investigate the design\nsolutions of the two modules, subgraph-extraction (Section 3.2) and\npath-retrieval (Section 3.3), with the aforementioned two types of\nmethods, structure-based and semantic-augmented methods.\n3.2\nDesign Solutions of Subgraph-Extraction\nThe design solution of the subgraph-extraction module is comprised\nof structure-based extraction (SBE in Section 3.2.1) and semantic-\nbased extraction (SAE in Section 3.2.2).\n3.2.1\nStructure-Based Extraction (SBE). These solutions ex-\nploit the structural information of a graph 𝐺= (𝑉, 𝐸), starting with\nthe query-relevant entities {𝑣(𝑞)\n𝑖\n} ∈𝜖𝑞,3 to identify correspond-\ning nodes (entities) and edges (relations) that are pertinent to the\nquery 𝑞. This process constructs smaller subgraphs centered around\neach 𝑣(𝑞)\n𝑖\n, which are then aggregated to produce the query-related\nsubgraph 𝑔𝑞. Formally, the procedure is defined as:\n𝑆𝐸(𝐺,𝑞,𝜖𝑞) →𝑔𝑞=\n⋃︂\n𝑣(𝑞)\n𝑖\n∈𝜖𝑞𝑔(𝑞,𝑣(𝑞)\n𝑖\n)\nHere, 𝑔𝑞represents the union of the subgraphs 𝑔(𝑞,𝑣(𝑞)\n𝑖\n), each fo-\ncusing on an entity𝑣(𝑞)\n𝑖\nin𝜖𝑞. For each query entity, three extraction\nstrategies can be applied: random walk-based, neighborhood-based,\nand structural importance-based strategies.\na) Random Walk-based. The random walk (RW) algorithm [87,\n116] and its variants [25, 83, 85, 94, 95] enumerate all entities in 𝜖𝑞,\nand, for each entity 𝑣(𝑞)\n𝑖\n, iteratively expand a subgraph by randomly\nselecting edges and nodes at each step. This simple yet effective\nstrategy gradually constructs a subgraph of the desired size, making\nit a commonly used baseline method [38, 45, 71, 72].\nb) Neighborhood-based. This strategy aims to capture the local\nstructure around an entity 𝑣(𝑞)\n𝑖\n∈𝜖𝑞by expanding its neighborhood.\nThe most representative method is K-hop subgraph-extraction (KSE)\n[38], where the subgraph is constructed by including all nodes 𝑣𝑗\nand edges 𝑒𝑖𝑗such that the shortest path distance 𝑑(𝑣(𝑞)\n𝑖\n, 𝑣𝑗) ≤𝐾.\nThis approach ensures that the subgraph captures the immediate\nrelational context surrounding the query entity, progressively in-\ncluding nodes and edges within the specified radius (i.e., 𝐾-hops).\nc) Structural Importance-based. These methods [37, 82, 118] iden-\ntify nodes based on their structural importance relative to an entity\n𝑣(𝑞)\n𝑖\n. Nodes are ranked by importance scores, and a subset of high-\nranking 𝑁𝑝𝑝𝑟nodes, along with their directly connected edges, are\nselected to form the subgraph. Prominent methods, such as PageR-\nank [82] and Personalized PageRank (PPR) [37], compute a relevance\nscore for each node based on topological position, link density and\ncentrality. Specifically, PageRank assigns an importance score 𝑆(𝑣𝑗)\nto each node 𝑣𝑗, which is iteratively updated based on both its lo-\ncal connectivity and the scores of its neighboring nodes, thereby\ncapturing its global significance. Personalized PageRank (PPR) en-\nhances this process by “personalizing” the distribution towards the\nquery entity 𝑣(𝑞)\n𝑖\n, ensuring that nodes closer to 𝑣(𝑞)\n𝑖\nreceive higher\nscores. While PPR is a widely adopted approach for SBE solutions,\nits computational cost scales with the size of the graph, present-\ning a considerable bottleneck for real-time GraphRAG pipelines.\nSection 4.3 discusses this efficiency challenge in detail.\n3.2.2\nSemantic-Augmented Extraction (SAE). Semantic mod-\nels play a key role in facilitating subgraph-extraction by assessing\nthe semantic relevance of graph components. These models fall\ninto two main categories: End-to-End Models (EEMs) and Large\nLanguage Models (LLMs).\na) End-to-End Models (EEMs). EEMs process the text associated\nwith graph components (i.e., nodes, edges, triples) and queries to\ncompute semantic relevance. They either output statistical proper-\nties and embeddings for relevance calculation or directly provide\nrelevance scores. Formally, for nodes, edges, or triples 𝑣,𝑒, or 𝜏∈𝐺\n3While subgraphs can be built from relationship 𝑒(𝑞)\n𝑖\n∈𝜖𝑞, practical methods predom-\ninantly use entities (nodes) as the basis for subgraph construction [45, 65, 70, 72, 98].\n\n\n--- Page 5 ---\n\nTable 1: Design Solutions of LEGO-GraphRAG Framework: The framework comprises of four groups of instances: (I) SBE & SBR; (II) SBE & I/OSAR;\n(III) SAE & SBR; (IV) SAE & I/OSAR. When the subgraph-extraction module is optional, an additional group, (V) SBR or I/OSAR, is included.\nModule I: Subgraph-Extraction\nModule II: Path-Retrieval\nSolution I:\nStructure-based\nMethods\nSolution II:\nSemantic-Augmented\nMethods\n◦Structure-Based Extraction (SBE)\n-Random Walk-based: Random Walk (RW)\n-Neighborhood-based: K-hop Subgraph-Extraction (KSE)\n-Structural Importance-based: Personalized PageRank (PPR)\n◦Structure-Based Retrieval (SBR)\n-Enumerated Path-Retrieval (EPR)\n-Shortest Path-Retrieval (SPR)\n◦Semantic-Augmented Extraction (SAE)\n-with End-to-End Models (EEMs)\n-with Large Language Models (LLMs)\n◦One-way Semantic-Augmented Retrieval (OSAR)\n-with End-to-End Models (EEMs)\n-with Large Language Models (LLMs)\n◦Interactive Semantic-Augmented Retrieval (ISAR)\n-with End-to-End Models (EEMs)\n-with Large Language Models (LLMs)\nTable 2: Five Groups of Instances under the LEGO-GraphRAG Framework\nGroup\nSubgraph-Extraction\nPath-Retrieval\nImplemented Instances\nStructure-based Methods on Both\nModules (Group (I): SBE & SBR)\nSBE (-RW/KSE/PPR)\nSBR (-EPR/SPR)\nOur Instance: No.1\nSemantic-Augmented Methods\non Both Modules\n(Group (II): SAE & I/OSAR)\nSAE (-EEMs/LLMs)\nOSAR (-EEMs/LLMs)\nOur Instances: No.2, 3, 4, 5\nSAE (-EEMs/LLMs)\nISAR (-EEMs/LLMs)\nGCR (arXiv24) [72]\nOur Instances: No.6, 7, 8, 9\nSemantic-Augmented Methods\non Subgraph-Extraction\n(Group (III): SAE & SBR)\nSAE (-EEMs/LLMs)\nSBR (-EPR/SPR)\nRoG (ICLR24) [71]\nGSR (EMNLP24) [45]\nOur Instances: No.10, 11\nSemantic-Augmented Methods\non Path-Retrieval\n(Group (IV): SBE & I/OSAR)\nSBE (-RW/KSE/PPR)\nOSAR (-EEMs/LLMs)\nOur Instances: No.12, 13\nSBE (-RW/KSE/PPR)\nISAR (-EEMs/LLMs)\nStructGPT (EMNLP23) [50]\nOur Instances: No.14, 15\nWithout Subgraph-Extraction\nModules\n(Group (V): SBR or I/OSAR)\nNone\nSBR (-EPR/SPR)\n-\nNone\nOSAR (-EEMs/LLMs)\nKELP (ACL24) [65]\nNone\nISAR (-EEMs/LLMs)\nToG (ICLR24) [98], DoG (arXiv24) [74]\nand a query 𝑞, the goal is: EEMs(𝑣or 𝑒or 𝜏,𝑞) →scores. Using\nthese scores, the most relevant graph components are identified for\nsubgraph-extraction. EEM can be categorized into two main types:\nStatistical Models [73, 90, 91] and Embedding Models/Re-Rankers\n[13, 88]. The former relies on statistical computations, while the\nlatter consists of end-to-end neural network models to produce\nembeddings or rank relevance.\n⁓⁓⁓\na).1 ⁓⁓⁓⁓⁓⁓⁓\nStatistical⁓⁓⁓⁓⁓⁓\nModels. Statistical models are foundational in text\nanalysis and semantic modeling, relying on metrics like term fre-\nquency (TF) and inverse document frequency (IDF) [91]. Popular\nmethods include BM25 [90] for ranking relevance in information\nretrieval, Latent Semantic Analysis (LSA) [18] for uncovering latent\nsemantic structures via dimensionality reduction, and Latent Dirich-\nlet Allocation (LDA) [5] for topic modeling, which aids semantic\nsimilarity through topic distributions.\nFor example, BM25 treats textual information attached to graph\ncomponents (nodes, edges, or triples) as “documents” within a\ncorpus (the graph). During precomputation, the text of each graph\ncomponent is tokenized, enabling the calculation of TF and IDF.\nWhen a query is issued, its text undergoes tokenization, and BM25\ncomputes a relevance score for each graph component based on\nthe query terms and their frequency distributions across the graph.\nStatistical models are efficient, with low computational costs and\nshort execution times. They are suitable for simpler semantic tasks\nor as baselines in semantic-augmented workflows.\n⁓⁓⁓\na).2 ⁓⁓⁓⁓⁓⁓⁓⁓\nEmbedding⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\nModels/Re-Rankers. Embedding models and re-\nrankers use pre-trained neural network (NN) models with smaller\nparameter sizes (millions) compared to LLMs (billions). Trained\non large corpora, they generate dense embeddings that encode\nrich semantic information [88]. Embedding models (e.g., Sentence-\nTransformers [111]) independently generate embeddings for queries\nand graph components (nodes, edges, triples). Semantic similarity\nis computed using metrics [9, 48] like cosine similarity:\nST(𝑣or 𝑒or 𝜏,𝑞) →cos(emb(𝑣or 𝑒or 𝜏), emb(𝑞))\nRe-Rankers (e.g., BGE-Reranker [13]) take combined query and\ngraph components as inputs and output a similarity score, allowing\nfor deeper contextual interactions:\nBGE(𝑣or 𝑒or 𝜏,𝑞) →NeuralNetworks(emb(𝑣or 𝑒or 𝜏) ⊕emb(𝑞))\nThese models generally require longer execution and pre-training\ntimes compared to statistical models. However, their balance of\nefficiency and semantic precision makes them a common choice for\nsemantic modeling in GraphRAG. Domain-specific fine-tuning can\nfurther enhance these models’ performance [45, 65, 71, 72], though\nit comes at an additional cost.\n⁓⁓⁓\na).3 ⁓⁓⁓\nSAE⁓⁓⁓⁓\nwith ⁓⁓⁓⁓⁓\nEEMs. In the subgraph-extraction module, EEMs\nprune nodes, edges, or triples from graph 𝐺based on their semantic\nrelevance to the query, reducing computational overhead. However,\ndirectly applying EEMs to large graphs is costly. Thus, it is desired\nto have a pre-filtering step before applying semantic models, which\n\n\n--- Page 6 ---\n\nextracts a smaller subgraph 𝑔∈𝐺(e.g., via SBE methods-PPR) to\nfocus on relevant components [45, 65]. Thus, this subgraph pruning\nprocess generally follows three key strategies: node pruning, edge\npruning, and triple pruning.\n• Node Pruning (NP): Nodes 𝑣∈𝑔are ranked by semantic rele-\nvance scores to query 𝑞. Top 𝑁𝑣nodes are retained, ensuring\nconnected edges are included: 𝑔(𝑞,𝑣) = {𝑣| 𝑣in top 𝑁𝑣} ∪{𝑒|\n𝑒connects 𝑔(𝑞,𝑣)}.\n• Edge Pruning (EP): Edges 𝑒∈𝑔are scored, with top 𝑁𝑒edges re-\ntained. Disconnected nodes are removed:𝑔(𝑞,𝑒) = {𝑒| 𝑒in top 𝑁𝑒}\n∪{𝑣| 𝑣connected by 𝑔(𝑞,𝑒)}.\n• Triple Pruning (TP): Top 𝑁𝜏triples 𝜏∈𝑔are selected, form-\ning a minimal subgraph containing these triples: 𝑔(𝑞,𝜏) = {𝜏|\n𝜏in top 𝑁𝜏}.\nBatch processing can be used for EEMs to efficiently compute rel-\nevance scores for all graph components in𝑔, streamlining subgraph-\nextraction:\n𝑆𝐸(𝐺,𝑞,𝜖𝑞), 𝐸𝐸𝑀𝑠→𝑔𝑞= 𝑔(𝑞,𝑣/𝑒/𝜏)\nb) Large Language Models (LLMs). LLMs, such as Llama [102, 103,\n105] , Qwen [117], and GPT [7, 80], are large pre-trained language\nmodels known for their ability to capture nuanced semantics and\ncontextual understanding due to extensive training on large-scale\ncorpora. Unlike EEMs, LLMs provide flexible evaluation of graph\ncomponents (nodes, edges, triples) through prompt-based interac-\ntions. For instance, to evaluate the node relevance to a query, LLMs\ncan generate relevance scores or directly a ranked list of entities.\nFormally, this process is represented as:\nLLMs(𝑣or 𝑒or 𝜏,𝑞, Prompt) →scores or ranked list\nIn practice, LLM outputs often fall short of prompt requirements\ndue to output length limits and inherent limitations (e.g., hallucina-\ntion [44]). Generated entity lists (from subgraph-extraction module)\nor reasoning paths (from path-retrieval module) may be sparse or\nlow-quality. Mitigation approaches include: prompt tuning [76],\nfine-tuning [15] for long-context understanding, or adopting more\ncapable LLMs. Moreover, we explore some general purpose strate-\ngies to address this challenge in Section 5.3.\nWhile fine-tuning LLMs on domain-specific knowledge improves\nperformance for targeted queries, it is more resource-intensive than\ntuning embedding models or re-rankers4 and may generalize poorly\nacross scenarios [61]. LLMs also have high inference costs due to\nthe model size and autoregressive decoding. Hence, we recommend\nusing them selectively and offer non-LLM alternatives in Section 4.2.\nCost-efficient fine-tuning methods (e.g., LoRA [40], QLoRA [19])\nand inference optimizations (e.g., pruning [115], quantization [47],\nKV caching [49], parallel decoding [12], and semantic compres-\nsion [68]) further reduce overhead.\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\nb).1 SAE with LLMs. The subgraph extraction process using LLMs\nmirrors that of EEMs and can be formalized as follows:\n𝑆𝐸(𝐺,𝑞,𝜖𝑞), 𝐿𝐿𝑀𝑠, 𝑃𝑟𝑜𝑚𝑝𝑡𝑠→𝑔𝑞= 𝑔(𝑞,𝑣/𝑒/𝜏)\nEven on subgraph 𝑔, LLM-based semantic evaluation remains costly\nand faces token constraints, making it hard to process 𝑔with a\nsingle LLM call. To mitigate this, an additional pre-filtering step is\napplied, where candidate components within 𝑔can be ranked and\n4Details about fine-tuning for both EEMs and LLMs are in the technical report B.1.\npruned via lightweight heuristics (e.g., random selection or EEM-\nbased similarity selection) before invoking LLMs. Subsequently, the\nrefined 𝑔can be passed to LLMs for evaluation within a single call.5\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\nb).2 Other Methods with LLMs and Limitations. In certain scenar-\nios, LLMs fine-tuned on domain knowledge can act as agents [70, 71]\nto generate extraction rules (e.g., SPARQL queries [1, 43, 70]). For\nexample, in response to a query, RoG [71] utilizes LLMs fine-tuned\non domain-specific knowledge graphs to identify key relation-\nships between entities, which are then used to construct SPARQL\nqueries that extract relevant triples, forming the query-relevant\nsubgraph. The subgraph containing these triples ultimately serves\nas the query-relevant subgraph. While LLMs offer direct subgraph-\nextraction without pruning, these approaches are limited by high\ncomputational costs, reliance on accurate templates, and reduced\ngeneralizability for diverse queries. Fine-tuning LLMs for domain-\nspecific tasks further adds to the resource burden, making this ap-\nproach less scalable for subgraph-extraction on large-scale graphs.\n3.3\nDesign Solutions of Path-Retrieval\nThe design solution of the path-retrieval module is comprised of\nstructure-based retrieval (SBR in Section 3.3.1), one-way semantic-\naugmented retrieval (OSAR in Section 3.3.2), and interactive seman-\ntic augmented retrieval (ISAR in Section 3.3.3).\n3.3.1\nStructure-Based Retrieval (SBR). These methods operate\non either the original graph or extracted subgraphs, leveraging\ngraph algorithms to identify reasoning paths. Starting from the\nquery-relevant entities, these algorithms iteratively traverse node\nconnections to generate candidate paths, utilizing techniques such\nas Breadth-First Search (BFS), Depth-First Search (DFS), Dijkstra’s\nalgorithm, and their variants. The procedure is formally defined as:\n𝑃𝑅(𝐺𝑜𝑟𝑔𝑞,𝑞,𝜖𝑞) →P𝑞=\n⋃︂\n𝑣(𝑞)\n𝑖\n∈𝜖𝑞P(𝑞,𝑣(𝑞)\n𝑖\n)\nHere, P𝑞represents the union of the reasoning path sets P(𝑞,𝑣(𝑞)\n𝑖\n),\neach focusing on an entity 𝑣(𝑞)\n𝑖\nin 𝜖𝑞. Two primary approaches are\napplied to extract paths for each query entity: enumerated path-\nRetrieval and shortest path-Retrieval.\na) Enumerated Path-Retrieval (EPR). EPR [65] enumerates all pos-\nsible paths from an entity 𝑣(𝑞)\n𝑖\n∈𝜖𝑞to every reachable node within\nthe graph or subgraph. It captures diverse reasoning chains, pro-\nviding additional context for LLMs, although it may introduce in-\nformation that is not related to the query, potentially adding noise\nto the reasoning process of the LLMs.\nb) Shortest Path-Retrieval (SPR). SPR [36, 92, 97, 112, 121] identi-\nfies all shortest paths from an entity 𝑣(𝑞)\n𝑖\n∈𝜖𝑞to reachable nodes,\nensuring that the extracted paths are concise and directly relevant\nto the query.\n3.3.2\nOne-way Semantic-Augmented Retrieval (OSAR). Sim-\nilar to the SAE methods in the subgraph-extraction module, the\nOSAR methods leverage semantic models to evaluate and select the\n𝑁𝑝most relevant paths from a candidate path set P ∈𝐺or 𝑔𝑞,6 to\nconstruct a refined set of reasoning paths P𝑞that are most relevant\n5Multiple LLM calls can be used to evaluate all graph components in 𝑔, but this incurs\nsubstantial overhead during subgraph-extraction and is typically avoided.\n6 P is typically obtained from SBR methods, such as EPR or SPR.\n\n\n--- Page 7 ---\n\nto the query 𝑞. Path semantic relevance can be evaluated using\neither EEMs or LLMs, as outlined in Section 3.2.2. Note that when\nusing LLMs to evaluate and select reasoning paths, it may not be\nfeasible to input all paths in P in a single call. In such cases, random\nselection or EEMs can be used as a preliminary filter to reduce the\nnumber of candidate paths.7\n3.3.3\nInteractive Semantic-Augmented Retrieval (ISAR). Un-\nlike OSAR, which separates path generation and evaluation, ISAR\nintegrates both within an interactive framework. Using interactive\nsearch algorithms [81, 96, 106] combined with semantic models,\nISAR directs the search process toward semantically relevant graph\nregions from the outset. By interactively and dynamically evaluat-\ning and prioritizing path extensions based on their relevance to the\nquery, ISAR effectively narrows the search space during execution.\nFor example, in Beam Search, 8 which is widely used in GraphRAG\nworkflow [16, 98], a fixed number of candidate paths (beams) are\nexplored iteratively to identify the most relevant reasoning path\nfor a query entity 𝑣(𝑞)\n𝑖\n∈𝜖𝑞. At each step, potential path extensions\nare evaluated for semantic relevance using a greedy strategy, and\nthe top-𝐵paths are retained. This process continues until a stop-\nping criterion is met, e.g., reaching the maximum path length 𝐿\nor exhausting relevant extensions. The final output consists of the\nmost relevant reasoning paths discovered during the search.\nIn ISAR, semantic models for evaluating path extensions can be\nEEMs, LLMs, or a hybrid of both to combine their strengths. Based\non practical exploration, we propose three hybrid strategies:\n• LLMs Refinement (LLMs-R, in ISAR-EEMs): After the ISAR-\nEEMs search process, LLMs are used to refine the retrieved paths\nby reducing redundancy and improving semantic coherence.\n• EEMs Pre-filter (EEMs-PF, in ISAR-LLMs): At each step of\nthe LLMs-based search, EEMs pre-rank and filter candidate ex-\ntensions before passing them to the LLMs, reducing input size\nand improving efficiency.\n• EEMs Supplement (EEMs-S, in ISAR-LLMs): At each step\nof path expansion, if the LLMs generate too few or low-quality\ncandidates, EEMs are used to supplement the expansion with\ntop-ranked relevant paths.\nAdditionally, for small-scale graphs, LLM-agent-based ISAR meth-\nods [74] can generate reasoning paths through single- or multi-turn\ninteractions, if LLMs are fine-tuned on domain-specific knowledge.\nDespite domain-specific effectiveness, this approach suffers from\nlimited generalizability and efficiency due to its reliance on fine-\ntuned LLMs, context length constraints, and poor scalability. Hence,\nit is not our focus of the framework.\n4\nEXPERIMENT\nBuilding upon the LEGO-GraphRAG framework, we develop an\nimplementation that facilitates the seamless integration of modules\nand methods for constructing GraphRAG instances.9 To assess the\n7Alternatively, multiple LLM calls can be used to process all reasoning paths, improving\nquality at the cost of efficiency. This feature is supported in our implementation (see\ntechnical report B.6 and our open-source repository).\n8See the technical report B.8 for detailed algorithm\n9Our implementation is built in Python and integrates libraries such as Transformers\n[114], iGraph [17], PyTorch[2], and vLLM [58] to support flexible configuration of\nalgorithms, models, and reasoning. The semantic models used are publicly available\non the Hugging Face [46].\nTable 3: Existing Instances vs. LEGO-GraphRAG Instances\nGraphRAG Instances\nWebQSP\nCWQ\nHits@1\nRecall\nHits@1\nRecall\nRoG [71] (RoG planning w/ChatGPT)\n81.51\n71.60\n52.68\n48.51\nLEGO-RoG (RoG planning w/ChatGPT)\n82.79\n64.41\n56.06\n49.76\nKELP [65] (one-hop w/gpt-4o-mini)\n31.06\n-\n14.16\n-\nLEGO-KELP (one-hop w/gpt-4o-mini)\n77.36\n63.99\n48.65\n43.88\nToG [98] (w/Llama3-8B)\n59.76\n43.05\n36.97\n32.69\nLEGO-ToG (w/Llama3-8B)\n66.44\n44.77\n40.26\n33.63\neffectiveness of the LEGO-GraphRAG framework and our imple-\nmentation, we create versions of our framework based on the core\nideas of three state-of-the-art instances—RoG [71], KELP [65], and\nToG [98]—and align the experimental setups to ensure a fair com-\nparison. As shown in Table 3, the instances implemented within\nour framework exhibit performance comparable to their original\ncounterparts across two datasets. For KELP, our implementation\noutperforms the original due to the effective integration of two\nmodules and the optimization of the employed methods.\nAs shown in Table 2, the existing instances are far from covering\nthe four groups of GraphRAG instances within our framework.\nThus, we construct 15 distinct GraphRAG instances (numbered\nand labeled in Table 2) by selecting representative methods from\nsubgraph-extraction and path-retrieval modules for a detailed em-\npirical study (Section 4.2). In addition, we conduct separate eval-\nuations of the various methods, strategies, and semantic models\nutilized in the two modules (Sections 4.3 and 4.4).\n4.1\nExperiment Settings\nGraph and GraphRAG Query Datasets. We leverage Freebase [6],\na large-scale, multi-domain knowledge base (e.g., on finance, law,\nsports) widely used in GraphRAG research [45, 50, 51, 70–72, 77, 98,\n108, 121], along with four established query datasets, WebQSP [120],\nCWQ [100], GrailQA [30], and WebQuestions [16], covering di-\nverse and challenging GraphRAG scenarios. Following standard\nsettings [38, 51, 71, 98], we construct dataset-specific graphs, in-\ncluding all triples reachable within the maximum reasoning hops\nfrom query entities (100M nodes, 300M edges) and sample 1,000 test\nqueries per dataset (one-hop: multi-hop is 1:1). All experiments (Sec-\ntions 4.2–4.4) are conducted based on these datasets. We also include\nMetaQA [125], a non-Freebase dataset built on Wiki-Movies [78],\na single-domain base focused on movies (40K nodes, 130K edges),\nwith 1,000 test queries to assess cross-base and cross-domain trans-\nferability of our framework.10\nMetrics. Our empirical study is based on three key metrics: qual-\nity, efficiency, and cost. a) Quality. ⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\na).1 For the subgraph-extraction,\nF1 Score serves as the primary evaluation metric, balancing Pre-\ncision and Recall to reflect overall extraction quality.11 However,\nsince this module directly influences the retrieval space for down-\nstream modules, we introduce a minimum Recall threshold to en-\nsure sufficient coverage of relevant entities and relations. Rely-\ning solely on high F1 Score may favor Precision at the cost of\n10Further details of datasets are in our technical report B.2 and B.3.\n11Precision = |C𝑞∩A𝑞|/|C𝑞|, Recall = |C𝑞∩A𝑞|/|A𝑞|, where C𝑞and A𝑞represent\nthe predicted and ground truth sets, respectively. The F1 score is the harmonic mean\nof Precision and Recall: F1 score = (2 · Precision · Recall)/(Precision + Recall)\n\n\n--- Page 8 ---\n\nmissing important components. In practice, a fixed Recall thresh-\nold (e.g., around 60%) is usually sufficient to ensure downstream\nperformance, after which F1 Score can guide subgraph-extraction.\nWe also explore adaptive thresholding based on query complexity,\ngraph density, downstream tasks, and historical logs, balancing\ncoverage and efficiency.12\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\na).2 For the path-retrieval, the F1 score11\nmeasures the alignment between the retrieved reasoning paths\nand the ground truth answers. For sets containing the same num-\nber of reasoning paths, a higher F1 score indicates better quality.\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\na).3 For the GraphRAG instance, we adopt Hits@1 as the main eval-\nuation metric, following prior work [3, 50, 62, 63, 75, 98]. It measures\nthe proportion of outputs that exactly match the ground truth.13 We\nalso report two commonly used complementary metrics—F1 score\nand LLM-based evaluation. Both show similar trends to Hits@1\nacross different GraphRAG instances and do not affect our experi-\nmental conclusions.14 b) Efficiency. We record the runtime of both\nmodules in the GraphRAG instance per query. c) Cost. We track\ntoken cost and peak GPU memory usage of the GraphRAG instance\non EEMs and LLMs, reflecting processing load and computational\nresource demands, respectively.\nSettings for Graph Instance Experiments. For GraphRAG in-\nstances in this study, we selected methods and semantic models that\nbalance efficiency and quality within their respective categories,\nbased on experiments on subgraph-extraction and path-retrieval\nmodules (Sections 4.3 and 4.4). To ensure fair comparisons, we\nadjusted parameters within each instance so that all instances ulti-\nmately retrieved the same number of reasoning paths.15 The key\nexperimental settings are as follows, and detailed settings for each\ninstance are provided in supplemental material: a) EEMs&LLMs.\nA sentence-Transformer (ST) [111] and Qwen2-72B [117] are em-\nployed as the EEMs and LLMs implementations, respectively. Note\nthat for all experiments in this paper, the batch size of EEMs is set\nto 64 and the max input token of LLMs is set to 16k. b) SBE. The\nPPR algorithm is applied, with a maximum of nodes 𝑁𝑝𝑝𝑟= 1, 000\nretained. c) SAE. Edge pruning (EP) is used for EEMs and LLMs\nbased on subgraphs extracted by PPR, with a maximum of edges\n𝑁𝑒= 64 retained. d) SBR. The Dijkstra algorithm is employed for\nSPR implementation. e) OSAR. Select 𝑁𝑝= 32 reasoning paths from\nthose retrieved using SPR, leveraging both EEMs and LLMs. f) ISAR.\nThe Beam Search algorithm is selected for implementation and com-\nbined with both EEMs and LLMs. The beam width is set to 𝐵= 8,\nand the maximum path length retained is 𝐿= 4. g) Generation. We\nutilize various LLMs with differing architectures and scales .16\nSettings for Module Experiments. To evaluate the quality and\nefficiency of methods in subgraph-extraction and path-retrieval\nmodules, we implement representative methods from each method\ncategory, and conduct independent experiments. The settings are\nas follows: a) EEMs&LLMs Selection. A variety of EEMs, including\nstatistical model, embedding model, and reranker, are employed.\n12Detailed strategies are available in our technical report B.5.\n13Note the gap between the evaluation of path-retrieval and that of the GraphRAG\ninstance, which arises from the ability of LLMs to utilize the reasoning paths.\n14Detailed results are provided in the technical report B.4.\n15In this paper, we follow prior work [45, 71, 77] by setting the number of reasoning\npaths to 32. Note that for instances integrating LLMs in OSAR and ISAR, the number\nof reasoning paths may be fewer than 32 due to the uncertainty in their outputs.\n16Glm4-9B [29], Llama3.3-70B [103], Qwen2-7B and Qwen2-72B [117]\nSpecifically, we adopt BM25 as a representative statistical model,\na sentence-Transformer (ST) [111] as a widely used embedding\nmodel, BGE-Reranker (BGE) [13] as an effective re-ranker, and\nQwen2-72B [117] as the LLM implementation.\nb) Settings for the Subgraph-Extraction Module.⁓⁓⁓⁓⁓⁓⁓\nb).1 SBE. We em-\nploy PPR as a structural importance-based method, RW as a random\nwalk-based algorithm, and KSE as a neighborhood-based strategy.\n⁓⁓⁓⁓⁓⁓\nb).2 SAE. Node Pruning (NP), Edge Pruning (EP), and Triple Prun-\ning (TP) are applied across all selected EEMs and LLMs, based on\nsubgraphs extracted by SBE, with PPR selected due to its superior\nquality and efficiency in our evaluations.\nc) Settings for the Path-Retrieval Module. We extract 3,000 sub-\ngraphs from four evaluation datasets using methods implemented\nin the subgraph-extraction module. From each dataset, 250 test\nqueries (1,000 total) are sampled, and three subgraphs per query\nare generated using SBE, SAE-EEM, and SAE-LLM, with the most\neffective implementations selected based on our experiments. These\nsubgraphs are used to evaluate the methods in the path-retrieval\nmodule independently. The settings for each method are as:⁓⁓⁓⁓⁓⁓\nc).1 SBR.\nThe Dijkstra algorithm is employed for SPR, while the DFS algo-\nrithm is employed for EPR. ⁓⁓⁓⁓⁓⁓⁓⁓\nc).2 OSAR. We use all selected EEMs\nand LLMs to perform semantic evaluation and refinement on the\npaths obtained through SPR (a superior SBR method based on our\nevaluation). ⁓⁓⁓⁓⁓⁓⁓\nc).3 ISAR. We combine beam search algorithm (𝐵= 8\nand 𝐿= 4) with all selected EEMs and LLMs.\n4.2\nEvaluation of GraphRAG Instances\nWe first present the results of all GraphRAG instances across various\nmetrics and analyze key observations.17\na) Reasoning Performance. As shown in Figure 2, the reasoning\nperformance of different GraphRAG instances across four datasets\nfollows a consistent trend. With the increase in reasoning model\nsize and capabilities (i.e., from 7B LLMs to 72B LLMs), all instances\nshow significant performance improvements. Next, we analyze the\ninstances according to the groups defined in Table 2.\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\na).1 Structural Methods on Both Modules (Group (I)). Instance\nNo.1 of Group (I) exhibits the worst overall performance. For ex-\nample, its Hits@1 is only 0.49 (WebQSP with Qwen2-7B), while\nmost other instances are close to or exceed 0.6. This indicates that\nstructure-based methods alone yield baseline solutions, necessitat-\ning semantic models for further improvement. Notably, Instance\nNo.1 outperforms certain semantically augmented instances on spe-\ncific datasets, such as No.6, No.8, and No.14 (CWQ with Llama3.3-\n70B). It shows that while semantic augmented methods generally\nperform better, exceptions depend on query complexity, domain,\nand integration of semantic models, as discussed later.\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\na).2 Semantic Augmentation on Both Modules (Group (II)). In\nGroup (II), instances using EEMs for subgraph-extraction (e.g., No.2,\n3, 6, 7) consistently outperform those using LLMs (e.g., No.4, 5, 8, 9)\nacross GrailQA, and all reasoning models. This indicates that LLMs\nare unsuitable for subgraph-extraction when both modules are\nsemantically augmented, as they may overly prune graph informa-\ntion, losing critical intermediate information in the reasoning paths,\na limitation seen in instances like GCR [72], which compromises\nstability and practicality.\n17We supplement the evaluation with a case study comparing representative reasoning\npaths and answers (see technical report B.12).\n\n\n--- Page 9 ---\n\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nHit@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nHit@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nHit@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\nQwen2-7B\n0.3\n0.5\n0.7\nHit@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\nGlm4-9B\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\nQwen2-72B\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\nLlama3.3-70B\n0.3\n0.5\n0.7\nNo.\nNo.1:SBE-PPR+SBR-SPR\nNo.2:SAE-EEMs+OSAR-EEMs\nNo.3:SAE-EEMs+OSAR-LLMs\nNo.4:SAE-LLMs+OSAR-EEMs\nNo.5:SAE-LLMs+OSAR-LLMs\nNo.6:SAE-EEMs+ISAR-EEMs\nNo.7:SAE-EEMs+ISAR-LLMs\nNo.8:SAE-LLMs+ISAR-EEMs\nNo.9:SAE-LLMs+ISAR-LLMs\nNo.10:SAE-EEMs+SBR-SPR\nNo.11:SAE-LLMs+SBR-SPR\nNo.12:SBE-PPR+OSAR-EEMs\nNo.13:SBE-PPR+OSAR-LLMs\nNo.14:SBE-PPR+ISAR-EEMs\nNo.15:SBE-PPR+ISAR-LLMs\nGRAG-M\n(a) CWQ\n(b) WebQSP\n(c) GrailQA\n(d) WebQuestions\nFigure 2: Reasoning Results of LEGO-GraphRAG Instances Across Four Datasets\n1\n23456789\n10\n11\n12\n13\n14\n15\nGRAG-M\nQwen2-7B\n0.3\n0.5\n0.7\nHits@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\nGRAG-M\nGlm4-9B\n0.3\n0.5\n0.7\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\nGRAG-M\nQwen2-72B\n0.3\n0.5\n0.7\nNo.\nFigure 3: Results of LEGO-GraphRAG Instances and GraphRAG-M Instance on MetaQA Dataset\n1\n23456789\n10\n11\n12\n13\n14\n15\n0\n50\nMemory (GB)\n▲\n▲\n▲\n▲\n▲\n▲\nNo.\n▲:<1GB\nFigure 4: Peak GPU Memory Usage\n1\n23456789\n10\n11\n12\n13\n14\n15\n40\n90\n140\n190\n240\nTime (s)\n(a) CWQ\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n40\n90\n140\n190\n240\n(b) WebQSP\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n40\n90\n140\n190\n240\n(c) GrailQA\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n40\n90\n140\n190\n240\n(d) WebQuestions\nNo.\nSubgraph-Extraction Time\nPath-Retrieval Time\n   \nFigure 5: Runtime of SE and PR Modules for GraphRAG Instances\n1\n23456789\n10\n11\n12\n13\n14\n15\n20\n25\n210\n215\n220\nToken\n(a) CWQ\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n20\n25\n210\n215\n220\n(b) WebQSP\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n20\n25\n210\n215\n220\n(c) GrailQA\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n20\n25\n210\n215\n220\n(d) WebQuestions\nNo.\nEnd-to-End Models(EEMs)\nLarge Language Models(LLMs)\nFigure 6: Token Costs for EEMs and LLMs in GraphRAG Instances\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Hop=1 (46.69%)\n0.4\n0.6\nHits@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Hop=2 (28.85%)\n0.4\n0.6\nHits@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Hop>2 (24.46%)\n0.4\n0.6\nHits@1\nNo.\n(a) Queries with Varied Number of Reasoning Hops\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Ans=1 (65.69%)\n0.2\n0.4\nF1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Ans=2 (9.48%)\n0.2\n0.4\nF1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Ans>2 (24.83%)\n0.2\n0.4\nF1\nNo.\n(b) Queries with Varied Number of Answers\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Ent=1 (85.26%)\n0.4\n0.6\nHits@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Ent=2 (13.39%)\n0.4\n0.6\nHits@1\nNo.\n1\n23456789\n10\n11\n12\n13\n14\n15\n#Ent>2 (1.35%)\n0.4\n0.6\nHits@1\nNo.\n(c) Queries with Varied Number of Entities\nFigure 7: Instance Performance w.r.t. Queries\nAlso, Instances No.2 and 3 in Group (II) achieve the best overall\nperformance, consistently ranking in the top-3 in all reasoning\nmodels (WebQSP). This highlights the effectiveness of combining\nEEMs for subgraph-extraction with OSAR for path-retrieval, a strat-\negy yet unexplored by existing methods like GSR [45], which pairs\nEEMs with structure-based path-retrieval. Notably, among Group\n(II) instances using EEMs for subgraph-extraction (i.e., No.2, 3, 6,\n7), Instance No.6 performs the worst. For example, its performance\nis similar to that of Instance No.1, which uses only structure-based\nmethods (CWQ with all reasoning models). This suggests that ISAR\npairs better with LLMs than EEMs, as EEMs’ weaker semantics\ncan yield noisy scores on long paths and small score gaps, making\npruning unreliable. One way to improve ISAR-EEMs is to enhance\nthe EEMs themselves, for example, through customization or fine-\ntuning for specific domains or graphs. In addition, we propose two\n\n\n--- Page 10 ---\n\n0\n1000\n3000\n5000\n1.2e6\n2.7e7\n1.2e8  \n0.00\n0.02\n0.04\n0.06\nF1\n0\n1000\n3000\n5000\n1.2e6\n2.7e7\n1.2e8  \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nPPR\nRW\nKSE-1\nKSE-2\nKSE-3\nKSE-4\nSubgraph Size (Number of the Tripes)\nFigure 8: Results w.r.t. Subgraph Size (SBE)\n24\n25\n28\n29\n210\n211\n212\n0\n0.01\n0.02\n0.03\n0.06\n0.18\n0.20\nF1\n24\n25\n28\n29\n210\n211\n212\n0.4\n0.5\n0.6\n0.7\n0.8\nRecall\nSubgraph Size (Number of the Tripes)\nLLM-NP\nLLM-EP\nLLM-TP\nST-NP\nST-EP\nST-TP\nBGE-NP\nBGE-EP\nBGE-TP\nBM25-NP\nBM25-EP\nBM25-TP\nFigure 9: Results w.r.t. Subgraph Size (SAE)\n210\n211\n212\n213\n214\n0.1\n0.2\nF1\n210\n211\n212\n213\n214\n0.4\n0.6\nRecall\nToken Num.\nNP\nEP\nTP\nFigure 10: Results w.r.t. Token Cost (SAE-LLMs)\ncomplementary strategies: triple-level scoring, which evaluates\nonly the newly added step at each expansion to reduce error ac-\ncumulation, and dynamic pruning, which adjusts the number of\nretained paths based on score distribution, helping preserve good\ncandidates when scores are close.18\n⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓⁓\na).3 Semantic Augmentation on one Module (Group (III)&(IV)).\nWe observe that the performance of instances in Group IV (i.e.,\nInstances No.12-15) generally surpasses that of Group III (i.e., In-\nstances No.10, 11). This suggests that if only one module is se-\nmantically augmented, prioritizing the path-retrieval module over\nthe subgraph-extraction module is more effective. Using structure-\nbased methods for subgraph extraction remains efficient and suf-\nficient, unlike existing instances such as RoG [71] and GCR [72],\nwhich use LLMs (Llama-2-7b, Llama-3.1-8b) for subgraph-extraction\nand encounter performance and efficiency bottlenecks. Addition-\nally, Group (II) instances do not consistently outperform Groups\n(III) and (IV), implying that semantically augmenting both modules\nis unnecessary in resource-limited contexts. Moreover, as shown\nin Figure 3, results on MetaQA align with findings from Freebase-\nbased datasets. The small scale and single-domain nature of the\ngraph lead to uniformly high scores across all instances. Notably,\nInstance No.14 (ISAR-EEMs) performs comparably to or slightly\nbetter than Instance No.15 (ISAR-LLMs), suggesting that EEMs can\nbe a viable and more efficient alternative to LLMs in such settings.\nb) Runtime. Figure 5 shows the average runtime of different\nGraphRAG instances for subgraph-extraction and path-retrieval\nmodules on a single query. A key observation is that subgraph-\nextraction is the primary bottleneck in GraphRAG runtime of all\ninstances. For example, Instance No.1, using structure-based meth-\nods, requires approximately 70 seconds for subgraph-extraction\n(i.e., PPR algorithm) but less than 0.1 seconds for path-retrieval.\nSince subgraph-extraction is a crucial but time-intensive step in the\nGraphRAG workflow, it poses significant challenges for the practi-\ncal development of GraphRAG systems on large-scale graphs like\nFreebase. Unfortunately, most existing instances, such as KELP [65],\nToG [98], and DoG [74], overlook the efficiency of this process.\nAdditionally, in the path-retrieval, methods using LLMs (espe-\ncially under ISAR) exhibit longer runtimes compared to those using\nEEMs. For example, Instance No.12 (OSAR-EEMs) and No.14 (ISAR-\nEEMs) complete path-retrieval in under 1.15 seconds and 0.19 sec-\nonds, respectively, while Instance No.13 (OSAR-LLMs) and No.15\n(ISAR-LLMs) take 66.64 seconds and 122.43 seconds, respectively,\non GrailQA. This shows that using the ISAR-LLMs methods in the\npath-retrieval may result in unacceptably low query efficiency.\nc) Cost. Figures 6 and 4 show the average token overhead (EEMs\nand LLMs) and peak GPU memory usage (average of four datasets)\n18Detailed discussion and results are available in our technical report B.8.\nacross GraphRAG instances. Instances mixing EEMs and LLMs (e.g.,\nNo.3, 4, 7, 8) incur higher token costs than those using the same\nmodel type (e.g., No.2, 5, 6, 9), with EEM-only setups being the most\neconomical. GPU usage remains low (<1GB) for EEM-only instances\nbut rises sharply for LLM-based ones, peaking at 80GB when LLMs\nare used in both modules (e.g., No.5, 9). The results underscore\nthe performance–cost trade-offs of semantic augmentation and the\nimportance of corresponding optimization.\nd) Analysis by Query Type. We analyze performance across dif-\nferent query types by varying reasoning hops, number of answers,\nand number of query entities. As shown in Figure 7a, performance\nconsistently declines with more reasoning hops, especially for\nstructure-based methods (e.g., Instance No.1), which struggle on\nmulti-hop queries. Figure 7b shows a similar drop from single- to\nmulti-answer queries, where semantic methods are more resilient.\nFigure 7c shows improved performance with more query entities,\nlikely due to increased chances of retrieving relevant paths—even\nbenefiting structure-only instances like No.1.\ne) Integrating Microsoft GraphRAG. To evaluate compatibility with\nMicrosoft GraphRAG, we implement an instance (GRAG-M), which\nfollows its workflows: performing community detection on the\ngraph and precomputing textual summaries for each community.\nOn the MetaQA dataset, GRAG-M uses the reasoning paths from\nInstance 12 (which showed strong performance in our experiments)\nand provides both the paths and summaries as input to the LLMs.\nAs shown in Figure 3, GRAG-M performs well on Qwen2-7B and\nGlm4-9B. Gains diminish with Qwen2-72B, where the larger model\nalready captures sufficient context, reducing the benefit of precom-\nputed summaries.\n4.3\nEvaluation of Subgraph-Extraction Module\nStructure-Based Extraction (SBE). Figure 8 shows the variation\nin F1 score and Recall for the three SBE methods (RW, PPR, and\nKSE) as the extracted subgraph size changes. The F1 score for all\nmethods decreases as the subgraph size increases, while their Recall\nconsistently improves. Across all subgraph sizes, PPR consistently\noutperforms RW in both F1 score and Recall, demonstrating the\nsuperior effectiveness of PPR over RW. Additionally, RW strug-\ngles to achieve a Recall above 60%, rendering it nearly unusable\nin GraphRAG. Moreover, PPR offers greater flexibility compared\nto KSE, enabling precise control over subgraph size through hy-\nperparameters. In contrast, KSE relies on hierarchical extraction,\nwhich lacks such flexibility.19 These advantages establish PPR as the\ncurrent optimal solution for SBE. As Table 4 demonstrates, PPR’s\ncomputational costs become prohibitive for large graphs.\n19Many prior studies [45, 50, 71, 72] first apply KSE to extract 2-hop or 4-hop subgraphs,\nfollowed by PPR to distill them into smaller, query-relevant subgraphs.\n\n\n--- Page 11 ---\n\nTable 4: PPR Runtime w.r.t. Graph\nSize\nNodes\nAve. Edges\nAve. Time (s)\n100000\n431414.5\n0.58\n1000000\n5567538.1\n1.37\n10000000\n48865879.2\n11.5\n100176641\n298458255\n75.29\nSBR\nOSAR-EEMS\nOSAR-LLMs\nISAR-EEMS\nISAR-LLMs\n0.1\n0.2\n0.3\n0.4\nF1\nSPR\nEPR\nBM25\nST\nBGE\nLLM\nFigure 11: F1 Score of Meth-\nods in Path-Retrieval Module\nSemantic-Augmented Extraction (SAE). Figure 9 shows the\nRecall and F1 score of different semantic model combinations with\nthree subgraph pruning strategies under the SAE methods, as the\nsubgraph size varies. Among the three semantic models, the BGE\nand ST significantly surpass BM25 across all pruning strategies\nand subgraph scales. BGE is slightly better in quality, while ST is\neven better in efficiency, as discussed later. Considering the trade-\noff between quality and efficiency, we propose to use ST as the\nrepresentative EEMs. For the three pruning strategies, TP exhibits\nthe best quality at smaller subgraph scales, while NP performs the\nworst. However, as subgraph size increases, the F1 score and Recall\nof EP, TP, and NP converge. Ultimately, EP slightly outperforms\nTP, while TP marginally surpasses NP. Thus, TP is recommended\nwhen the size of the extracted subgraph is small; otherwise, EP.\nDue to the inherent uncertainty in LLM outputs, SAE-LLMs meth-\nods often struggle to control the size of subgraphs extracted, typi-\ncally producing smaller subgraphs that can reduce path-retrieval\nquality (see Finding 2). Figure 9 shows the F1 score and Recall for\nsubgraphs generated using SAE-LLMs under three pruning strate-\ngies, plotted against the average subgraph size. Among the three\npruning strategies, EP outperforms NP and TP for SAE-LLMs. Fig-\nure 10 further shows the trends in F1 score and Recall as the number\nof tokens input into the LLMs varies for NP, TP and EP. Recall for\nNP consistently surpasses that of both TP and EP, maintaining a\ngap of around 0.2. Although EP underperforms in terms of F1 score\ndue to the larger subgraphs it extracts, the relatively low Recall of\nNP and EP may diminish their effectiveness in path-retrieval. As\nthe number of tokens increases, Recall for NP and TP initially rises,\nthen declines, while EP remains nearly constant, indicating that\nEP achieves more effective subgraph extraction at a lower token\ncost for the SAE-LLMs methods. The efficiency of SAE methods\nis related to the semantic models used. Runtime is influenced by\nfactors such as model parameters, algorithmic complexity, and data\nvolume. In general, the runtime for processing a graph component\nfollows this order: LLMs > BGE > ST > BM25, as shown in Figure 13.\nTable 5: Results of Hybrid ISAR Combining EEMs and LLMs\nMethod\nCWQ\nWebQSP\nGrailQA\nWebQuestion\nSBE+ISAR-EEMs\n0.183\n0.157\n0.395\n0.16\nSBE+ISAR-EEMs (LLMs-R)\n0.271\n0.219\n0.379\n0.175\nSBE+ISAR-LLMs\n0.249\n0.264\n0.371\n0.249\nSBE+ISAR-LLMs (EEMs-PF)\n0.271\n0.316\n0.396\n0.245\nSBE+ISAR-LLMs (EEMs-S)\n0.310\n0.373\n0.435\n0.276\n4.4\nEvaluation of Path-Retrieval Module\nFigure 11 shows the F1 score of different methods in path-retrieval\nmodule. It shows that OSAR outperforms ISAR in all semantic\nmodels. SPR method slightly outperforms EPR and I/OSAR with\nBM25, but underperforms all I/OSAR solutions with ST, BGE or\n1\n4\n8\n16\n32\nPath Num.\n0.1\n0.2\n0.3\n0.4\nF1\nSBR-EPR\nSBR-SPR\nOSAR-BGE\nOSAR-BM25\nOSAR-ST\nISAR-BGE\nISAR-BM25\nISAR-ST\nFigure 12: F1 Score vs. # of Paths\n(for Methods of Path-Retrieval)\nSBR\nOSAR-EEMs\nOSAR-LLMs\nISAR-EEMs\nISAR-LLMs\n0\n1\n2\n3\n4\n10\n40\n80\nTime(s)\n0.007\nSPR\nEPR\nBM25\nST\nBGE\nLLM\nFigure 13: Runtime of Meth-\nods in Path-Retrieval (WebQSP)\nLLM, which suggests that the use of poorer semantic models to\naid enhanced retrieval may lead to a loss of quality. We further\ninvestigate the variation in the F1 score of the path-retrieval mod-\nule when the number of reasoning paths changes, as shown in\nFigure 12. Since the number of paths output by LLM is inherently\nuncertain, methods using LLM are excluded. It shows that the F1\nscore of SBR and I/OSAR with BM25 gradually increases with the\nnumber of paths, yet their F1 score remains low-level, consistently\ntrailing I/OSAR with BGE and ST by approximately 0.1. The gap\nwidens to 0.2 or more when fewer paths are retrieved. In contrast,\nthe performance of I/OSAR with ST and BGE initially improves\nand then declines as the number of retained paths increases, with\nOSAR generally outperforming ISAR. Note that F1 scores may be\nhigher for smaller sets of reasoning paths, as they tend to prioritize\nprecision. A slight decrease in F1 for larger reasoning path sets is\nnot indicative of lower quality but rather reflects the broader scope\nof the retrieved paths.\nFigure 13 shows the runtime of different methods in the path-\nretrieval module (WebQSP). Among the SBR solutions, EPR has the\nlongest runtime due to the large number of paths it retrieves, while\nSPR reduces computation time by retrieving only a single path\nfor each node. The EPR-based OSAR method becomes impractical\nbecause its excessive data volume results in prohibitive runtime\noverhead when used with semantic models. As a result, only SPR\nis employed in this context. Among the three semantic models,\nboth BGE and ST demonstrate significant quality improvements\nover BM25 combined with I/OSAR. BGE achieves marginally higher\nquality, whereas ST exhibits superior efficiency.\nTable 5 reports the results of hybrid ISAR methods combining\nEEMs and LLMs. All three methods outperform single-model base-\nlines on most datasets, confirming the benefit of combining model\nstrengths. Notably, the EEMs-S strategy yields the best results, sug-\ngesting that combining EEMs and LLMs helps reduce the risk of\nmissing important paths during expansion.\n5\nFINDINGS AND DISCUSSION\nWe summarize key findings from our empirical study and discuss\npromising ways for addressing GraphRAG’s two key bottlenecks:\nsubgraph-extraction efficiency and LLM generation quality.\n5.1\nSummary of Key Findings\nFinding 1: GraphRAG Trade-off Structure. GraphRAG entails\nmulti-aspect trade-offs among quality (Hits@1), efficiency (run-\ntime), and cost (token and GPU usage). This trade-off spans two\nmodules (subgraph-extraction vs. path-retrieval), two method types\n(structure-based vs. semantic-augmented), and two types of seman-\ntic models (EEMs vs. LLMs).\n\n\n--- Page 12 ---\n\nFinding 2: Module-level Trade-offs. Subgraph-extraction is\nthe main efficiency bottleneck for large-scale graphs. Overuse of se-\nmantic augmentation (e.g., using LLMs) in subgraph-extraction can\ncompromise downstream path-retrieval quality. Targeting semantic\naugmentation in the path-retrieval module is a more cost-effective\nstrategy to maintain high quality.\nFinding 3: Method-level Trade-offs. Structure-based methods\noffer higher efficiency and lower cost, but semantic-augmented\nmethods are essential for quality, especially for complex (two- or\nmulti-hop) queries. One-way semantic methods (OSAR) balance\nquality and efficiency better than interactive methods (ISAR).\nFinding 4: Model-level Trade-offs. EEMs are more cost effec-\ntive in token and GPU usage, while LLMs generally provide better\nquality, particularly for interactive methods (ISAR). However, LLMs\ncan exhibit unstable generation in some cases, affecting consistency.\nA balanced approach includes applying LLM-based augmentation\nin path-retrieval or EEM-based augmentation in both modules.\nFinding 5: A Promising yet Underexplored Strategy. Based\non our analysis, the effective approach for GraphRAG would be the\none that combines structure based methods for subgraph-extraction\nwith OSAR for path-retrieval. Despite its potential, this strategy\nremains underexplored in existing works. Also, it opens up op-\nportunities for further optimizations for handling multi-hop and\nmulti-answer queries, as well as improving overall efficiency.\n5.2\nEfficiency of the Subgraph-Extraction\nWe explore promising directions for faster subgraph-extraction, sup-\nported by empirical analysis, including limitations, trade-offs, and\nresearch opportunities.20 Below, we briefly outline each direction.\nComputational acceleration yields significant efficiency im-\nprovement for SBE methods (e.g., PPR) through approximate and\ndistributed algorithms (Table 6). Approximate methods may lower\nRecall; distributed solutions depend on system setup.\nPrecomputation-based acceleration pre-generates subgraphs\n(e.g., via clustering or community detection) to scale down the\nsearch space for the extraction process in order to reduce run-\ntime overhead. As shown in Table 7, our implemented prototype\ndemonstrates the potential of this direction, but it also incurs pre-\nprocessing overhead and may limit the ability to access relevant\ninformation spread across different subgraphs.\nVector database-based acceleration encodes graph compo-\nnents (e.g., nodes or triples) into embedding vectors and stores them\nin a vector database, thus efficiently identifying query-relevant com-\nponents based on semantic similarity, followed by subgraph con-\nstruction centered on them. Table 7 shows our prototype achieves\nnotable speedups with high Recall. However, the retrieved compo-\nnents often form multiple weakly connected subgraphs, which may\nhinder multi-hop reasoning.\n5.3\nGeneration Quality of LLMs\nFor the issue of LLMs generating fewer or lower-quality compo-\nnents, we take the path-retrieval module as a example and explore\ntwo effective strategies: Multi-round LLMs (M-LLMs) conduct\n20See technical report B.7 for details.\nTable 6: Computational acceleration of PPR\nApproximate PPR (Freebase)\nDistributed PPR\nMethod\nRecall\nAve. Time (s)\nMethod\nSpeedup\nRBS [107]\n0.31\n0.51\nHGPA [31]\n3.4–4.1×\nFora [110]\n0.18\n7.61\nPAFO [109]\n58.7×\nTopPPR [113]\n0.44\n42.08\nDelta-Push [39]\n123–162×\nStandard PPR\n0.96\n75.29\nStandard PPR\n1× (baseline)\nTable 7: Performance and Cost Analysis of the SE Acceleration\nMethods on Freebase (About 100M Nodes, 300M Edges)\nMethod\nPre-time\nOnline-time\nRecall\nF1\nWCC\nPrecomputation\n19373s\n19.02s\n0.52\n0.0021\n1\nVector Database\n14570s\n0.85s\n0.65\n0.0023\n12.86\nStandard PPR\n0s\n75.29s\n0.96\n0.0038\n1\nTable 8: Performance of strategies designed to mitigate the limita-\ntions of LLM-generated outputs\nMethod\nCWQ\nWebQSP\nGrailQA\nWebQuestions\nSBE+OSAR-LLMs\n0.304\n0.401\n0.401\n0.328\nSBE+OSAR-LLMs (M-LLMs)\n0.349\n0.438\n0.381\n0.348\nSBE+OSAR-LLMs (EEMs-S)\n0.381\n0.427\n0.476\n0.353\niterative refinement via repeated LLM calls. EEMs supplementa-\ntion (EEMs-S) compensates for insufficient outputs by incorpo-\nrating top-ranked candidate paths from EEMs. Table 8 shows both\nstrategies improve performance across most datasets (applied in\nthe SBE+OSAR-LLMs instance), though with added computational\noverhead of multiple model calls and EEM-based ranking steps.21\n6\nCONCLUSION\nIn this paper, we introduce LEGO-GraphRAG, a unified framework\nfor the modular analysis and design of GraphRAG instances. By\ndividing the GraphRAG retrieval process into distinct modules and\nidentifying corresponding design solutions, LEGO-GraphRAG pro-\nvides a viable approach for building advanced GraphRAG systems.\nBuilding upon the LEGO-GraphRAG framework, we conduct exten-\nsive empirical studies on large-scale real-world graphs and diverse\nGraphRAG query sets, leading to key findings: 1) GraphRAG must\nbalance quality, efficiency, and cost across three aspects: modules,\nmethod types, and semantic models. 2) Extracting query-relevant\nsubgraphs is the primary efficiency bottleneck for GraphRAG on\nlarge-scale graphs and remains underexplored. 3) Graph structural\ninformation enables efficient solutions for GraphRAG, while se-\nmantic information is crucial for improving the quality of complex\nqueries. The optimal solution should integrate structural informa-\ntion to identify query-relevant subgraphs and leverage semantic\ninformation to retrieve reasoning paths.\nACKNOWLEDGMENTS\nThis work was supported in part by the National Natural Science\nFoundation of China under Grant 62472400, Grant 62072428, Grant\n62271465, in part by the Suzhou Basic Research Program under\nGrant SYG202338, and in part by the HK RGC grants 12202024,\nR1015-23, and C1043-24GF. Yukun Cao and Zengyi Gao contributed\nequally to this work. Xike Xie is the corresponding author.\n21More details and analysis are in technical report B.9.\n\n\n--- Page 13 ---\n\nREFERENCES\n[1] Ibrahim Abdelaziz, Razen Harbi, Zuhair Khayyat, and Panos Kalnis. 2017. A\nsurvey and experimental comparison of distributed SPARQL engines for very\nlarge RDF data. Proc. VLDB Endow. 10, 13 (Sept. 2017), 2049–2060.\n[2] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski,\nGeeta Chauhan, and et al. Chourdia. 2024. PyTorch 2: Faster Machine Learning\nThrough Dynamic Python Bytecode Transformation and Graph Compilation.\nIn 29th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 2 (ASPLOS 24). ACM.\n[3] Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented\nLanguage Model Prompting for Zero-Shot Knowledge Graph Question Answer-\ning. arXiv:2306.04136 [cs.CL]\n[4] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic\nParsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing(EMNLP 2013),\nDavid Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven\nBethard (Eds.). Association for Computational Linguistics, Seattle, Washington,\nUSA, 1533–1544.\n[5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet\nallocation. 3, null (March 2003), 993–1022.\n[6] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008. Freebase: a collaboratively created graph database for structuring human\nknowledge. In Proceedings of the 2008 ACM SIGMOD international conference on\nManagement of data(SIGMOD 2008). 1247–1250.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems(NeurIPS 2020) 33 (2020), 1877–1901.\n[8] Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, and S. Kevin Zhou.\n2024. GraphInsight: Unlocking Insights in Large Language Models for Graph\nStructure Understanding. arXiv:2409.03258 [cs.CL]\n[9] Dhivya Chandrasekaran and Vijay Mago. 2022. Evolution of Semantic Similarity\n- A Survey. ACM Comput. Surv. 54, 2 (2022), 41:1–41:37.\n[10] Harrison Chase. 2022. LangChain. https://github.com/langchain-ai/langchain\n[11] Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, and Jieping Ye. 2024.\nSAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors\nfor Domain Knowledge Graph. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics(ACL 2024) (Volume 1: Long Pa-\npers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\nComputational Linguistics, Bangkok, Thailand, 4345–4360.\n[12] Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko,\nStylianos I Venieris, and Hongxiang Fan. 2024. Hardware-aware parallel prompt\ndecoding for memory-efficient acceleration of llm inference. arXiv preprint\narXiv:2405.18628 (2024).\n[13] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and\nZheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality,\nMulti-Granularity Text Embeddings Through Self-Knowledge Distillation.\narXiv:2402.03216 [cs.CL]\n[14] Yongrui Chen, Huiying Li, Guilin Qi, Tianxing Wu, and Tenggou Wang. 2022.\nOutlining and Filling: Hierarchical Query Graph Generation for Answering\nComplex Questions over Knowledge Graphs. arXiv:2111.00732 [cs.AI]\n[15] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and\nJiaya Jia. 2023. Longlora: Efficient fine-tuning of long-context large language\nmodels. arXiv preprint arXiv:2309.12307 (2023).\n[16] Zi-Yuan Chen, Chih-Hung Chang, Yi-Pei Chen, Jijnasa Nayak, and Lun-Wei\nKu. 2019. UHop: An Unrestricted-Hop Relation Extraction Framework for\nKnowledge-Based Question Answering. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein,\nChristy Doran, and Thamar Solorio (Eds.). Association for Computational Lin-\nguistics, Minneapolis, Minnesota, 345–356.\n[17] Gábor Csárdi and Tamás Nepusz. 2006. The igraph software package for complex\nnetwork research. InterJournal, Complex Systems (2006), 1695.\n[18] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer,\nand Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of\nthe American Society for Information Science 41, 6 (1990).\n[19] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.\nQlora: Efficient finetuning of quantized llms, 2023. 2 (2023).\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]\n[21] Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin.\n2024. Don’t Forget to Connect! Improving RAG with Graph-based Reranking.\narXiv:2405.18414 [cs.CL]\n[22] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva\nMody, Steven Truitt, and Jonathan Larson. 2024. From Local to Global: A Graph\nRAG Approach to Query-Focused Summarization. arXiv:2404.16130 [cs]\n[23] Wenfei Fan. 2022. Big graphs: challenges and opportunities. Proc. VLDB Endow.\n15, 12 (Aug. 2022), 3782–3797.\n[24] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,\nTat-Seng Chua, and Qing Li. 2024. A survey on rag meeting llms: Towards\nretrieval-augmented large language models. In Proceedings of the 30th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining(KDD 2024). 6491–\n6501.\n[25] Yasuhiro Fujiwara, Makoto Nakatsuji, Makoto Onizuka, and Masaru Kitsure-\ngawa. 2012. Fast and exact top-k search for random walk with restart. 5, 5 (Jan.\n2012), 442–453.\n[26] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot\nDense Retrieval without Relevance Labels. arXiv:2212.10496 [cs]\n[27] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi\nDai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented\nGeneration for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]\n[28] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. 2024. Modular rag:\nTransforming rag systems into lego-like reconfigurable frameworks. arXiv\npreprint arXiv:2407.21059 (2024).\n[29] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego\nRojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, and\nJiadai Sun et al. 2024. ChatGLM: A Family of Large Language Models from\nGLM-130B to GLM-4 All Tools. arXiv:2406.12793\n[30] Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu\nSu. 2021. Beyond I.I.D.: Three Levels of Generalization for Question Answering\non Knowledge Bases. In Proceedings of the Web Conference 2021 (Ljubljana,\nSlovenia) (WWW 2021). Association for Computing Machinery, New York, NY,\nUSA, 3477–3488.\n[31] Tao Guo, Xin Cao, Gao Cong, Jiaheng Lu, and Xuemin Lin. 2017. Distributed\nAlgorithms on Exact Personalized PageRank. In Proceedings of the 2017 ACM\nInternational Conference on Management of Data (SIGMOD 2017). Association\nfor Computing Machinery, New York, NY, USA, 479–494.\n[32] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG:\nSimple and Fast Retrieval-Augmented Generation. arXiv:2410.05779 [cs.IR]\n[33] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su.\n2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large\nLanguage Models. arXiv:2405.14831 [cs.CL]\n[34] Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua,\nBo Long, Hui Liu, and Jiliang Tang. 2025. RAG vs. GraphRAG: A Systematic\nEvaluation and Key Insights. arXiv:2502.11371 [cs.IR]\n[35] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei,\nMahantesh Halappanavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng\nTang, et al. 2025. Retrieval-augmented generation with graphs (graphrag).\narXiv preprint arXiv:2501.00309 (2025).\n[36] Mohamed S. Hassan, Walid G. Aref, and Ahmed M. Aly. 2016. Graph Indexing\nfor Shortest-Path Finding over Dynamic Sub-Graphs. In Proceedings of the 2016\nInternational Conference on Management of Data (San Francisco, California,\nUSA) (SIGMOD 2016). Association for Computing Machinery, New York, NY,\nUSA, 1183–1197.\n[37] Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of the 11th\nInternational Conference on World Wide Web (Honolulu, Hawaii, USA) (WWW\n2002). Association for Computing Machinery, New York, NY, USA, 517–526.\n[38] Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021.\nImproving Multi-hop Knowledge Base Question Answering by Learning In-\ntermediate Supervision Signals. In Proceedings of the 14th ACM International\nConference on Web Search and Data Mining (Virtual Event, Israel) (WSDM 2021).\nAssociation for Computing Machinery, New York, NY, USA, 553–561.\n[39] Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, and Zhewei Wei.\n2023. Personalized PageRank on evolving graphs with an incremental index-\nupdate scheme. Proceedings of the ACM on Management of Data(SIGMOD 2023)\n1, 1 (2023), 1–26.\n[40] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large\nlanguage models. ICLR 1, 2 (2022), 3.\n[41] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. 2024.\nA Survey of Knowledge Enhanced Pre-Trained Language Models. IEEE Trans-\nactions on Knowledge and Data Engineering 36, 4 (2024), 1413–1430.\n[42] Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao.\n2024. GRAG: Graph Retrieval-Augmented Generation. arXiv:2405.16506 [cs]\n[43] Jiewen Huang, Daniel J. Abadi, and Kun Ren. 2011. Scalable SPARQL querying\nof large RDF graphs. Proc. VLDB Endow. 4, 11 (Aug. 2011), 1123–1134.\n[44] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian\nWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2023. A Survey on Hallucination in Large Language Models: Principles,\nTaxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL]\n[45] Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella\nLapata, and Jeff Z. Pan. 2024. Less is More: Making Smaller Language Models\nCompetent Subgraph Retrievers for Multi-hop KGQA. arXiv:2410.06121 [cs.CL]\n\n\n--- Page 14 ---\n\n[46] Hugging Face. 2025. Hugging Face: The AI community building the future.\nAccessed: 2025-01-01.\n[47] Erik Johannes Husom, Arda Goknil, Merve Astekin, Lwin Khin Shar, Andre\nKåsen, Sagar Sen, Benedikt Andreas Mithassel, and Ahmet Soylu. 2025. Sustain-\nable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Effi-\nciency, Output Accuracy, and Inference Latency. arXiv preprint arXiv:2504.03360\n(2025).\n[48] Paul Jaccard. 1912. THE DISTRIBUTION OF THE FLORA IN THE ALPINE\nZONE. New Phytologist 11, 2 (1912), 37–50.\n[49] Patrick Jaillet, Jiashuo Jiang, Chara Podimata, and Zijie Zhou. 2025. Online\nScheduling for LLM Inference with KV Cache Constraints. arXiv preprint\narXiv:2502.07115 (2025).\n[50] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen.\n2023. StructGPT: A General Framework for Large Language Model to Reason\nover Structured Data. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing(EMNLP 2023), Houda Bouamor, Juan Pino, and\nKalika Bali (Eds.). Association for Computational Linguistics, Singapore, 9237–\n9251.\n[51] Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2022. UniKGQA: Uni-\nfied Retrieval and Reasoning for Solving Multi-hop Question Answering Over\nKnowledge Graph. In The Eleventh International Conference on Learning Repre-\nsentations.\n[52] Sylvio Barbon Junior, Paolo Ceravolo, Sven Groppe, Mustafa Jarrar, Samira\nMaghool, Florence Sèdes, Soror Sahri, and Maurice van Keulen. 2024. Are Large\nLanguage Models the New Interface for Data Pipelines?. In Proceedings of the In-\nternational Workshop on Big Data in Emergent Distributed Environments(BiDEDE\n2024), Santiago, Chile, June 9-15, 2024, Philippe Cudré-Mauroux, Andrea Kö,\nand Robert Wrembel (Eds.). ACM, 6:1–6:6.\n[53] Arijit Khan, Sourav S. Bhowmick, and Francesco Bonchi. 2017. Summarizing\nstatic and dynamic big graphs. Proc. VLDB Endow. 10, 12 (Aug. 2017), 1981–1984.\n[54] Arijit Khan and Sameh Elnikety. 2014. Systems for big-graphs. Proc. VLDB\nEndow. 7, 13 (Aug. 2014), 1709–1710.\n[55] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\nSearch via Contextualized Late Interaction over BERT. arXiv:2004.12832 [cs]\n[56] Hanieh Khorashadizadeh, Fatima Zahra Amara, Morteza Ezzabady, Frédéric\nIeng, Sanju Tiwari, Nandana Mihindukulasooriya, Jinghua Groppe, Soror Sahri,\nFarah Benamara, and Sven Groppe. 2024. Research Trends for the Interplay be-\ntween Large Language Models and Knowledge Graphs. arXiv:2406.08223 [cs.AI]\n[57] Richard E. Korf. 1985. Depth-First Iterative-Deepening: An Optimal Admissible\nTree Search. Artif. Intell. 27 (1985), 97–109.\n[58] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nMemory Management for Large Language Model Serving with PagedAtten-\ntion. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems\nPrinciples(SOSP 2023).\n[59] Yunshi Lan and Jing Jiang. 2020. Query Graph Generation for Answering\nMulti-hop Complex Questions from Knowledge Bases. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics(ACL 2020), Dan\nJurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for\nComputational Linguistics, Online, 969–974.\n[60] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL]\n[61] Mufei Li, Siqi Miao, and Pan Li. 2024. Simple Is Effective: The Roles of Graphs\nand Large Language Models in Knowledge-Graph-Based Retrieval-Augmented\nGeneration. arXiv:2410.20724\n[62] Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu Chen. 2023.\nFew-shot In-context Learning on Knowledge Base Question Answering. In\nProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics(ACL 2023) (Volume 1: Long Papers), Toronto, Canada, July 9-14, 2023,\nAnna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association\nfor Computational Linguistics, 6966–6980.\n[63] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Sou-\njanya Poria, and Lidong Bing. 2024. Chain-of-Knowledge: Grounding Large Lan-\nguage Models via Dynamic Knowledge Adapting over Heterogeneous Sources.\narXiv:2305.13269 [cs]\n[64] Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling\nZhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin\nYuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen,\nWenguang Chen, and Jun Zhou. 2024. KAG: Boosting LLMs in Professional\nDomains via Knowledge Augmented Generation. arXiv:2409.13731 [cs.CL]\n[65] Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, and Jundong Li. 2024.\nKnowledge Graph-Enhanced Large Language Models via Path Selection. In\nFindings of the Association for Computational Linguistics, ACL 2024, Bangkok,\nThailand and virtual meeting, August 11-16, 2024. 6311–6321.\n[66] Jerry Liu. 2022. LlamaIndex. https://github.com/jerryjliu/llama_index\n[67] Shuhao Liu, Yang Liu, and Wenfei Fan. 2024. PrismX: A Single-Machine System\nfor Querying Big Graphs. Proc. VLDB Endow. 17, 12 (Nov. 2024), 4485–4488.\n[68] Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Bo Li, Xuming Hu, and\nXiaowen Chu. 2025. ChunkKV: Semantic-Preserving KV Cache Compression\nfor Efficient Long-Context LLM Inference. arXiv preprint arXiv:2502.00299\n(2025).\n[69] Peter A. Lofgren, Siddhartha Banerjee, Ashish Goel, and C. Seshadhri. 2014.\nFAST-PPR: scaling personalized pagerank estimation for large graphs. In Pro-\nceedings of the 20th ACM SIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining (New York, New York, USA) (KDD 2014). Association\nfor Computing Machinery, New York, NY, USA, 1436–1445.\n[70] Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang,\nChenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Luu Anh\nTuan. 2024. ChatKBQA: A Generate-then-Retrieve Framework for Knowl-\nedge Base Question Answering with Fine-tuned Large Language Models.\narXiv:2310.08975 [cs]\n[71] Linhao Luo, Yuan-Fang Li, Reza Haf, and Shirui Pan. 2024. Reasoning on Graphs:\nFaithful and Interpretable Large Language Model Reasoning. In The Twelfth\nInternational Conference on Learning Representations.\n[72] Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, and Shirui Pan.\n2024. Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs\nwith Large Language Models. arXiv:2410.13080 [cs.CL]\n[73] Yuanhua Lv and ChengXiang Zhai. 2011. Lower-bounding term frequency\nnormalization. In Proceedings of the 20th ACM International Conference on In-\nformation and Knowledge Management (Glasgow, Scotland, UK) (CIKM 2011).\nAssociation for Computing Machinery, New York, NY, USA, 7–16.\n[74] Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei,\nJing Tao, Lingyun Song, Jun Liu, Chen Zhang, and Lizhen Cui. 2024. Debate\non Graph: a Flexible and Reliable Reasoning Framework for Large Language\nModels. arXiv:2409.03155 [cs.CL]\n[75] Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, and Jian Guo.\n2024. Think-on-Graph 2.0: Deep and Interpretable Large Language Model\nReasoning with Knowledge Graph-guided Retrieval. arXiv:2407.10805 [cs]\n[76] Yansheng Mao, Yufei Xu, Jiaqi Li, Fanxu Meng, Haotong Yang, Zilong Zheng,\nXiyuan Wang, and Muhan Zhang. 2025. LIFT: Improving Long Context Under-\nstanding of Large Language Models through Long Input Fine-Tuning. arXiv\npreprint arXiv:2502.14644 (2025).\n[77] Costas Mavromatis and George Karypis. 2024. GNN-RAG: Graph Neural Re-\ntrieval for Large Language Model Reasoning. arXiv:2405.20139 [cs]\n[78] Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine\nBordes, and Jason Weston. 2016. Key-Value Memory Networks for Directly\nReading Documents. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing(EMNLP 2016), Austin, Texas, USA, November 1-4,\n2016. 1400–1409.\n[79] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019.\nMulti-hop Reading Comprehension through Question Decomposition and\nRescoring. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics(ACL 2019), Anna Korhonen, David Traum, and Lluís\nMàrquez (Eds.). Association for Computational Linguistics, Florence, Italy, 6097–\n6109.\n[80] OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[81] PENG SI OW and THOMAS E. MORTON. 1988. Filtered beam search in sched-\nuling. International Journal of Production Research 26, 1 (1988), 35–62.\n[82] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The\nPageRank Citation Ranking: Bringing Order to the Web. Technical Report 1999-66.\nStanford InfoLab. Previous number = SIDL-WP-1999-0120.\n[83] Jia-Yu Pan, Hyung-Jeong Yang, Christos Faloutsos, and Pinar Duygulu. 2004.\nAutomatic multimedia cross-modal correlation discovery. In Proceedings of the\nTenth ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining (Seattle, WA, USA) (KDD 2004). Association for Computing Machinery,\nNew York, NY, USA, 653–658.\n[84] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\n2024. Unifying Large Language Models and Knowledge Graphs: A Roadmap.\nIEEE Transactions on Knowledge and Data Engineering 36, 7 (2024), 3580–3599.\n[85] Serafeim Papadias, Zoi Kaoudi, Jorge-Arnulfo Quiané-Ruiz, and Volker Markl.\n2022. Space-efficient random walks on streaming graphs. Proc. VLDB Endow.\n16, 2 (Oct. 2022), 356–368.\n[86] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong,\nYan Zhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: A\nsurvey. arXiv preprint arXiv:2408.08921 (2024).\n[87] Rayleigh. 1905. The Problem of the Random Walk. 72, 1866 (aug 1905), 318.\n[88] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing(EMNLP 2019). Association for\nComputational Linguistics.\n[89] Nils Reimers and Iryna Gurevych. 2020. Making Monolingual Sentence Em-\nbeddings Multilingual using Knowledge Distillation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing(EMNLP 2020).\n\n\n--- Page 15 ---\n\nAssociation for Computational Linguistics.\n[90] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\nFramework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (April 2009), 333–389.\n[91] Stephen E. Robertson. 2004. Understanding inverse document frequency: on\ntheoretical arguments for IDF. J. Documentation 60 (2004), 503–520.\n[92] Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020. Improving Multi-\nhop Question Answering over Knowledge Graphs Using Knowledge Base Em-\nbeddings. In Proceedings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics(ACL 2020), Dan Jurafsky, Joyce Chai, Natalie Schluter,\nand Joel Tetreault (Eds.). Association for Computational Linguistics, Online,\n4498–4507.\n[93] Kevin Scott. 2024. Behind the Tech.\n[94] Yingxia Shao, Shiyue Huang, Xupeng Miao, Bin Cui, and Lei Chen. 2020.\nMemory-Aware Framework for Efficient Second-Order Random Walk on Large\nGraphs. In Proceedings of the 2020 ACM SIGMOD International Conference on\nManagement of Data (Portland, OR, USA) (SIGMOD 2020). Association for Com-\nputing Machinery, New York, NY, USA, 1797–1812.\n[95] Jianbing Shen, Yunfan Du, Wenguan Wang, and Xuelong Li. 2014. Lazy Random\nWalks for Superpixel Segmentation. IEEE Transactions on Image Processing 23,\n4 (2014), 1451–1462.\n[96] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel\nCollier. 2024. A contrastive framework for neural text generation. In Proceedings\nof the 36th International Conference on Neural Information Processing Systems\n(New Orleans, LA, USA) (NeurIPS 2022). Curran Associates Inc., Red Hook, NY,\nUSA, Article 1566, 14 pages.\n[97] Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. PullNet: Open\nDomain Question Answering with Iterative Retrieval on Knowledge Bases and\nText. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP 2019). 2380–2390.\n[98] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun\nGong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. 2024. Think-on-Graph:\nDeep and Responsible Reasoning of Large Language Model on Knowledge\nGraph. In The Twelfth International Conference on Learning Representations.\n[99] Shixuan Sun, Yuhang Chen, Shengliang Lu, Bingsheng He, and Yuchen Li. 2021.\nThunderRW: an in-memory graph random walk engine. Proc. VLDB Endow. 14,\n11 (July 2021), 1992–2005.\n[100] Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for\nAnswering Complex Questions. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics(ACL\n2018): Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker,\nHeng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics,\nNew Orleans, Louisiana, 641–651.\n[101] Yixuan Tang and Yi Yang. 2024. MultiHop-RAG: Benchmarking Retrieval-\nAugmented Generation for Multi-Hop Queries. arXiv:2401.15391 [cs.CL]\n[102] Llama team. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288 [cs.CL]\n[103] Llama team. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI]\n[104] Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. 2021.\nAugmented SBERT: Data Augmentation Method for Improving Bi-Encoders for\nPairwise Sentence Scoring Tasks. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics(ACL\n2021): Human Language Technologies. Association for Computational Linguistics,\nOnline, 296–310.\n[105] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.\narXiv:2302.13971 [cs.CL]\n[106] Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun,\nStefan Lee, David Crandall, and Dhruv Batra. 2018. Diverse Beam Search: Decod-\ning Diverse Solutions from Neural Sequence Models. arXiv:1610.02424 [cs.AI]\n[107] Hanzhi Wang, Zhewei Wei, Junhao Gan, Sibo Wang, and Zengfeng Huang.\n2020. Personalized pagerank to a target node, revisited. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining(KDD 2020). 657–667.\n[108] Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao\nYin, Wenge Rong, and Zhang Xiong. 2023. Knowledge-Driven CoT: Exploring\nFaithful Reasoning in LLMs for Knowledge-intensive Question Answering.\narXiv:2308.13259 [cs]\n[109] Runhui Wang, Sibo Wang, and Xiaofang Zhou. 2019. Parallelizing approximate\nsingle-source personalized pagerank queries on shared memory. The VLDB\nJournal 28, 6 (2019), 923–940.\n[110] Sibo Wang, Renchi Yang, Xiaokui Xiao, Zhewei Wei, and Yin Yang. 2017. FORA:\nSimple and Effective Approximate Single-Source Personalized PageRank. In\nSIGKDD 2017. 505–514.\n[111] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\n2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\nof Pre-Trained Transformers. In Advances in Neural Information Processing\nSystems(NeurIPS 2020), Vol. 33. Curran Associates, Inc., 5776–5788.\n[112] Ye Wang, Qing Wang, Henning Koehler, and Yu Lin. 2021. Query-by-Sketch:\nScaling Shortest Path Graph Queries on Very Large Networks. In Proceedings of\nthe 2021 International Conference on Management of Data (Virtual Event, China)\n(SIGMOD 2021). Association for Computing Machinery, New York, NY, USA,\n1946–1958.\n[113] Zhewei Wei, Xiaodong He, Xiaokui Xiao, Sibo Wang, Shuo Shang, and Ji-Rong\nWen. 2018. Topppr: top-k personalized pagerank queries with precision guar-\nantees on large graphs. In Proceedings of the 2018 International Conference on\nManagement of Data(SIGMOD 2018). ACM, 441–456.\n[114] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natu-\nral Language Processing. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing(EMNLP 2020): System Demonstrations.\nAssociation for Computational Linguistics, Online, 38–45.\n[115] Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-wei Kuo, Nan Guan,\nand Chun Jason Xue. 2025. EvoP: Robust LLM Inference via Evolutionary\nPruning. arXiv preprint arXiv:2502.14910 (2025).\n[116] Yubao Wu, Ruoming Jin, and Xiang Zhang. 2014. Fast and unified local search for\nrandom walk based k-nearest-neighbor query in large graphs. In Proceedings\nof the 2014 ACM SIGMOD International Conference on Management of Data\n(Snowbird, Utah, USA) (SIGMOD 2014). Association for Computing Machinery,\nNew York, NY, USA, 1139–1150.\n[117] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou,\nChengpeng Li, and et al. Chengyuan Li. 2024. Qwen2 Technical Report. arXiv\npreprint arXiv:2407.10671 (2024).\n[118] Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, and Ji-Rong Wen. 2024.\nEfficient Algorithms for Personalized PageRank Computation: A Survey. IEEE\nTrans. on Knowl. and Data Eng. 36, 9 (March 2024), 4582–4602.\n[119] Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic Parsing for\nSingle-Relation Question Answering. In Proceedings of the 52nd Annual Meeting\nof the Association for Computational Linguistics(ACL 2014) (Volume 2: Short\nPapers), Kristina Toutanova and Hua Wu (Eds.). Association for Computational\nLinguistics, Baltimore, Maryland, 643–648.\n[120] Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina\nSuh. 2016. The Value of Semantic Parse Labeling for Knowledge Base Ques-\ntion Answering. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics(ACL 2016) (Volume 2: Short Papers), Katrin Erk\nand Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin,\nGermany, 201–206.\n[121] Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun\nWang, Yiqun Hu, William Wang, Zhiguo Wang, and Bing Xiang. 2023. DecAF:\nJoint Decoding of Answers and Logical Forms for Question Answering over\nKnowledge Bases. arXiv:2210.00063 [cs]\n[122] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya\nSanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather\nthan Retrieve: Large Language Models are Strong Context Generators. In The\nEleventh International Conference on Learning Representations.\n[123] Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou,\nZijin Hong, Junnan Dong, Hao Chen, Yi Chang, and Xiao Huang. 2025. A Survey\nof Graph Retrieval-Augmented Generation for Customized Large Language\nModels. arXiv:2501.13958 [cs.CL]\n[124] Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao\nHuang, Hui Xue, Xiaofeng He, and Jun Huang. 2024.\nR4: Reinforced\nRetriever-Reorder-Responder for Retrieval-Augmented Large Language Models.\narXiv:2405.02659 [cs.CL]\n[125] Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song.\n2018. Variational Reasoning for Question Answering with Knowledge Graph.\nIn AAAI.\n[126] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-\naugmented generation for ai-generated content: A survey.\narXiv preprint\narXiv:2402.19473 (2024).\n[127] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large\nLanguage Models. arXiv preprint arXiv:2303.18223 (2023).\n[128] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H.\nChi, Quoc V. Le, and Denny Zhou. 2024. Take a Step Back: Evoking Reasoning\nvia Abstraction in Large Language Models. arXiv:2310.06117 [cs]",
    "chunk_order_index": 0,
    "full_doc_id": "doc-fa46ef3acf35b900d9b09945f5135ac3"
  }
}