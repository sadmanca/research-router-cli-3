{
  "doc-792ee78d563dd146ab2e518df79ba1bf": {
    "content": "--- Page 1 ---\n\narXiv:2505.13006v1  [cs.CL]  19 May 2025\nEvaluating the Performance of RAG Methods for Conversational AI in the\nAirport Domain\nYuyang Li1, Philip J.M. Kerbusch2, Raimon H.R. Pruim2, Tobias Käfer1\n1Karlsruhe Institute of Technology, 2Royal Schiphol Group\n2Royal Schiphol Group, 1Karlsruhe Institute of Technology\nyuyang.li@kit.edu,\ntobias.kaefer@kit.edu\nAbstract\nAirports from the top 20 in terms of annual\npassengers are highly dynamic environments\nwith thousands of flights daily, and they aim\nto increase the degree of automation. To con-\ntribute to this, we implemented a Conversa-\ntional AI system that enables staff in an air-\nport to communicate with flight information\nsystems. This system not only answers stan-\ndard airport queries but also resolves airport ter-\nminology, jargon, abbreviations, and dynamic\nquestions involving reasoning. In this paper, we\nbuilt three different Retrieval-Augmented Gen-\neration (RAG) methods, including traditional\nRAG, SQL RAG, and Knowledge Graph-based\nRAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy\nusing BM25 + GPT-4 but occasionally pro-\nduced hallucinations, which is risky to airport\nsafety. In contrast, SQL RAG and Graph RAG\nachieved 80.85% and 91.49% accuracy respec-\ntively, with significantly fewer hallucinations.\nMoreover, Graph RAG was especially effective\nfor questions that involved reasoning. Based\non our observations, we thus recommend SQL\nRAG and Graph RAG are better for airport en-\nvironments, due to fewer hallucinations and the\nability to handle dynamic questions.\n1\nIntroduction\nAmsterdam Airport Schiphol, one of the top 20\nairports in the world, ranked by annual passenger\nnumbers, handles thousands of flights each day.\nThese airports rely on staff like gate planners and\napron controllers to access and update data across\nsystems. For these employees, traditional database\nqueries can be complex and time-consuming for\nsome employees who are not query experts when\nthey need flight information. A conversational AI\nsystem with a natural language query (NLQ) inter-\nface allows all employees to interact with systems\nnaturally, asking questions like, “Which fights are\nat ramp D07?” and receiving instant answers. This\nimproves productivity, and streamlines workflows,\nespecially in high-pressure areas like at the gate,\nwhere less educated workers require access to up-\nto-date information. By replacing strict query for-\nmats with intuitive, real-time responses, conversa-\ntional AI enhances decision-making and efficiency,\nmaking it a suitable solution for dynamic environ-\nments such as airports.\nBuilding such a system is challenging because\nflight data is stored by experts in tables using avi-\nation abbreviations. We need our system to un-\nderstand these datasets to answer questions from\nthe airport domain. Additionally, ensuring avia-\ntion safety is a major concern; the system must\nbe safe and enable employees to perform accurate\noperations. We address those challenges using two\nresearch questions.\nThe first question is how to handle flight data so\nthat our system can answer different questions. We\ndivided the questions into three types:\n• Straightforward questions: Questions that\ncan be directly answered from the flight data.\n• Questions involving specialized airport jar-\ngon, abbreviations, and incomplete queries:\nOperators often use shorthand or omit con-\ntext. Flight “KL0123” might be referred to as\n“0123” or “123,” while gate “C05” might be\nshortened to “C5.” Abbreviations like “KLM”\nfor “KLM Royal Dutch Airlines” or “Delta”\nfor “Delta Air Lines” are also common. Op-\nerators frequently ask short, incomplete ques-\ntions, e. g., “Which flights are at D04?” or\n“What is the gate for that Delta airline?” With-\nout resolving missing details such, these ques-\ntions cannot be answered.\n• Dynamic questions: Questions that involve\nadditional calculations and reasoning, espe-\ncially related to time.\nExamples include\n“What is the connecting flight’s onramp time\n\n\n--- Page 2 ---\n\nfor DL1000?” or “What is DL1000’s next\nflight from the same ramp?” These queries re-\nquire reasoning through connections between\nflights and retrieving specific details.\nThe second research question is about how to re-\nduce hallucinations (Xu et al., 2024) for the safety\nof aviation operations. Hallucinations occur when\nLLMs generate information not based on facts or\ntheir training data. In high-safety environments\nsuch as airports, however the output should be\nfactual and not imaginative (Jacobs and Jaschke,\n2024). For example, if the system gives wrong gate\nnumbers, flight schedules, or safety instructions,\nthis might disrupt aviation operations, cause de-\nlays, or even risk passenger safety. Thus, accurate\nresponses are important.\nIn this case study, we examine three Retrieval-\nAugmented Generation (RAG) techniques for the\nairport environment: Traditional RAG (Lewis et al.,\n2021) Retrieves relevant information from the flight\ndatabase and uses LLMs to generate answers based\non the retrieved data and original questions. SQL\nRAG (Guo et al., 2023) stores all datasets in an\nSQL database and converts natural language ques-\ntions (NLQ) into structured SQL queries. Knowl-\nedge Graph-based Retrieval-Augmented Genera-\ntion (Graph RAG) (Edge et al., 2024) aims to im-\nprove the performance of LLM tasks by applying\nRAG techniques to Knowledge Graphs (KGs), re-\nquiring the original datasets to be stored in the\nknowledge graph. A key challenge is retrieving the\ncorrect flight information from thousands of flights\nwhile minimizing hallucinations.\nThe paper is structured as follows: We first sur-\nvey related work (Sec. 2), then present our dataset\n(Sec. 3), followed by a high-level description of\nour experiments (Sec. 4). We then present the re-\nsults for the research questions (Sec. 5), and lastly\nconclude (Sec. 6). In the Appendix A, we provide\nfurther details, especially on the question genera-\ntion and classification, next to our prompting.\n2\nRelated Work\n2.1\nTraditional RAG\nTraditional\nRetrieval-Augmented\nGeneration\n(RAG) consists of two main stages: the Retriever\nand the Generator (Louis et al., 2023).\nThe\nRetriever identifies relevant documents based on\nuser input, and the Generator uses these documents\nto produce responses. We explore three retrieval\nmethods: keyword search, semantic search, and\nhybrid search,\nusing large language models\n(LLMs) for answer generation.\nIn keyword search, TF-IDF and BM25 are em-\nployed to evaluate retrieval performance. TF-IDF\ncomputes term frequency (TF) and inverse docu-\nment frequency (IDF) (Liu et al., 2018; Robertson,\n2004), measuring how important a term is within\na document and across the corpus. BM25 extends\nTF-IDF with a term saturation function (Robert-\nson and Zaragoza, 2009), reducing the influence\nof extremely frequent terms that often carry less\ninformative value (Chen and Wiseman, 2023).\nSemantic search methods include similarity\nsearch, vector databases like FAISS (Jegou et al.,\n2017; George and Rajan, 2022), k-Nearest Neigh-\nbors (KNN), Locality-Sensitive Hashing (LSH) (Ja-\nfari et al., 2021), and Maximal Marginal Relevance\n(MMR) (Mao et al., 2020). Unlike keyword search,\nsemantic search aims to understand user intent and\nword meanings (Gao et al., 2024). Embedding mod-\nels such as Word2Vec convert words into vectors\n(Mikolov et al., 2013), where cosine similarity mea-\nsures similarities between queries and documents.\nHybrid search combines keyword and seman-\ntic methods, re-ranking results using the Recip-\nrocal Rank Fusion (RRF) algorithm (Robert Lee,\n2024). By combining two search methods, the hy-\nbrid search can not only find flight information by\nkeywords but also find information by the deeper\nmeaning of the queries (Sarmah et al., 2024).\n2.2\nSQL RAG\nText-to-SQL aims to transfer natural language au-\ntomatically questions(NLQs) into SQL queries.\nLLMs recently emerged as an option for Text-to-\nSQL task (Rajkumar et al., 2022). The trick to\nhandling text-to-SQL tasks with LLMs is to apply\nprompt engineering. Five prompt styles for Text-\nto-SQL are explored in the previous research (Gao\net al., 2023). Basic Prompt (BSP) is a simple repre-\nsentation with no instructions; Text Representation\nPrompt (TRP) adds basic task guidance; OpenAI\nDemonstration Prompt (ODP) adds explicit rules\nlike “Complete sqlite SQL query only,”; Code Rep-\nresentation Prompt (CRP) uses SQL-style schema\ndescriptions with detailed database information\nlike primary/foreign keys, and Alpaca SFT Prompt\n(ASP) adopts Markdown for structured training\nprompts. In (Gao et al., 2023), CRP achieves the\nbest performance in most LLMs, by providing com-\nplete database information and utilizing the LLMs’\nstrength in understanding code.\n\n\n--- Page 3 ---\n\n2.3\nGraph RAG\nA Knowledge Graph (KG) is a structured repre-\nsentation of entities (nodes), their attributes, and\nrelationships (edges), typically stored in graph\ndatabases or triple stores (Sarmah et al., 2024). Its\nbasic unit is a triple: subject, predicate, object.\nIn Graph RAG (Retrieval-Augmented Generation),\nnatural language questions are converted into query\nlanguages like SPARQL for RDF graphs or Cypher\nfor Neo4j property graphs. Research indicates that\nNeo4j’s labeled property graph model offers faster\nand more efficient real-time analysis and dynamic\nquerying compared to complex RDF ontologies in\nenterprise projects (Barrasa et al., 2023). Neo4j’s\nproperty graph model better meets industrial needs.\nFlight information can be automatically integrated\ninto the knowledge graph by matching the row and\ncolumn names of the flight table, with relationships\nmanually defined based on flight numbers.\n3\nDataset and Questions\nOur flight information dataset is tabular containing\nthousands of flights with key details such as flight\nnumber, aircraft category, bus gate, bus service\nneeded, flight UID, ramp, expected on-ramp time,\nconnecting flight number, etc.\nTo evaluate the effectiveness of different retrieval\nmethods, we classified the questions, and then\nbased on these questions, we created two ground\ntruth datasets: a straightforward dataset and a com-\nplicated, ambiguous dataset.\nThe straightforward dataset consists of un-\nambiguous questions that can be directly an-\nswered from flight information. Examples include:\n\"What category of aircraft is designated for flight\nKL1000?\" and \"Which ramp is assigned for flight\nKL1000?\". Such questions are easily handled by re-\ntrieval methods to select the most relevant informa-\ntion. This dataset contains thousands of question-\nanswer pairs, with around 100 to 200 pairs selected\nfor the RAG methods comparison.\nThe complicated and ambiguous dataset contains\nquestions with variables that may be unclear or\nmissing from the flight information which cannot\nbe directly queried from the tabular dataset. Exam-\nples are: \"Which flight is at gate B24?\" or \"Which\ngate is assigned to the 0164 flight?\", \"When is\nDelta landing?\" Here, ’B24’ might relate to mul-\ntiple flights or meanings (bus gate or ramp num-\nber), and ’0164’ is not a complete flight number,\n’Delta’ also needs clarification. This dataset also\ncontains thousands of question-answer pairs, with\n185 pairs randomly selected for comparison. More\ninformation on question generation and question\nclassification is provided in the Appendix A.\n4\nExperiments\nTo handle the flight tabular dataset, our conversa-\ntional AI should understand the meaning of these\nflight terms, it also needs to understand specific\njargon and terminology. We explore three RAG\nmethods for a conversational system on flight data.\nFigure 1 shows the traditional RAG method.\nWhen a user asks a question, various retrieval meth-\nods are employed to retrieve the correct flight data\nfrom the flight information dataset. These methods\nare mainly divided into three categories: keyword\nsearch, semantic search, and hybrid search. Af-\nter retrieving the relevant flight information, Large\nLanguage Models (LLMs) generate answers to the\nuser’s questions based on this data. Several LLMs\nwere tested to assess their performance, includ-\ning GPT models, Llama-3-8B-Instruct, BERT, and\nBERT-related models.\nFigure 2 shows the SQL RAG method, which\nbegins with users asking natural language ques-\ntions. An LLM processes these questions using\nthe SQL database schema to generate appropriate\nSQL queries. The queries retrieve relevant informa-\ntion from the SQL database, which the LLM then\ninterprets and reformulates into human-readable an-\nswers. Following the approach in (Gao et al., 2023),\nwe experimented with Code Representation Prompt\n(CRP) and OpenAI Demonstration Prompt (ODP)\nto fine-tune the prompts and improve the SQL RAG\nresults. More details of SQL RAG prompts are pro-\nvided in Appendix A.\nFigure 3 shows the Graph RAG method, which\nalso starts with users asking natural language ques-\ntions. An LLM processes these questions using\nthe graph schema from the graph database to gen-\nerate graph queries. We use Neo4j’s APOC plu-\ngin to extract the schema by executing ’CALL\napoc.meta.schema() YIELD value RETURN value’\nand include it in the prompt. and the LLM in-\nterprets this data to formulate human-readable an-\nswers. The graph structure enables context-aware\nretrieval and reasoning, more details of Graph RAG\nprompts are provided in Appendix A.\nThe three RAG methods described above can\nhandle straightforward datasets easily because the\nanswers all exist in the flight tabular, we will add\n\n\n--- Page 4 ---\n\nsome explanations about flight row names’ mean-\nings to the prompts, so that LLMs can generate\nbetter more accurate answers. However, questions\nabout jargon and short sentences from complicated\nor ambiguous datasets need to be classified using a\nquestion classification prompt, as shown in Figure\n4. After classification, each question is directed to\ndifferent prompts to answer jargon and abbrevia-\ntions.\nFigure 1: Traditional RAG Method\nFigure 2: SQL RAG Method\nFigure 3: Graph RAG Method\nFigure 4: Method on Ambiguous question dataset\n5\nResults\nIn this section, we present the experimental results,\nstructured using our research questions.\n5.1\nRQ1: How to handle flight data for\ndifferent questions?\n5.1.1\nStraightforward questions\nTable 1 summarizes the performance of various re-\ntrieval methods within the traditional RAG pipeline\nin the straightforward dataset. BM25 outperforms\nother methods, achieving approximately 86.54%\naccuracy in retrieving the correct articles. The hy-\nbrid search, which combines BM25 and the vector\ndatabase FAISS in a 9:1 proportion, performs sec-\nond best, with an accuracy of 85.78% for identi-\nfying the correct article as the highest-ranked and\n98.00% accuracy for including the correct article\nwithin the top 10 results. This indicates the success-\nful retrieval of correct articles among the top 10\nmost relevant ones. However, changing the propor-\ntion to 1:9 yields only 0.59% accuracy within the\ntop 30 articles, suggesting that the correct articles\nrarely appear among the top 30 results. Following\nBM25 and the hybrid search, TF-IDF with cosine\nsimilarity and Euclidean distance achieve accura-\ncies of 67.70% and 67.55%, respectively. The vec-\ntor database FAISS alone performs the worst, with\nan accuracy of 0%.\nTable 2 shows how various LLMs perform in\ngenerating answers for the simple dataset. Because\nthe dataset is large, we randomly selected 100 ques-\ntions for the experiment. The LLMs’ answers were\nmanually compared to the standard answers; cor-\nrect ones were marked \"True,\" and incorrect ones\nwere \"False.\" Accuracy was calculated by dividing\nthe number of correct answers by the total num-\nber of questions. In these two tables, we chose\nBM25+GPT4 as the traditional RAG pipeline and\nachieved a total accuracy of 84.40% in the end.\nThe reason keyword search outperforms seman-\ntic search is probably because, in the airport envi-\nronment, most questions are about specific flights,\ntimes, or ramps. These questions don’t require a\ndeep semantic understanding of the content.\nTable 3 shows the performance of SQL RAG.\nThe results indicate that CRP significantly outper-\nforms ODP in most of the cases. EM(Exact Match)\nmeasures the strict match between the predicted\nSQL query and the ground truth regarding syntax\nand structure. while EX(Execution Match) evalu-\nates whether the execution outputs of the predicted\nSQL match the ground truth on the database. Few-\nshot learning was applied using 47 manually cre-\nated examples, including questions, SQL queries,\nand corresponding answers. With CRP, GPT-4\n\n\n--- Page 5 ---\n\nRetrieval Methods\nTotal Rows\nAccuracy\n(Highest)\nAccuracy (Top\n10)\nAccuracy (Top\n30)\nBM25\n1350\n86.54%\n100%\n100%\nTF-IDF + Cosine Similarity\n1350\n67.70%\n100%\n100%\nTF-IDF + Euclidean Distance\n1350\n67.55%\n100%\n100%\nWord2Vec + Cosine Similarity + MMR\n1350\n33.70%\n34.00%\n34.00%\nLSI\n1350\n21.82%\n37.00%\n45.00%\nFAISS\n1350\n0%\n1.00%\n12.00%\nHybrid Search (BM25 : FAISS = 9:1)\n1350\n85.78%\n98.00%\n98.00%\nHybrid Search (BM25 : FAISS = 5:5)\n1350\n82.37%\n98.00%\n98.00%\nHybrid Search (BM25 : FAISS = 1:9)\n1350\n0.59%\n0.59%\n0.59%\nTable 1: Retrieval method results for the Traditional RAG in the straightforward dataset\nModel Name\nAccuracy\nGPT-4\n88.78%\nGPT-4o Mini\n88.12%\nGPT-3.5 Turbo\n83.33%\nLlama-3-8B-Instruct\n76.54%\nRoBERTa\n56.16%\nBERT\n29.73%\nDistilBERT\n28.00%\nDeBERTa\n41.89%\nmDeBERTa\n53.33%\nElectra\n41.33%\nElectra Large\n41.33%\nTable 2: LLMs results in straightforward dataset\nachieves the highest performance (EM: 78.72%,\nEX: 80.85%), followed by GPT-4o Mini, Llama-3-\n8B-Instruct and GPT-3.5 Turbo, CRP consistently\ndelivers better accuracy in most of LLMs, indicat-\ning the importance of detailed schema representa-\ntion for SQL generation.\nLLM\nODP\nCRP\nEM\nEX\nEM\nEX\nGPT-4\n74.47% 78.72% 76.60% 80.85%\nGPT-4o Mini\n76.60% 70.21% 78.72% 80.85%\nGPT-3.5 Turbo\n38.30% 38.30% 25.53% 27.70%\nLlama-3-8B-Instruct\n31.91% 29.79% 68.83% 46.81%\nTable 3: SQL RAG results on the straightforward\ndataset.\nTable 4 presents the performance of Graph RAG,\nshowing strong results across all models when us-\ning the schema prompt. GPT-4 leads with the high-\nest accuracy (EM: 14.89%, EX: 91.49%), followed\nby GPT-4o Mini (EM: 10.64%, EX: 89.36%).\nThe differing EM and EX results between SQL\nRAG and Graph RAG indicate the differences be-\ntween the two methods. In SQL RAG, the data\nis highly structured, leading to more fixed SQL\nqueries and higher EM scores whenever we exe-\ncute it. In contrast, Graph RAG shows a much\nlower EM but high EX, indicating that the graph\nquery language is more flexible and can generate\ndifferent formats while still providing highly accu-\nrate answers.\nLLM\nSchema Prompt\nEM\nEX\nGPT-4\n14.89%\n91.49%\nGPT-4o Mini\n10.64%\n89.36%\nGPT-3.5 Turbo\n10.64%\n82.98%\nTable 4: Graph RAG Results with schema prompt on\nthe straightforward dataset.\n5.1.2\nSpecialized airport jargon,\nabbreviations, and incomplete questions\nAs mentioned in the dataset section, we manually\ncreated a complicated, ambiguous dataset contain-\ning thousands of airport jargon, abbreviations, and\nincomplete questions. We classified these questions\ninto six categories: Time Ambiguous Questions\n(TAQ), and Time With Ambiguous Flight Number\nQuestions (TWAQ). Board Gate Questions (BGQ),\nNext Flight Questions (NFQ), Board Questions of\nAircraft (BQA), and Ambiguous Flight Number\nQuestions (AFQ).\nBoard Questions of Aircraft (BQA) and Am-\nbiguous Flight Number Questions (AFQ) involve\nabbreviations and jargon, such as \"Where is the\ndelta?\" and \"At what gate is the 144?\" Without the\nfull airline names or additional flight details, these\nquestions are challenging to answer. Time Am-\nbiguous Questions (TAQ), Board Gate Questions\n(BGQ), and Time With Ambiguous Flight Number\nQuestions (TWAQ) represent incomplete questions\nlike \"Which flight is currently at gate F09?\" or\n\"What’s at C14?\" These lack critical details such as\nflight numbers. Next Flight Questions (NFQ), on\nthe other hand, are dynamic and will be discussed\nfurther in a later section.\nWe analyzed 220 questions in total to evaluate\nthe robustness of the question classification prompt.\nSince large language models (LLMs) showed some\nvariability in each time response, we employed\na few-shot learning approach by integrating 60\ncarefully selected question classification examples\n\n\n--- Page 6 ---\n\nFigure 5: Confusion Matrix of Question Classifications\nwithin the context window into the prompt. These\n60 examples included six different questions and\ntheir correct categories. We repeated the classifica-\ntion experiments five times on the same questions.\nThe accuracies for these five times’ classification\nruns were 90.45%, 90.45%, 90.91%, 90.45%, and\n90.00%, the average accuracy is 90.45%. The low\nvariance among these runs suggests our prompt is\nrobust and effective. Few-shot learning with ex-\ntensive examples significantly improved accuracy\nand ensured consistent performance for different\nquestion types.\nThe final classification results are shown in the\nFigure 5. Although most questions were classified\ncorrectly, about 22 questions were misclassified.\nHowever, TAQ and BGQ share the same subse-\nquent step of extracting a gate number, so swap-\nping them does not affect outcomes. Similarly,\nTWAQ and BQA both prompt users for additional\ninformation; hence the confusion between these\ntwo also does not have too much impact on final\nresults. When TWAQ or BQA are misclassified\nas TAQ, the system fails to extract a gate number,\nreturns [’0’], and prompts the user for more details\nbefore re-running RAG. Because subsequent steps\nrely on correct classification, we added additional\nmeasures to mitigate the impact of misclassifica-\ntion. Our experiments show that most errors oc-\ncur within these similar categories, and we have\nworked to minimize them as much as possible. Fur-\nther details on the question classification prompts\nare provided in Appendix A.\n5.1.3\nDynamic questions\nThe Next Flight Questions (NFQ) involves two sit-\nuations: determining the next flight from the same\nairline or the same ramp. For the same airline, the\nanswer is directly found in the table ’connecting\nflight number’. For the same ramp, we need to\ndetermine the expected on-ramp time for the cur-\nrent flight and then identify the closest expected\non-ramp time for other flights at that ramp. Dy-\nnamic questions require additional calculation and\nreasoning. for example, if the question is ’What is\nthe expected on-ramp time for the connecting flight\nof DL0123?’ we must first identify DL0123’s next\nconnecting flight, then we can find its expected on-\nramp time. We created a dataset of 30 reasoning\nquestions to test RAG methods. As shown in Table\n5, Graph RAG performed well, leveraging graph\nrelationships for improved retrieval.\nRAG Pipeline\nReasoning Question Dataset\nGraph RAG\n68.75%\nSQL RAG\n6.25%\nTraditional RAG\n9.38%\nTable 5: Performance of different RAG pipelines on the\nreasoning question dataset\n5.2\nRQ2: How to reduce hallucinations?\nHallucinations mainly happen in traditional RAG\nwhen LLMs generate flight destinations not in-\ncluded in our dataset. This issue mainly exists\nin responses to complex and ambiguous queries.\nAfter performing question classification and re-\ntrieving flight information, we conducted few-shot\nlearning with 20 examples, observing a hallucina-\ntion rate of approximately 10%. This phenomenon\nis likely due to the excessive amount of informa-\ntion included in the input prompts for traditional\nRAG, which increases the likelihood of halluci-\nnation compared to SQL RAG and Graph RAG.\nAdditionally, airline companies often reuse flight\nnumbers, leading to conflicting data in LLM train-\ning and causing the generation of information ab-\nsent from the dataset.\nSQL RAG and Graph RAG reduce hallucina-\ntions by converting natural language questions into\nSQL or Cypher queries. Thereby, the input to the\nLLM is accurate data, which significantly reduces\nhallucinations. However, if the question requires a\nlot of context, the conversion to a query may fail.\nIt is important to note that hallucinations are not\ncommon even in traditional RAG and are not elim-\ninated in SQL RAG or Graph RAG. Additionally,\ncalculating the exact accuracy or rate of halluci-\nnations across these RAG methods is challenging.\nHowever, SQL RAG and Graph RAG tend to re-\nduce the occurrence of hallucinations compared\n\n\n--- Page 7 ---\n\nto traditional RAG. Given the high safety require-\nments in airport and aviation environments, SQL\nRAG and Graph RAG are safer for aviation opera-\ntions. Both support dynamic storage of real-time\nflight information. Among them, Graph RAG per-\nforms better due to its stronger reasoning capabil-\nities, enabling it to handle more complex queries\neffectively. More details of the experiment are pro-\nvided in the Appendix A.\n6\nConclusion\nOur evaluation of three RAG methods shows that\nof the traditional RAG methods, BM25+GPT-4\nis more efficient than other methods, because of\nthe terminology used in the airport. However, tra-\nditional RAG can produce hallucinations, which\nposes a safety risk. SQL RAG and Graph RAG\nproduce fewer hallucinations, and Graph RAG on\ntop has higher accuracy. Our overall system ef-\nfectively handles specialized airport terminology\nthrough question classification and prompt engi-\nneering; specifically, we address airport jargon and\nabbreviations. Graph RAG is particularly effective\nin handling reasoning tasks and questions about\ndynamic data, making it efficient in the airport do-\nmain.\n7\nFuture Work\nIn our current research, the experiments are based\non a static environment that does not capture any\nreal-time changes such as delays or gate changes.\nIn future research, we plan to connect the system\nwith live APIs that provide real-time flight status\nand gate information, so that the system can dy-\nnamically retrieve and use real-time data. Another\nlimitation is the relatively small size of our cur-\nrent dataset. In future work, we want to signifi-\ncantly expand and diversify the dataset. A larger\nand more diverse dataset will help ensure that our\nperformance improvements hold across different\nscenarios and strengthen the validity of our conclu-\nsions.\nLimitations\nWe openly acknowledge that this study is not a\nfinalized product but an initial research investiga-\ntion. The system’s performance in the real world\nhas not been demonstrated; it is a prototype that\nwas tested in a controlled environment. Moreover,\nour evaluation is specific to the Schiphol airport\ndomain; adapting the model to other airports or\ndomains may present new challenges. Any de-\nployment would need careful incremental trials,\nuser feedback, and regulatory compliance checks\nto meet the high-reliability standards expected in\naviation contexts.\nEthics Policy\nOur research uses a dataset that has been autho-\nrized by Amsterdam Airport Schiphol and contains\noutdated flight information. Most of the flight in-\nformation is publicly available online and does not\ninclude sensitive information. In this paper, the\ndataset is not publicly released, and it is only used\nto discuss its structure and to provide examples\nof question-answer pairs. No personal or confi-\ndential data are involved. Importantly, this work\nis an exploratory study focused on benchmarking\nperformance in a controlled environment without\nimpacting actual airport operations. We have im-\nplemented methods to reduce AI hallucinations—a\nkey safety concern in this domain. However, any fu-\nture deployment would require additional security\nreviews and strong safeguards to prevent misuse.\nAcknowledgments\nThis work was supported in part by the Deutsche\nForschungsgemeinschaft (DFG, German Research\nFoundation) under Research Unit FOR 5339\n(Project No.459291153). We also gratefully ac-\nknowledge Amsterdam Airport Schiphol for pro-\nviding the raw dataset used in this study, and we\nthank our co-authors from the Royal Schiphol\nGroup for their valuable contributions.\nReferences\nJ. Barrasa, J. Webber, and J. Webber. 2023.\nBuild-\ning Knowledge Graphs: A Practitioner’s Guide.\nO’Reilly.\nXiaoyin Chen and Sam Wiseman. 2023.\nBm25\nquery augmentation learned end-to-end. Preprint,\narXiv:2305.14087.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua\nBradley, Alex Chao, Apurva Mody, Steven Truitt,\nand Jonathan Larson. 2024. From local to global: A\ngraph rag approach to query-focused summarization.\nPreprint, arXiv:2404.16130.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun,\nYichen Qian, Bolin Ding, and Jingren Zhou. 2023.\nText-to-sql empowered by large language models: A\nbenchmark evaluation. Preprint, arXiv:2308.15363.\n\n\n--- Page 8 ---\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang,\nand Haofen Wang. 2024. Retrieval-augmented gener-\nation for large language models: A survey. Preprint,\narXiv:2312.10997.\nGodwin George and Rajeev Rajan. 2022. A faiss-based\nsearch for story generation. In 2022 IEEE 19th India\nCouncil International Conference (INDICON), pages\n1–6.\nChunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li,\nZhihua Wen, Kaixuan Wang, and Ting Wang. 2023.\nRetrieval-augmented gpt-3.5-based text-to-sql frame-\nwork with sample-aware prompting and dynamic re-\nvision chain. Preprint, arXiv:2307.05074.\nSven Jacobs and Steffen Jaschke. 2024.\nLeverag-\ning lecture content for improved feedback: Explo-\nrations with gpt-4 and retrieval augmented generation.\nPreprint, arXiv:2405.06681.\nOmid Jafari, Preeti Maurya, Parth Nagarkar, Khand-\nker Mushfiqul Islam, and Chidambaram Crushev.\n2021.\nA survey on locality sensitive hash-\ning algorithms and their applications.\nPreprint,\narXiv:2102.08942.\nHervé Jegou, Matthijs Douze, and Jeff Johnson. 2017.\nFaiss: A library for efficient similarity search. Face-\nbook AI Research, Data Infrastructure, ML Applica-\ntions. Accessed: insert access date.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2021.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. Preprint, arXiv:2005.11401.\nCai-zhi Liu, Yan-xiu Sheng, Zhi-qiang Wei, and Yong-\nQuan Yang. 2018. Research of text classification\nbased on improved tf-idf algorithm. In 2018 IEEE\nInternational Conference of Intelligent Robotic and\nControl Engineering (IRCE), pages 218–222.\nAntoine Louis, Gijs van Dijck, and Gerasimos Spanakis.\n2023. Interpretable long-form legal question answer-\ning with retrieval-augmented large language models.\nIn Proceedings of the Thirty-Eighth AAAI Confer-\nence on Artificial Intelligence (AAAI-24), Maastricht\nUniversity, Law & Tech Lab. Under review. Code\navailable at https://arxiv.org/abs/2309.17050.\nYuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, and\nJiawei Han. 2020. Multi-document summarization\nwith maximal marginal relevance-guided reinforce-\nment learning. Preprint, arXiv:2010.00117.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space. Preprint, arXiv:1301.3781.\nNitarshan Rajkumar,\nRaymond Li,\nand Dzmitry\nBahdanau. 2022.\nEvaluating the text-to-sql ca-\npabilities of large language models.\nPreprint,\narXiv:2204.00498.\nHeidi\nSteen\nRobert\nLee.\n2024.\nHybrid\nsearch\nusing\nvectors\nand\nfull\ntext\nin\nazure\nai\nsearch.\nhttps://learn.microsoft.com/en-\nus/azure/search/hybrid-search-overview.\nStephen Robertson. 2004. Understanding inverse doc-\nument frequency: On theoretical arguments for idf.\nJournal of Documentation - J DOC, 60:503–520.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3:333–389.\nBhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Pa-\ntel, Stefano Pasquali, and Dhagash Mehta. 2024. Hy-\nbridrag: Integrating knowledge graphs and vector\nretrieval augmented generation for efficient informa-\ntion extraction. Preprint, arXiv:2408.04948.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024.\nHallucination is inevitable: An innate limitation of\nlarge language models. Preprint, arXiv:2401.11817.\nA\nAppendix\nA.1\nData Generation\nIn this section, we explained the methods to gener-\nate ground truth datasets including Question gener-\nation and Question classification.\nA.1.1\nQuestion Classification\nTo classify the questions, a flight information\ndataset is used to create different categories of ques-\ntions. The flight information dataset contains in-\nformation for thousands of flights which include\nseveral key items: The flight number identifies a\nspecific flight, aircraft category, bus gate, bus ser-\nvice needed (remote or none), flight UID, direction\n(departure or arrival), ramp, main ground handler,\nexpected on-ramp time, expected off-ramp time,\nconnecting flight number, connecting flight UID,\nmodified date and time, previous ramp, aircraft\nregistration, flight state, MTT (minimum transfer\ntime), MTT single leg, EU indicator, safe town\nairport (J or P), scheduled block time, best block\ntime, expected block time, expected tow-in time,\nexpected tow-off time, actual final approach time,\nactual block time, actual take-off time, actual board-\ning time, actual tow-in request time, actual tow-off\ntime, actual on-ramp time, actual off-ramp time,\nflight nature, push back, and pier. Based on this\nflight information, we make some classifications\nfor the questions.\nThe Question Classification pipeline is shown\nin Figure 6 Multiple types of questions need to be\naddressed in the project. Firstly, there are Hetero-\ngeneous datasets, which contain different formats\n\n\n--- Page 9 ---\n\nof datasets, including static data and dynamic data.\nStatic data are flight information that remains con-\nstant for example, flight number, flight uid, EU\nindicator, flight nature, etc. while dynamic data\nare the flight information that changes dynamically,\nsuch as the time information expected on-ramp\ntime, expected off-ramp time, modified date and\ntime, connecting flight number, etc. this informa-\ntion are changed dynamically. Secondly, there are\ncommunication specifics of operations specialists’\nquestions, which require handling abbreviations\nand short sentences. Thirdly, there are ambigu-\nity resolution questions, which include ambiguity\nquestions such as airport slang, and short questions\nthat assume context. For example, some user ques-\ntions are very short and not clear, such as \"What is\nat A74?\" or \"Delta airline, any information?\" These\ntypes of questions are also taken into consideration.\nFigure 6: Question Classifications\nTo evaluate how effectively different retrieval\nmethods performed, several tests were run. Two\nground truth datasets were created: a straight-\nforward dataset and a complicated and ambigu-\nous dataset.\nThe straightforward dataset con-\ntains questions without any ambiguities, which can\nbe directly retrieved from flight information arti-\ncles, Examples of straightforward questions were:\n\"What category of aircraft is designated for flight\nKL1000?\", \"Which ramp is assigned for flight\nKL1000?\", and \"What time is the expected on-\nramp for flight KL0923?\" Such questions are easily\nidentified for retrieval methods, which enables the\nselection of the most relevant articles.\nThe second type of dataset is a complicated and\nambiguous dataset; it is made of variables that\nmay be ambiguous and missing from the flight\ninformation dataset. Examples of such questions\nare: \"It is now 2023-05-14 18:07:34+0000. Which\nflight is at gate B24?\" or \"Can you tell me which\nflight is scheduled at gate B24 for 2023-05-14\n18:07:34+0000?\" The gate number remains a con-\nstant variable in this case, but the given time indi-\ncates a random variable that is one hour before the\nscheduled block time in the flight table. When there\naren’t sufficient keywords, the pipeline finds it very\nchallenging to find the exact correct content to gen-\nerate correct answers. Besides, the complicated\nand ambiguous dataset also includes questions with\nambiguous information, such as \"Which flight is\nat gate B24?\" These questions lack specific time\nand aircraft. Which results in multiple flight infor-\nmation that mention gate B24. In addition, many\narticles contain the B24 gate, in this case, BM25 is\ncapable of retrieving all articles containing B24 as\na keyword, and it returns correct articles within the\ntop 30 results, indicating that the relevant article is\namong these top 30 articles.\nA.1.2\nQuestion generation\nThis part includes how to generate benchmark\ndatasets.\nAs previously mentioned, we manually created\ntwo benchmark datasets: a straightforward dataset\nand a complicated/ambiguous dataset. The straight-\nforward dataset contains questions that can be di-\nrectly answered using flight information tables. In\ncontrast, the complicated/ambiguous dataset in-\ncludes more vague questions that depend on vari-\nables like time, airline, and flight number. For ex-\nample, the question \"Which flight is in B24?\" could\nrefer to many flights, so additional information is\nneeded for an accurate answer. To generate the\nstraightforward dataset, we created question tem-\nplates with placeholders like: \"Is there a problem\nwith aircraft separation at <gate_nr>?\" \"What air-\nlines have flights departing from gate <gate_nr>?\";\n\"Can you tell me the aircraft category for flight\n<flight_number>?\" We then manually filled these\nplaceholders with actual gate and flight numbers\nfrom the flight information table.\nTo enrich our questions, we used language mod-\nels to generate more variations. To enrich our ques-\ntions, we used language models to generate more\nvariations. For example, as Figure 7 shows, we\ntook the question \"What is the aircraft category for\nflight [flight_number]?\" and prompted the model:\nWe provide prompts like: \"For each example ques-\ntion, please generate new, unique questions simi-\nlar to the examples given, Do not repeat any spe-\n\n\n--- Page 10 ---\n\ncific flight numbers or questions from the exam-\nples. Use ’[flight number]’ as a placeholder for\nthe flight number. Return only the question text.\"\nThe same method will also be used for other types\nof straightforward questions. After that, the exact\nvalues in the placeholders such as [flight number],\n[ramp], [bus gate], etc., will be queried from the\nflight information dataset manually.Using this ap-\nproach, we generated thousands of straightforward\nquestions to test the performance of the conver-\nsational AI system. During our experiments, we\nrandomly selected 150-200 question-answer pairs\nfrom the straightforward dataset. When evaluating\ndifferent RAG methods and the performance of lan-\nguage models, we manually labeled each response\nas ’True’ or ’False’ to calculate accuracy.\nFigure 7: Dataset generation examples of straightfor-\nward questions\nSimilarly, for the ambiguous and complicated\ndataset, examples of such questions include: \"It\nis now 2023-05-14 18:07:34+0000. Which flight\nis at gate B24?\" or \"Can you tell me which\nflight is scheduled at gate B24 for 2023-05-14\n18:07:34+0000?\" In these cases, the gate number\nis constant, but the provided time varies—usually\nset to one hour before the scheduled block time in\nthe flight table. When keywords are insufficient,\nthe system struggles to find the exact information\nneeded for correct answers. The dataset also con-\ntains questions with ambiguous information, such\nas: \"Which flight is at gate B24?\" These ques-\ntions lack specific time or aircraft details, leading\nto multiple flights associated with gate B24. As\nFigure 8 showed, during our experiments, we clas-\nsified these complicated questions. We randomly\nselected 100-200 question-answer pairs, manually\nlabeled their categories for question classification,\nand marked their prompt engineering results as\n’True’ or ’False’ after classification.\nA.2\nExperiment\nA.2.1\nQuestion Classification and Prompt\nEngineering\nIn this step, questions are categorized into six types:\nTime Ambiguous Questions, Board Gate Questions,\nNext Flight Questions, Time with Airline Ques-\ntions, Board Questions of Airline, and Ambiguous\nFlight Number Questions. The definitions of these\nquestion types are given to LLMs, and prompt them\nto response values from [’1’] to [’6’]. Time Am-\nbiguous Questions are questions that include spe-\ncific times or terms referring to the current moment\nsuch as ’currently’, ’at this moment’, ’right now’,\n’now’, ’when’, ’last hour’, ’next hour’, etc., return-\ning [’1’]. Board Gate Questions are the questions\nthat include gate numbers, like B24, A74, and C07,\nand are brief in length. returning [’2’]. Next Flight\nQuestions are the questions that inquire about a\nflight number and its next or connecting flight. re-\nturning [’3’]. Time with Aircraft Questions involve\nreferences to time—exact moments or terms like\n’right now’, ’later’, ’soon’, ’a while’, ’one hour\nago’, etc., and also mentions airline names like\nKLM, Delta, Transavia, EasyJet...etc, returning\n[’4’]. Board Questions of Aircraft includes airline\nnames, such as KLM, Delta, Transavia, EasyJet,\netc.. returning [’5’]. Ambiguous Flight Number\nQuestions are the queries containing flight numbers\nthat may have been ignored in the airline prefix,\n\n\n--- Page 11 ---\n\nFigure 8:\nDataset generation examples of compli-\ncated/ambiguous questions\nfor example, \"Which gate is assigned to the 0164\nflight?\", \"At what gate is the 0164?\". These flight\nnumbers might be incomplete, possibly consisting\nonly of numbers, or may include letters but do\nnot directly mention a specific airline’s name, then\nthese are Ambiguous Flight Number Questions re-\nturn [’6’]. The details of this prompt are shown in\nthe Figure 9\nAfter classifying questions, we use different\nprompts for each type. For Time Ambiguous Ques-\ntions and Board Gate Questions, we direct them\nto prompts that extract gate numbers from the\nqueries. In traditional Retrieval Augmented Gener-\nation (RAG), we can query the ramp or gate table\nfor language models. For example, the ambiguous\nquestion \"Which airline at A74?\" is reformatted to\nextract the ramp number like [’ramp’: ’A74’]. If it\ndoes not extract the ramp information successfully,\nit will return [’0’]. After testing these prompts, we\nachieve over 80% accuracy in extracting gate num-\nbers. We then retrieve all tables containing these\ngate numbers from the flight information database.\nThese tables, along with the original question, are\nused to generate answers. SQL RAG and Graph\nRAG directly generate query languages based on\nthe ramp and gate numbers.\nQuestions that include airline names, like Time\nwith Aircraft Questions and Board Questions of\nAircraft, don’t provide enough information because\nthe airport has many flights from the same airline.\nTherefore, we prompt users to provide more de-\ntails. For example, if someone asks, \"Which flight\nis at Delta?\", we respond: \"I cannot determine\nthe specific location of the Delta flight with the\ninformation provided. Please provide additional\ninformation like: - Flight UID (Unique Identifier)\n- Flight Number (Flight_NR) - Aircraft Registra-\ntion - Connecting Flight UID (The UID of any\nconnected flight provided by the airline) - Connect-\ning Flight Number (The number of any connected\nflight provided by the airline). If you do not have\nthis information, I can still attempt to process your\nquery but it might require additional search time.\nIn this case, please let me know if you are looking\nfor information about the Ramp (Gate), Bus Gate,\nor Pier.\" After the user provides more information,\nwe use the RAG methods again.\nFor Next Flight Questions, there are two scenar-\nios: the next flight is from the same airline or the\nsame departure ramp. If it’s the same airline, we\nreturn the connecting_flight_nr from the table. If\nit’s the same ramp, we find all flights at that ramp\nand identify the one with the closest on-ramp time.\nTo handle these questions, we write prompts for the\nRAG methods to find the relevant results. For the\nAmbiguous Flight Number Questions, we would\nlike to extract the number and match it with the\nreal-time flight APIs to find the relevant flights that\ncontain those mentioned numbers. However, Since\nwe were researching in the static environments, we\nresponded: \" We could not find more information\nabout the flight number you mentioned, could you\nplease provide us with more information?\"\nA.2.2\nSQL RAG\nthe OpenAI Demonstration Prompt(ODP) and\nCode Representation Prompt(CRP) prompts are\nshowed in Figure 10, Figure 11\nThe ODP, as shown in Figure 10, focuses on sim-\nplicity and explicit rules. It lists table names and\ntheir respective columns without additional data\n\n\n--- Page 12 ---\n\nFigure 9: Prompts of Question Classifications\ntypes or constraints. The ODP style emphasizes\nstraightforward task instructions, such as “Include\nonly valid SQL syntax, without additional format-\nting or explanation” guiding the model to generate\nSQL queries directly without unnecessary explana-\ntions.\nIn contrast, the CRP, shown in Figure 11, adopts\na detailed SQL-style schema description. This ap-\nproach uses CREATE TABLE statements to in-\nclude comprehensive database information, such\nas column types and relationships (e.g., primary\nand foreign keys). By simulating database creation\nscripts, CRP uses the model’s coding capabilities\nto enhance query precision, especially for complex\ndatabases with intricate relationships.\nODP is suitable for simpler, direct tasks, while\nCRP is better for handling more complex databases\nwith comprehensive schema context.\nA.2.3\nGraph RAG in dynamic dataset\nThe Prompt of Graph RAG is shown in Figure\n15, focusing on guidelines for writing a Cypher\nquery. The schema is extracted from the Neo4j\ngraph database using the APOC plugin, specifically\nthrough ’CALL apoc.meta.schema() YIELD value\nRETURN value’, and then used in the prompts. As\nshown in Figure 16, Graph RAG enables flights to\nbe connected through their relationships, allowing\nretrieval of detailed information about connecting\nflights. In contrast, traditional RAG and SQL RAG\n\n\n--- Page 13 ---\n\nFigure 10: ODP Prompt for SQL RAG\nFigure 11: CRP Prompt for SQL RAG\ntreat connecting flights as merely a column in the\ntable, limiting access to further relational informa-\ntion.\nA.2.4\nHallucination Analyses\nThis section provides additional information about\nhallucinations in different RAG methods.\nThe hallucination in traditional RAG is illus-\ntrated in Figure 12. Using the question \"Which\nflights are at B18\" as an example, this query is\nclassified as a Board Gate Question (BGQ) dur-\ning the question classification step. For traditional\nRAG, the gate number \"B18\" is extracted from the\nquestion, and all table rows containing \"B18\" are\nretrieved. These rows, along with the question it-\nself, are then passed to the LLM to generate the\nfinal answer. However, due to the large amount of\nflight information stored in LLMs, hallucinations\nare more likely to happen if the retrieval process\nbrings in too much unrelated information.\nIn contrast, for SQL RAG and Graph RAG, the\nretrieval process is more precise. In SQL RAG\n(Figure 13), the natural language question is first\nconverted into an SQL query that retrieves only the\nrelevant information—in this case, flight numbers\nat gate B18. The results are then passed to the\nLLM to generate the final answer. Similarly, in\nGraph RAG (Figure 14), a Cypher query retrieves\nonly the flight numbers associated with gate B18.\nSince both SQL RAG and Graph RAG retrieve\nmore targeted and accurate information compared\nto traditional RAG, they significantly reduce the\nlikelihood of hallucinations.\nIt is important to note that hallucinations are not\ncommon even in traditional RAG, and they are not\neliminated in SQL RAG or Graph RAG. Addition-\nally, calculating the exact accuracy or rate of hallu-\ncinations across these RAG methods is challenging.\nHowever, because SQL RAG and Graph RAG re-\ntrieve information more accurately, they tend to\nreduce the occurrence of hallucinations compared\nto traditional RAG.\n\n\n--- Page 14 ---\n\nFigure 12: Hallucination case for traditional RAG\nFigure 13: The same case for SQL RAG\nFigure 14: The same case for Graph RAG\n\n\n--- Page 15 ---\n\nFigure 15: Graph RAG Prompt\nFigure 16: Graph RAG in Reasoning Questions"
  }
}