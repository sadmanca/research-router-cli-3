{
  "doc-e0b88d32f06db02af102ff4143604dd2": {
    "content": "--- Page 1 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with\nProcess-Constrained Reinforcement Learning\nChuanyue Yu1, Kuo Zhao1, Yuhan Li3, Heng Chang4∗, Mingjian Feng1, Xiangzhe Jiang1, Yufei Sun1,\nJia Li3, Yuzhi Zhang1∗, Jianxin Li2, Ziwei Zhang2∗†\n1Nankai University 2Beihang University 3HKUST (Guangzhou) 4Huawei Technologies Co., Ltd.\nyuchuanyue@mail.nankai.edu.cn,changh.heng@gmail.com,zyz@nankai.edu.cn,zwzhang@buaa.edu.cn\nAbstract\nGraph Retrieval-Augmented Generation (GraphRAG) has shown\ngreat effectiveness in enhancing the reasoning abilities of Large\nLanguage Models (LLMs) by leveraging graph structures for knowl-\nedge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottle-\nnecks when handling complex problems that require multi-hop\nreasoning, as their query and retrieval phases are largely based on\npre-defined heuristics and do not fully utilize the reasoning poten-\ntials of LLMs. To address this problem, we propose GraphRAG-R1,\nan adaptive GraphRAG framework by training LLMs with process-\nconstrained outcome-based reinforcement learning (RL) to enhance\nthe multi-hop reasoning ability. Our method can decompose com-\nplex problems, autonomously invoke retrieval tools to acquire nec-\nessary information, and perform effective reasoning. Specifically,\nwe utilize a modified version of Group Relative Policy Optimization\n(GRPO) that supports rollout-with-thinking capability to train the\nmodel. Next, we design two process-constrained reward functions.\nTo handle the shallow retrieval problem, we design a Progressive Re-\ntrieval Attenuation (PRA) reward to encourage essential retrievals.\nThen, to handle the over-thinking problem, we design Cost-Aware\nF1 (CAF) reward to balance the model performance with computa-\ntional costs. We further design a phase-dependent training strategy,\ncontaining three training stages corresponding to cold start and\nthese two rewards. These stages empower GraphRAG with format\nfollowing, behavior shaping, and smartness optimization abilities,\nrespectively. Lastly, our method adopts a hybrid graph-textual re-\ntrieval to improve the reasoning capacity. Extensive experimental\nresults demonstrate that GraphRAG-R1 significantly boosts LLM\ncapabilities in solving complex reasoning problems compared to\nstate-of-the-art GraphRAG methods on both in-domain and out-\nof-domain datasets. Furthermore, our framework can be flexibly\nintegrated with various existing retrieval methods, consistently\ndelivering performance improvements.\n∗Project co-leader.\n†Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym ’XX, Woodstock, NY\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nChuanyue Yu1, Kuo Zhao1, Yuhan Li3, Heng Chang4∗, Mingjian Feng1,\nXiangzhe Jiang1, Yufei Sun1, Jia Li3, Yuzhi Zhang1∗, Jianxin Li2, Ziwei\nZhang2 and 1Nankai University 2Beihang University 3HKUST (Guangzhou)\n4Huawei Technologies Co., Ltd.. 2018. GraphRAG-R1: Graph Retrieval-\nAugmented Generation with Process-Constrained Reinforcement Learning\n. In Proceedings of Make sure to enter the correct conference title from your\nrights confirmation email (Conference acronym ’XX). ACM, New York, NY,\nUSA, 13 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nLarge Language Models (LLMs) have attracted ever-growing atten-\ntion in the past few years due to their outstanding ability in answer-\ning complex questions and generating human-quality text [40, 43].\nDespite remarkable progress, LLMs still exhibit significant limi-\ntations when faced with complex tasks that require integrating\nvast and diverse knowledge domains or require deep reasoning\nand multi-step logical inference [30, 36]. As shown in Figure 1(a),\nLLMs will provide an incorrect response when the question is over-\ncomplex and requires knowledge beyond their boundary. Graph\nRetrieval-Augmented Generation (GraphRAG) [13, 24, 29, 45, 53,\n54], which utilizes knowledge graphs for knowledge representa-\ntion, enables effective modeling of complex real-world relationships,\nsuch as causality, hierarchy, and dependency [5, 29].\nHowever, existing GraphRAG methods still face significant per-\nformance bottlenecks when handling complex problems that re-\nquire multi-hop reasoning. The main bottleneck is that existing\nGraphRAG methods rely on pre-defined or heuristic rules during\nquery processing and graph retrieval, which are too rigid and inflex-\nible. Concretely, during query processing, the semantic or lexical\nsimilarity search [6, 8, 13] remains the most widely adopted. For\nsemantic similarity search, queries with high complexity are av-\neraged during vectorization, making it difficult to gather accurate\ninformation for each distinct semantic element. On the other hand,\nlexical similarity search captures only the superficial meaning of\nthe query, easily losing implicit relationships and reasoning logic.\nTherefore, as shown in Figure 1(b), existing GraphRAG methods are\nunable to answer the query, since the retrieval results are incom-\nplete or incorrect when queries are overly complex. Besides, during\nthe graph retrieval phase, current methods employ a combination\nof graph traversal and subgraph retrieval for path reasoning, which\nare rather rigid [11, 20, 23, 25, 35]. For instance, ToG [25, 35], a rep-\nresentative GraphRAG method, starts from a node in the knowledge\ngraph and traverses the surrounding nodes based on breadth-first\nsearch strategy with pruning. While this strategy might work for\nshort paths, it struggles with long-path problems, which require\narXiv:2507.23581v1  [cs.LG]  31 Jul 2025\n\n\n--- Page 2 ---\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nYu et al.\ncapturing logically sound but distant or even disconnected rela-\ntionships. This inflexibility often leads to broken reasoning chains\nand failed retrievals, making it unsuitable for multi-hop problems\nwith complex reasoning logic. As a result, the existing GraphRAG\nmethods fail to fully leverage the LLM’s deep reasoning potential.\nIn this paper, we try to further enhance the multi-hop reason-\ning ability of GraphRAG. Reinforcement learning (RL) has recently\nbeen shown to be effective in endowing LLMs with powerful rea-\nsoning abilities. For example, in the case of DeepSeek-R1 [10], it\nhas been discovered that large-scale reinforcement learning can\nendow models with deep thinking capabilities [18, 37, 46], even\nwithout any supervised fine-tuning. This effectiveness may result\nfrom RL’s use of reward signals to encourage exploration, refining\ndynamic reasoning strategies from static knowledge [27] beyond\nstatic reasoning patterns [1, 9, 47, 52]. However, how to utilize\nRL for GraphRAG training remains unexplored in the literature.\nThe major technical obstacle lies in how to design proper reward\nfunctions so that the reasoning process of LLMs within GraphRAG\ncan be properly constrained, avoiding potential problems such as\nshallow retrieval [4, 15] and over-thinking [2, 3, 34].\nTo address this issue, we propose GraphRAG-R1, an adaptive\nGraphRAG framework with deep reasoning capabilities through\nprocess-constrained outcome-based RL. As shown in Figure 1(c),\nour method can decompose complex multi-hop problems by rea-\nsoning and autonomous, on-demand invocation of retrieval tools.\nSpecifically, we employ a modified version of Group Relative Policy\nOptimization (GRPO) that supports rollout-with-thinking mech-\nanism [33] to autonomously leverage external retrieval tools on\ndemand. Next, we design two process-constrained reward func-\ntions, Progressive Retrieval Attenuation (PRA) and Cost-Aware F1\n(CAF), aiming to inject prior knowledge into the LLM’s training\nprocess. In detail, PRA aims to dynamically adjust the reward for re-\ntrieval actions. Initially, it encourages necessary retrievals to gather\nkey information, preventing the model from shallow retrieval. As\nthe reasoning deepens, PRA progressively attenuates retrieval re-\nwards, guiding the model to rely more on acquired information\nfor internal reasoning and avoid unnecessary retrieval overhead.\nOn the other hand, CAF evaluates the final answer correctness\nusing the F1 score while also accounting for the computational\nresources consumed during the reasoning and retrieval process. It\nencourages the model to achieve correct results more efficiently\nto prevent over-thinking. These two rewards collectively balance\nbetween effectiveness and computational cost. We further design a\nthree-stage phase-dependent training strategy. In the first stage, we\nfocus on the answer format to alleviate cold start problems. In the\nsecond and third stage, we focus on improving the answer quality\nusing the aforementioned two rewards. Lastly, our method employs\na hybrid graph-textual retrieval instead of only retrieving textual\ninformation as in existing GraphRAGs. By leveraging graph struc-\ntures to precisely capture entity relationships while preserving the\nrich semantic details of the original text, our method provides more\ncomprehensive and integrated knowledge for complex reasoning.\nExperiments show that GraphRAG-R1 significantly outperforms\nexisting GraphRAG methods in solving complex reasoning prob-\nlems, across both in-domain and out-of-domain datasets. Addition-\nally, our framework offers flexible integration with various retrieval\nmethods, consistently enhancing their performance.\nOur main contributions are summarized as follows:\n• We propose GraphRAG-R1, a pioneering framework that uti-\nlizes GRPO that supports rollout-with-thinking to enhance the\nreasoning ability of GraphRAG. To the best of our knowledge,\nwe are the first to explore RL-based LLM training in GraphRAG.\n• We propose PRA and CAF, two tailored reward designs to guide\nthe training process of RL and handle the shallow retrieval and\nover-thinking problems. We also propose a phase-dependent\ntraining strategy and a hybrid graph-textual retrieval.\n• Experiments demonstrate that our method exhibits state-of-the-\nart performance on both in-domain and out-of-domain datasets,\nand can be flexibly integrated with various existing retrieval\nmethods with consistently improved performance.\n2\nRelated Work\n2.1\nGraph Retrieval-Augmented Generation\nGraph Retrieval-Augmented Generation (GraphRAG) offers a novel\nparadigm to enhance the reasoning abilities of LLMs by explic-\nitly modeling entity relationships through graph structure such as\nknowledge graphs [13, 29]. GraphRAG has been widely adopted\nacross multiple domains. For instance, GraphRAG based on knowl-\nedge graph [28] incorporates techniques like subgraph retrieval\nand Graph Neural Networks (GNNs) [14] to enhance the accuracy\nand explainability of LLMs in complex knowledge reasoning ques-\ntion answering. GraphRAG based on document graphs [32] model\nsemantic relationships between documents and internal document\nstructures, effectively supporting long-document summarization\nand cross-document information integration [31]. Furthermore,\nGraphRAG for specialized domains are increasingly trending. Ex-\namples include leveraging social graphs [19, 51] to analyze user re-\nlationship for precision recommendations and constructing medical\nor disease graphs [22, 48] that integrate multi-source heterogeneous\ndata to enable personalized treatment plan generation.\nHowever, existing GraphRAG methods still face significant per-\nformance bottlenecks when facing complex reasoning problem, pri-\nmarily due to pre-defined or heuristic rules during query processing\nand graph retrieval that struggle with highly complex multi-hop\nproblems. For example, ToG [25, 35] introduces reasoning steps us-\ning breadth-first search (BFS) with pruning approach, but it heavily\nrelies on prompt engineering and external LLMs and thus lacks flex-\nibility. G-Retriever [14] adopts K-Nearest Neighbors-based retrieval\nwith limited efficacy in multi-hop scenarios. Moreover, existing\napproaches suffer from shallow retrieval depth, since they often\ndepend on predefined fixed paths or hop counts. HippoRAG [12, 20]\noptimizes knowledge integration through personalized PageRank\nbut and only supports single-hop retrieval. KGP [44] and Ligh-\ntRAG [11] are typically confined to 2-3 hops, exhibiting significant\nperformance degradation when handling problems that require\ndeep-dependence mining. In contrast, our work improves the rea-\nsoning ability of GraphRAG for complex problems using process-\nconstrained RL, aiming to effectively resolves the issues of rigid\nretrieval strategies and complex reasoning shortcomings in existing\nGraphRAG approaches.\n\n\n--- Page 3 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nQuestion：\nWhere do Greyhound buses leave from in the city where the band that recorded the album Never Too Loud formed?\n(a) LLM-only\nThe answer to this question is: Greyhound buses leave \nfrom the Greyhound station located at 1000 S. 10th St, \nKansas City, Missouri. The band that recorded the \nalbum \"Never Too Loud\" is The Black Keys, and they \nformed in Akron, Ohio.\nGenerate\n(b) GraphRAG\n1) Philadelphia is also a major hub for Greyhound Lines……\n2) Never Too Loud is the fourth studio album by Canadian hard rock band Danko Jones ……\n3) New York City's public bus fleet is the largest in North America ……\nRetrieve\nI am unable to answer this question.\nGenerate\n1. Identify the band that recorded the album Never Too Loud.\n2. Find the city where this band is from.\n3. Determine where Greyhound buses leave from in that city.\nThink\n1. The band that recorded the album Never Too Loud is Danko Jones.\n2. Danko Jones, also known as Scott Danko, is a Canadian hard rock band from \nBrampton, Ontario, Canada.\n3. Determine where Greyhound buses leave from in Brampton, Ontario, Canada.\nThink\nDanko Jones has an album titled Never Too Loud, and they are from Brampton, \nOntario, Canada. I searched for where Greyhound buses leave from in Brampton, \nOntario. From these queries, I found that the Toronto Coach Terminal might be the \nhub for both. Therefore, I think Greyhound buses in the city where Danko Jones is \nfrom leave from the Toronto Coach Terminal.\nThink\nWho recorded the album \nNever Too Loud?\nQuery\nWhere does Greyhound \nbus leave from in \nBrampton, Ontario?\nQuery\nGreyhound buses in the city where the band that recorded the album \nNever Too Loud formed (Brampton, Ontario, Canada) leave from the \nToronto Coach Terminal.\n(c) GraphRAG-R1\n<never too loud, by, danko jones>\nRetrieve\n<toronto coach terminal, was leased \nto, greyhound canada}>\n<toronto coach terminal, is located in, \nOntario>\nRetrieve\nFigure 1: An example of comparing using only LLMs, GraphRAG, and GraphRAG-R1 in answering complex problems: (a)\nThe LLM (Qwen-2.5-7b) directly produces an output, but the answer is incorrect, (b) HippoRAG2 [12] enhances the LLM\n(Qwen-2.5-7b) by retrieving external knowledge, but fail to generate correct outputs for complex problem, (c) our method\nsuccessfully decomposes the problem and autonomously invokes retrieval methods, producing the correct output.\n2.2\nRL-Enhanced Retrieval Scheme\nRecently, reinforcement learning (RL) has become increasingly im-\nportant for complex reasoning [27, 46, 49]. For example, DeepSeek-\nR1 [10] shows that large-scale RL can endow LLMs with deep\nthinking capabilities. Some pioneering work have also adopted RL\nto enhance RAG [21, 33]. For example, R1-Searcher [33] and Search-\nR1 [21] achieve co-training of retrieval and generation, effectively\nimproving the model’s ability to solve complex problems. How-\never, a systematic exploration of RL schemes within the GraphRAG\ndomain remains open. Technically speaking, directly extending ex-\nisting RL methods into GraphRAG faces several challenges, Firstly,\nthe existing methods usually employ a single outcome-oriented\nreward function, e.g., based solely on the correctness of final an-\nswer. However, this design becomes too simple for GraphRAG,\nsince complex problem may involve multiple different retrieval calls\nto different components of the external knowledge graph. More\ncritically, existing methods have often encountered with reward\nhacking in RL training, potentially resulting in shallow retrieval\nand over-thinking in GraphRAG. To tackle these issues, we propose\nGraphRAG-R1 with tailored reward functions and training strategy\nto non-trivially explore the potential of RL in enhancing GraphRAG,\nto the best of our knowledge, for the first time.\n3\nMethod\nIn this section, we present our proposed method. First, we introduce\nthe rollout retrieval enhanced GRPO algorithm. Second, we detail\nthe designs of process-constrained reward , which includes PRA and\nCAF, together with the phase-dependent training strategy. Finally,\nwe present our hybrid graph-textual retrieval design. An overall\nframework of our method is shown in Figure 2.\n3.1\nRollout Retrieval Enhanced GRPO\nIn this subsection, we introduce the details of our training method,\nwhich is a modified version of GRPO that supports the roll-out-\nwith-thinking capability [33].\nOur goal is to optimize the LLM using RL to generate high-quality\noutputs augmented by an external retrieval tool R that assists the\nreasoning generation process. Specifically, denote the policy of RL\nas 𝜋𝜃(·) parametrized by 𝜃. Let 𝑞represent input queries drawn\nfrom the training dataset D, 𝑜represent the generated outputs, and\n𝑟(·) be the reward function. We formalize the objective as follows:\nmax\n𝜋𝜃E𝑞∼D,𝑜∼𝜋𝜃(·|𝑞;R)\n\u0002\n𝑟(𝑞,𝑜) −𝛽· DKL(𝜋𝜃∥𝜋ref)\n\u0003\n,\n(1)\nwhere 𝜋ref(·) is the reference LLM, DKL(·) is the KL-divergence\nmeasure, and 𝛽is a hyperparameter that controls the strength of the\nKL-divergence regularization. More specifically, our method builds\non the GRPO algorithm [10], which uses the average reward of\nmultiple sampled outputs as a baseline. Specifically, for each input\nquestion 𝑞, GRPO samples a group of 𝐺responses 𝑜1,𝑜2, ...,𝑜𝐺\nusing the old policy 𝜋𝜃old. The policy model is then optimized by\n\n\n--- Page 4 ---\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nYu et al.\n(c) Hybrid Graph-Textual Retrieval\n(b) Processed-Constrained Rewards with Phase-Dependent Training Strategy\ngroup \ncomputation\nReference model\nReward function\nKnowledge Graph\nText\nReasoning\nReasoning\n…\n…\nReasoning\nPolicy model\nQuestion\nPRA\nCAF\nRollout with thinking\nReward Calculation\n: Reward\n𝑟𝑖\n…\n…\n𝑟1\n𝑟2\n𝑟𝑛\n…\n…\n𝑜1\n𝑜2\n𝑜𝑛\n: Output\n𝑜𝑖\n…\n…\n𝐴1\n𝐴2\n𝐴𝑛\n: Advantage\n𝐴𝑖\n: Optimal range\nCold Start SFT\nReward\n①\n②\n③\nCount\nCount\nOverthinking\nShallow Retrieval\nRetrieval frequency\nPRA\nPRA: encourage retrieval\nFormat \nReward\nCount\nFrequency\nFrequency\nFormat \nReward\nCAF: refine retrieval decisions\nRetrieval frequency\nF1=0.8\nF1=0.5\nF1=0.2\nCAF\nReward\nCount\nFrequency\nFrequency\nOver-thinking\nCount\n①②③: Training Stage\n(a) Rollout Retrieval Enhanced Group Relative Policy Optimization (GRPO)\nFigure 2: An overview of GraphRAG-R1 : (a) the rollout retrieval enhanced GRPO as the training strategy of LLM, (b) process-\nconstrained reward designs, containing PRA and CAF rewards and the phase-dependent training strategy, (c) the hybrid\ngraph-textual retrieval, which is more informative than text fragments.\nmaximizing the following objective function:\nJGRPO(𝜃) = E\n𝑞∼𝐷\n{𝑜𝑖}𝐺\n𝑖=1∼𝜋𝜃old\n\u0014 1\n𝐺\n𝐺\n∑︁\n𝑖=1\nmin\n\u0012 𝜋𝜃(𝑜𝑖| 𝑞)\n𝜋𝜃old (𝑜𝑖| 𝑞) · 𝐴𝑖,\nclip\n\u0012 𝜋𝜃(𝑜𝑖| 𝑞)\n𝜋𝜃old (𝑜𝑖| 𝑞) , 1 −𝜖, 1 + 𝜖\n\u0013\n· 𝐴𝑖\n\u0013\n−𝛽· DKL(𝜋𝜃∥𝜋ref)\n\u0015\n,\n(2)\nwhere the group size 𝐺determines the number of responses gen-\nerated per query, 𝜃old denotes the previous policy parameters, 𝜖\nis the clipping range hyperparameter to constrain policy updates,\nand 𝐴𝑖represents the advantage, computed based on the relative\nrewards of outputs within each group as follow:\n𝐴𝑖= 𝑟𝑖−𝜇𝑟\n𝜎𝑟\n,\n(3)\nwhere 𝑟𝑖is the reward for individual output, 𝜇𝑟means the average\nreward of all 𝐺outputs for the same query 𝑞, and 𝜎𝑟is the group\nreward standard deviation.\nTo better adapt GRPO to our GraphRAG scenario, we adopt\ntwo key modifications proposed by R1-Searcher [33]: Rollout-with-\nThinking and retrieval-masked loss. The core objective is to enable\nthe model to autonomously leverage external retrieval tools on\ndemand during inference, i.e., accessing supplementary knowledge\nwhen needed to solve problems beyond its internal capacity, and\nreceive rewards for generating correct answers.\n3.1.1\nRollout-with-Thinking. The model is guided to utilize the ex-\nternal retrieval tool during generation by using two special tokens\n<begin_of_query>...<end_of_query> to indicate the call to the\nsearch tool. When <|end_of_query|> appears, it signifies that\nthe retrieval query has been fully generated. At this point, model\ngeneration is paused, the external retrieval tool is called, and the\nretrieved content is marked with <|begin_of_documents|> ...\n<|end_of_documents|>. This content then guides the model’s next\nstep of reasoning. This method ensures retrieval is seamlessly inte-\ngrated into the reasoning process, allowing the model to continue its\nreasoning based on the retrieved documents without interruption.\nDetailed prompts are provided in Appendix A.4\n3.1.2\nRetrieval-Masked Loss. In the original GRPO, token-level loss\nis computed throughout the rollout sequence. In the GraphRAG sce-\nnario, the rollout sequence consists of tokens generated by the LLM\nand tokens retrieved from external channels. Applying the same op-\ntimization to retrieved tokens and LLM-generated tokens may nega-\ntively impact the training process. Therefore, <begin_of_documents>\nand <end_of_documents> are designated as special tokens and\ntheir enclosed content is masked during training, ensuring the pol-\nicy gradient objective is calculated exclusively on LLM-generated\ntokens. This approach stabilizes training while preserving the flexi-\nbility of retrieval-augmented generation.\n\n\n--- Page 5 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\n3.2\nProcess-Constrained Reward Design\nIn this subsection, we introduce our reward functions to constrain\nthe retrieval process, enabling effective and efficient reasoning.\nRL methods have recently attracted considerable attention to\nimprove the reasoning abilities of LLMs. However, if simple reward\nfunctions are employed, such as relying solely on final answer\ncorrectness, existing methods easily face a technical problem known\nas “Reward Hacking” [7], i.e., the model exploits certain loopholes\nin the reward to gain high scores rather than genuinely improving\nreasoning capabilities. To address this potential issue, our reward\ndesign injects controlled human prior knowledge while preserving\nthe fundamental reward framework. Concretely, we introduce the\nfollowing three rewards: format reward, PRA, and CAF, as follows.\n3.2.1\nFormat reward. This reward function is designed to steer the\nmodel toward strictly adhering to the specified output format, ensur-\ning that it learns to correctly invoke external retrieval and generate\nthe final answer. The model should place the final response entirely\nwithin the <answer>...</answer> tags and guarantee that the\nentire output is fully readable and free of uninterpretable artifacts.\nWhen invoking retrieval, the model should encapsulate retrieval\nqueries within the <|begin_of_query|>...<|end_of_query|> tags\nwith retrieved information contained within <|begin_of_documents|>\n... <|end_of_documents|>. The model should explicitly invoke\nthe retrieval mechanism through this query process rather than\ndirectly generating content without retrieval. An illustration of the\nformat is shown in Figure 4 in Appendix A.4.\nThe format reward is defined as follows:\n𝑅format =\n(\n0.5,\ncorrect\n0,\nincorrect.\n(4)\n3.2.2\nProgressive Retrieval Attenuation (PRA). The most straight-\nforward design to encourage GraphRAG models to invoke retrieval\ncalls is to adopt a single retrieval reward. However, this strategy\ntends to cause the model to invoke the external retrieval only once\nand cause the shallow retrieve problem, as some complex multi-hop\nproblems require multiple retrieval calls. On the other hand, one\ncan sum up rewards for multiple retrieves, but it may trigger un-\nnecessary retrieval calls. To circumvent these issues, we propose\nProgressive Retrieval Attenuation (PRA), specifically designed to\nprovide tailored rewards for each invocation while incorporating a\ndecay factor. With each additional retrieval call, the reward under-\ngoes exponential decay relative to the base reward, concurrently\naccumulating rewards from the preceding invocations. Formally,\nthe retrieval reward is defined as follows:\n𝑅𝑛=\n(\n𝑅0,\n𝑛= 1\n𝑅𝑛−1 + 𝑅0 × 𝑘𝑛−1,\n𝑛> 1 ,\n(5)\nwhere 𝑅0 is the base reward, 𝑛is the number of retrieval calls, and\n0 < 𝑘< 1 is the decay factor. Through this dynamically adjusted\nreward weighting, the retrieval frequency of the model can be\nprecisely controlled within the desired threshold ranges. Using the\nproperties of geometric series, it is easy to see that:\n𝑅0 ≤𝑅𝑛<\n1\n1 −𝑘𝑅0, ∀𝑛.\n(6)\nWhen 𝑘= 0, PRA degenerates into a single retrieval reward. When\n𝑘= 1, it degenerates into the sum of rewards. The final reward is\n𝑅retrieval = 𝑅𝑁, where 𝑁is the actual number of retrieval calls.\nThrough this reward function, we can effectively restrict the\nnumber of model-triggered retrieval operations, effectively prevent-\ning shallow retrieval. For different data characteristics, the hyper-\nparameter 𝑘can be flexibly adjusted to better steer the model’s\nlearning process.\n3.2.3\nCost-Aware F1 (CAF). Although the PRA reward can regu-\nlate the retrieval frequency of the model within a reasonable range,\nthis mechanism alone still relies on prior knowledge and could not\nfully activate the reasoning capabilities of LLMs. For example, the\nLLM may adopt unnecessarily numerous of retrievals, leading to\nthe “over-thinking” problem and greatly waste the computational\nresources. An ideal GraphRAG model should deliver high-quality re-\nsponses with precisely needed retrieval operations, which requires\nthree core competencies: accurately assessing the problem diffi-\nculty, intelligently decomposing complex queries, and dynamically\noptimizing retrieval strategies.\nTo this end, we design the CAF reward, which achieves the\nabove objective through another decay coefficient. Formally, the\nCAF reward is defined as:\n𝑅CAF = 𝐹1 × 𝑎× 𝑒−𝑏×𝑁,\n(7)\nwhere 𝐹1 is the F1 score of the generated answers, 𝑎and 𝑏are hy-\nperparameters to control the coefficients in the regulatory function,\nand 𝑁denotes the total number of retrieval operations. The CAF\nrewards ensure that responses attaining identical F1 scores receive\nhigher rewards when using fewer retrievals, thereby incentivizing\nthe model to proactively recognize problem complexity and refine\nretrieval decisions. Consequently, CAF reward establishes a bal-\nance among reasoning depth, retrieval cost, and generation quality,\neffectively mitigating suboptimal behaviors of over-thinking.\n3.2.4\nPhase-Dependent Training Strategy. Despite the effectiveness\nof these reward functions, simultaneously optimizing them induces\ntechnical challenges. To address this challenge, we propose a phase-\ndependent training strategy to avoid the potential conflicts between\nobjectives. Concretely, we split the training into three sequential\nstages, detailed as follows.\nSpecifically, in the first stage, we utilize another LLM to generate\ndata following our desired format. Then, we utilize supervised\nfine-tuning (SFT) to train the LLM with the generated data. This\ntraining process can enable the model to follow our desired format\nby addressing the cold start problem and stabilizing the training\nprocess, laying the foundation for the following training process.\nIn the second stage, we aim to shape the behavior of the model,\nsynergistically combining the format reward and the PRA reward.\nThis design recognizes standardized tool-calling protocols and re-\ntrieval behavior as essential pre-requisites for high-quality reason-\ning. By learning with two objectives, the model can efficiently learn\nhow and when to invoke the retrieval tools with structured out-\nput patterns. The decay mechanism of PRA dynamically regulates\nretrieval behavior through a high initial reward, driving essential\ninformation retrieval, while its exponential decay characteristic\nsuppresses redundant operations.\n\n\n--- Page 6 ---\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nYu et al.\nIn the last stage, we further optimize the overall smartness of\nthe model, adopting CAF as the primary reward objective while\nretaining format rewards to prevent forgetting issues. As discussed\nin Section 3.2.3, CAF establishes a trade-off between maximizing\nanswer correctness and constraining retrieval counts. This incen-\ntivizes models to adaptively adjust retrieval depth based on query\ncomplexity, suppressing non-essential retrievals for simple queries\nwhile expanding information acquisition for complex multi-hop\nproblems, leading to effective and efficient reasoning.\nIn summary, our phase-dependent training strategy resolves\npotential gradient conflicts, improving training stability and sig-\nnificantly boosting model performance. The pseudo-code of the\ntraining process is shown in Appendix B.1.\n3.3\nHybrid Graph-Textual Retrieval\nCompared to conventional RAG, GraphRAG exhibits superior re-\nlational modeling capabilities using graph structure. We integrate\nthis advantage into both the training and generation phases of our\nframework. Specifically, we retrieve both structured triplets and\nrelevant texts, replacing text fragments as commonly retrieved in\nexisting RAGs and GraphRAGs with hybrid graph-textual retrieval.\nBy employing hybrid retrieval, our method has several advan-\ntages. Compared to lengthy text fragments, the compact nature of\ngraph can drastically reduce token consumption, improving pro-\ncessing efficiency. This structured representation offers superior\ninformation density, allowing the model to focus directly on essen-\ntial entity-relationships while inherently filtering out the redundant\nnoise prevalent in raw text snippets. Therefore, the integration of\nstructured knowledge significantly enhances the quality and effi-\nciency of generated answers, as the reasoning operates on a cleaner\nand more relevant information foundation. We empirically verify\nthe hybrid graph-textual retrieval in Section 4.3.3.\n4\nExperiments\nIn this section, we conduct experiments to evaluate GraphRAG-R1,\nin order to answer the following research questions:\n• RQ1: Does GraphRAG-R1 effectively improve the performance\nof existing LLMs for both in-domain and out-of-domain datasets?\n• RQ2: Do all components, including PRA and CAF rewards, and\nphase-dependent training, contribute positively to our method?\n• RQ3: Is GraphRAG-R1 flexible enough to be seamlessly inte-\ngrated with other retrievers?\n4.1\nExperimental Setup\n4.1.1\nDataset. To comprehensively evaluate the GraphRAG-R1 frame-\nwork, we follow [20, 41] and select four widely used multi-hop QA\nbenchmark datasets: HotpotQA [50], 2Wiki [16], MuSiQue [39],\nand PopQA [26]. For the training process, we utilize 80% of the\ntraining data from HotpotQA, 2Wiki, and MuSiQue. The remaining\n20% of these datasets are employed as the in-domain testing set.\nMeanwhile, we utilize PopQA as an out-of-domain testing set, aim-\ning to compare the generalization capabilities of different models.\nMore details are provided in Appendix A.1.\n4.1.2\nMetrics. Following the common literature, we employ F1\nScore [44], SBERT Similarity (SBERT) [38], and LLM-as-Judge Ac-\ncuracy (ACCL) [33] as evaluation metrics. The F1 Score measures\nthe lexical overlap between generated answers and ground-truth\nreferences. SBERT uses the cosine similarity between sentence\nembeddings generated by the SBERT model to measure semantic\nsimilarity. ACCL leverages an LLM (specifically, Qwen-3-Turbo) as\nan evaluator to assess the correctness of predicted answers based\non contextual understanding.\n4.1.3\nBaselines. We compare 7 representative GraphRAG methods:\n• KGP [44]: It utilizes fine-tuned LLMs to explore neighboring\nnodes and aggregate information from a KG to produce answer.\n• ToG [35]: It dynamically expands and evaluates the most prob-\nable inference path in the KG to perform reasoning.\n• LightRAG [11]: It enhances standard RAG by integrating graph-\nbased text indexing with a dual-level retrieval strategy.\n• PropRAG [41]: It optimizes the reasoning chain via proposi-\ntional representation and beam search.\n• G-Retriever [14]: It integrates LLM and graph neural network\n(GNN) with RAG for conversational graph QA.\n• HippoRAG2 [12]: It uses personalized PageRank for deep pas-\nsage integration inspired by the hippocampal indexing theory.\n• R1-Searcher [33]: It enhances the capability of LLMs to invoke\nRAG for retrieval through a two-stage RL framework.\nWe also compare with the following self-constructed baselines:\n• Vanilla LLM: It uses the raw backbone LLM for inference.\n• Naïve RAG: It adopts a single-hop retrieval from the vector\ndatabase for generation.\n• Prompt Engineering: It adopts GraphRAG via structural prompts.\n• SFT: It adopts GraphRAG via SFT on the KGQA data.\n4.1.4\nImplementation Details. We utilize Qwen-2.5-7B as the de-\nfault backbone LLM model. For baselines that require external\nclosed-source LLMs, we utilize Qwen-3-Turbo. We use HippoRAG2\nas the external retriever for LLM’s rollout retrieval.\nFor training, in the first stage, we generate training data using\nERNIE-3.5-128K [42]. The sampling temperature is set to 1.0 with a\nmaximum retrieval count of 8, the learning rate is 5𝑒−6, and the\nnumber of epochs for SFT is 1. In the second stage, we set the base\nreward for PRA as 𝑅0 = 0.5 and its decay factor as 𝑘= 1. In the third\nstage, we set the hyper-parameter for CAF as 𝑎= 2 and 𝑏= 0.1.\nThroughout three stages, LoRA fine-tuning [17] is employed with\n𝑟= 16 and lora_alpha = 32. More detailed settings and prompts are\nprovided in Appendix A.5.\n4.2\nMain Results\nTo answer Q1, we present the performance comparison between\nGraphRAG-R1 and baselines across four benchmarks in Table 1.We\ncan draw the following three observations:\nGraphRAG-R1 achieves consistent improvements across all datasets.\nSpecifically, we achieve the best results for evaluation metrics F1\nand SBERT on all datasets, and achieve three best results and one\nsecond-best result for ACCL. This fully validates the effectiveness\nof the GraphRAG-R1 framework in enhancing LLMs’ capability to\nhandle multi-hop reasoning problems. Particularly, it outperforms\nthe strongest baseline in F1 score by up to 38.03% on HotpotQA,\n\n\n--- Page 7 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 1: The results of different methods for multi-hop question answering. The first three datasets are in-domain, and PopQA\nis out-of-domain (i.e., unseen during training). The best results are in bold and the second-best results are underlined.\nMethod\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nVanilla LLM\n3.08\n30.00\n44.32\n2.01\n15.50\n43.14\n4.55\n29.50\n44.37\n10.46\n25.90\n50.07\nNaive RAG\n24.24\n49.00\n58.73\n8.99\n19.00\n48.97\n13.14\n25.00\n52.56\n23.72\n58.10\n58.60\nPrompt Engineering\n22.22\n41.00\n58.67\n7.72\n25.50\n49.91\n17.54\n36.00\n54.44\n13.37\n27.50\n51.61\nSFT\n12.10\n49.50\n51.76\n8.29\n21.00\n48.90\n11.40\n48.50\n49.46\n10.45\n54.10\n50.56\nKGP\n10.73\n21.00\n50.92\n4.61\n11.00\n46.70\n10.16\n18.00\n50.27\n21.01\n50.00\n56.10\nToG\n11.44\n21.50\n50.48\n5.02\n8.00\n47.05\n14.45\n21.50\n53.11\n29.21\n47.00\n62.94\nLightRAG\n7.96\n18.50\n49.15\n3.09\n6.00\n44.99\n7.58\n10.00\n47.95\n14.55\n34.30\n53.07\nPropRAG\n25.47\n55.00\n59.76\n10.11\n21.50\n50.77\n17.17\n35.00\n53.85\n23.29\n57.30\n58.22\nG-retriever\n14.81\n21.50\n54.89\n3.38\n5.00\n47.47\n15.12\n19.00\n52.77\n2.14\n6.80\n44.63\nHippoRAG2\n27.52\n53.00\n61.02\n8.90\n23.50\n50.19\n12.32\n29.00\n51.92\n23.99\n57.46\n58.16\nR1-Searcher\n26.82\n35.00\n59.15\n12.35\n14.50\n50.32\n15.89\n22.50\n51.51\n23.24\n43.20\n56.80\nGraphRAG-R1\n38.00\n52.00\n66.66\n20.06\n26.50\n57.49\n32.24\n49.50\n63.56\n35.04\n60.00\n65.31\nTable 2: The ablation study results on PRA and CAF rewards.\n#Calls denotes the number of retrieval calls.\nMethod\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nF1\n#Calls\nF1\n#Calls\nF1\n#Calls\nF1\n#Calls\nGraphRAG-R1 38.00\n1.94\n20.06\n2.36\n32.24\n2.05\n35.04\n1.49\n- PRA\n33.92\n1.77\n14.48\n2.22\n31.55\n1.75\n31.40\n1.32\n- CAF\n28.36\n2.33\n19.82\n2.80\n22.16\n2.36\n26.98\n2.05\n- PRA & CAF 33.34\n1.83\n15.32\n2.26\n28.94\n1.94\n30.06\n1.37\n62.43% on MuSiQue, and 83.81% on 2Wiki, respectively. GraphRAG-\nR1 get more improvement on a more difficult multi-hop dataset.\nThis demonstrates the framework’s efficacy in enhancing large\nlanguage models’ multi-hop reasoning capabilities.\nGraphRAG-R1 achieves leading performance on both in-domain\ndatasets (HotpotQA, MuSiQue, and 2Wiki) and the out-of-domain\ndataset PopQA. Concretely, it outperforms the best baseline by\nup to 19.96% in terms of F1 on PopQA, indicating that our pro-\nposed method possesses strong generalization ability and can effec-\ntively handle unseen complex questions. In comparison, PropRAG,\nwhich show the most competitive performance on HotpotQA and\nMuSiQue, do not generalize to the out-of-domain dataset, indicating\ngeneralization is challenging for existing GraphRAG methods.\nFrom the results of self-constructed baselines, existing tech-\nniques such as naive RAG, prompt engineering, and SFT can indeed\nimprove the ability of vanilla LLM on complex question answering\ntasks. However, our method, which adopts RL-based training, is\nclearly more effective, outperforming all these non-RL approaches\nusually by a large margin. The consistent performance improve-\nment validates that RL-based methods hold more potential in en-\nhancing GraphRAG and the reasoning ability of LLMs in general.\nTable 3: The results of the ablation study on the phase-\ndependent training strategy.\nMethod\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nF1\nACCF\nF1\nACCF\nF1\nACCF\nF1\nACCF\nGraphRAG-R1 38.00 92.00 20.06 89.50 32.24 89.00 35.04 95.10\n- cold start\n30.31\n89.00\n14.84\n89.50\n30.04\n89.50\n28.83\n92.80\n- all stages\n26.12\n71.00\n16.79\n74.00\n28.05\n76.50\n30.23\n82.50\n4.3\nFurther Analyses\nTo answer Q2, in this section, we conduct further analyses to verify\nthe designs of our reward functions, phase-dependent training strat-\negy, and hybrid retrieval. We also discuss the general compatibility\nof our method with various retrieval methods to answer Q3.\n4.3.1\nPRA & CAF Rewards. We perform ablation studies to evaluate\nthe contribution of PRA and CAF reward designs. We remove PRA,\nCAF, and both rewards, denoted as “-PRA”, “-CAF”, and “-PRA &\nCAF”, respectively, and present the results in Table 2. The results\nshow that removing either PRA or CAF, or both, leads to substantial\nperformance drops in terms of F1 score. This confirms that both\nmechanisms are indispensable components of GraphRAG-R1 .\nBesides the quantitative F1 score, we also delve deeper into the\nresults by reporting the number of retrieval calls. We make the\nfollowing observations. Without using PRA, our method has a\nreduced number of retrieval calls, indicating that it suffers from the\n“shallow retrieve” problem and leads to unsatisfactory performance.\nPRA dynamically regulates and encourages retrieval calls through\nits decay factor, making the retrieval behavior more controllable.\nOn the other hand, without using CAF, the number of retrieval\ncalls consistently increases across all datasets, indicating the model\nis more inclined to invoke external functions. However, this not\nyield to corresponding F1 improvements in answer quality. In fact,\nthe F1 score is even lower than methods invoking few retrievals,\n\n\n--- Page 8 ---\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nYu et al.\nTable 4: The results of integrating GraphRAG-R1 with other GraphRAG retrieval methods. Note that GraphRAG-R1 is only\ntrained once and do not access other RAG methods during training. Imp denotes the average improvement across all datasets.\nMethod\nModel\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nImp\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nNaive RAG\nOriginal\n24.24\n49.00\n58.73\n8.99\n19.00\n48.97\n13.14\n25.00\n52.56\n23.72\n58.10\n58.60\n+GraphRAG-R1 32.08\n48.00\n62.72\n20.77\n28.00\n56.59\n30.14\n37.50\n61.22\n31.96\n57.50\n52.96\n+ 18.04%\nKGP\nOriginal\n10.73\n21.00\n50.92\n4.61\n11.00\n46.70\n10.16\n18.00\n50.27\n21.01\n50.00\n56.10\n+GraphRAG-R1 36.16\n41.50\n65.81\n13.33\n14.50\n53.82\n27.79\n35.50\n60.41\n18.33\n55.00\n55.39\n+ 36.25%\nToG\nOriginal\n11.44\n21.50\n50.48\n5.02\n8.00\n47.05\n14.45\n21.50\n53.11\n29.21\n47.00\n62.94\n+GraphRAG-R1 16.82\n25.50\n55.36\n8.29\n8.50\n51.77\n21.95\n26.00\n57.68\n22.30\n30.50\n60.10\n+ 3.52%\nLightRAG\nOriginal\n7.96\n18.50\n49.15\n3.09\n6.00\n44.99\n7.58\n10.00\n47.95\n14.55\n34.30\n53.07\n+GraphRAG-R1 20.85\n27.00\n57.40\n8.59\n12.50\n48.95\n19.64\n23.00\n55.31\n28.42\n49.00\n60.50\n+ 38.37%\nPropRAG\nOriginal\n25.47\n55.00\n59.76\n10.11\n21.50\n50.77\n17.17\n35.00\n53.85\n23.29\n57.30\n58.22\n+GraphRAG-R1 30.87\n42.00\n62.25\n18.78\n21.00\n55.32\n34.86\n45.50\n64.44\n32.69\n57.80\n63.44\n+ 13.16%\nG-retriever\nOriginal\n14.81\n21.50\n54.89\n3.38\n5.00\n47.47\n15.12\n19.00\n52.77\n2.14\n6.80\n44.63\n+GraphRAG-R1\n13.52\n15.50\n54.64\n7.49\n2.50\n51.56\n15.23\n17.50\n54.07\n5.86\n11.10\n49.06\n+ 3.66%\nHippoRAG2 Original\n27.52\n53.00\n61.02\n8.90\n23.50\n50.19\n12.32\n29.00\n51.92\n23.99\n57.46\n58.16\n+GraphRAG-R1 38.00\n52.00\n66.66\n20.06\n26.50\n57.49\n32.24\n42.50\n63.56\n35.04\n60.00\n65.31\n+ 22.40%\nindicating that the model tends towards “over-thinking” and per-\nforming unnecessary retrievals, potentially due to reward-hacking\nof solely using the PRA reward.\nWhen both rewards are adopted, our method achieves the best\nperformance with a suitable number of retrieval calls, indicating\nour method can calibrate its retrieval strategy to the inherent dif-\nficulty of the problem. For HotpotQA and PopQA, which contain\nrelatively simpler problems, the number of retrieval calls is automat-\nically decreased. Conversely, for inherently more complex problems\ndemanding deeper information gathering, such as MuSiQue and\n2Wiki, our model learns to invoke retrievals more frequently. In\nsum, our method can dynamically balance efficiency and effective-\nness through reward designs.\n4.3.2\nPhase-Dependent Training Strategy. To validate the effective-\nness of our training strategy, we compare with two variants: one\nwithout using the first cold-start stage (denoted as “-cold start”),\nand the other one without using stages, i.e., directly mixing all\nreward functions for training (denoted as “-all stages”). The results\nare shown in Table 3. We have the following observations.\nIntroducing the cold-start strategy effectively improves perfor-\nmance. In this stage, the model can learn simple tasks like format\nlearning and retrieval invocation, which is beneficial for optimizing\nadvanced policies in the later two stages.\nCompared to training, which used format, PRA, and CAF re-\nwards simultaneously, we found that the three-stage training strat-\negy delivers superior performance. The first stage, which focused\non format and tool invocation learning, establishes a more solid\nfoundation for optimizing answer quality in the second stage. This\napproach avoids conflicts or convergence difficulties caused by\noverly complex reward signals.\n4.3.3\nHybrid Graph-Textual Retrieval. To demonstrate the effec-\ntiveness of our hybrid graph-textual retrieval design, we compare\ndifferent retrieval strategies. During training, we adopt three set-\ntings: using mixing text and graph (denoted as “Text+Graph”), text\nretrieval only (denoted as “Text”), and using graph only (denoted as\n\"Graph\"). During inference, we utilize 4 different retrieval methods:\nusing 5 text snippets (denoted as “Text5”), using 3 text snippets and\n10 graph triplets (denoted as “Text3+Graph10”), using 1 text snip-\npet and 10 graph triplets (denoted as “Text1+Graph10”), and using\n20 graph triplets(denoted as “Graph20”). The results are shown in\nFigure 3 and we make the following two observations.\nFor different training strategies, training only with texts and\ntraining only with graph generally lead to performance decrease in\nthe F1 score, demonstrating the effectiveness of mixed training .\nFor different inference strategies, as the proportion of graph data\nincreases, the number of tokens consumed significantly reduces,\nwhile the F1 score of the model only slightly decreases. This demon-\nstrates that graph information generally possess high information\ndensity, enabling the capture of valuable information needed for\nquestion answering. Our hybrid retrieval approach can achieve a\nsuitable balance between model effectiveness and computational\nresource consumption.\n4.3.4\nIntegration with Other Retrievers. To answer Q3, we con-\nduct experiments to integrate our proposed method with other\nexisting retrieval methods. Specifically, we choose all RAG meth-\nods used in our baselines, including Naive RAG, ToG, KGP, Ligh-\ntRAG, PropRAG, G-retriever, and HippoRAG2. As shown in Table\n4, GraphRAG-R1 generally improves the performance of different\nRAG methods across datasets. Specifically, GraphRAG-R1 achieves\nnotable average improvements over the original methods, with\nthe highest enhancement of 38.37% on LightRAG and consistent\ngains ranging from 3.52% to 36.25% in other baselines. Note that\n\n\n--- Page 9 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nText5\nText3+\nGraph10\nText1+\nGraph10\nGraph20\nInference Strategy\n0\n10\n20\n30\n40\nF1 Score\nTrain(Text+Graph)\nTrain(Text)\nTrain(Graph)\nText5\nText3+\nGraph10\nText1+\nGraph10\nGraph20\nInference Strategy\n0\n500\n1000\n1500\n2000\n2500\nToken Cost\nTrain(Text+Graph)\nTrain(Text)\nTrain(Graph)\nFigure 3: A comparison of different retrieval formats. Left:\nthe F1 score comparison. Right: the token cost for testing.\nour method is only trained once and only has access to HippoRAG2\nduring training, indicating that the reasoning and retrieval ability\nlearned by GraphRAG-R1 can be easily transferred and generalized\nto other GraphRAG methods.\nMore experiments including adopting LLaMA-3 as the backbone\nLLM, a comparison of the number of tokens, and detailed prompts\nused are provided in the Appendix.\n5\nConclusion\nIn this paper, we propose GraphRAG-R1, an adaptive GraphRAG\nframework to enhance the multi-hop reasoning ability of LLMs\nthrough process-constrained RL. Our core designs include the roll-\nout retrieval enhanced GRPO training method, process-constrained\nreward designs including PRA, CAF, and phase-depending training,\nand a hybrid graph-textual retrieval. Experimental results validate\nthe superiority of GraphRAG-R1 over state-of-the-art baselines\nacross both in-domain and out-of-domain datasets. We also demon-\nstrate our method can be flexibly integrated with other retrieval\nmethods. Future works include applying our method to more types\nof graphs and application scenarios.\nAcknowledgments\nReferences\n[1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.\n2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection.\n(2024).\n[2] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Lin-\nfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not\nthink that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint\narXiv:2412.21187 (2024).\n[3] Jhouben Cuesta-Ramirez, Samuel Beaussant, and Mehdi Mounsif. 2025. Large\nReasoning Models are not thinking straight: on the unreliability of thinking\ntrajectories. arXiv preprint arXiv:2507.00711 (2025).\n[4] Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. 2025.\nBoosting the generalization and reasoning of vision language models with cur-\nriculum reinforcement learning. arXiv preprint arXiv:2503.07065 (2025).\n[5] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva\nMody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan\nLarson. 2024. From local to global: A graph rag approach to query-focused\nsummarization. arXiv preprint arXiv:2404.16130 (2024).\n[6] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,\nTat-Seng Chua, and Qing Li. 2024. A survey on rag meeting llms: Towards\nretrieval-augmented large language models. In Proceedings of the 30th ACM\nSIGKDD conference on knowledge discovery and data mining. 6491–6501.\n[7] Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model\noveroptimization. In International Conference on Machine Learning. PMLR, 10835–\n10866.\n[8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin\nDai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented\ngeneration for large language models: A survey. arXiv preprint arXiv:2312.10997\n2, 1 (2023).\n[9] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin,\nXianpei Han, Le Sun, and Jie Zhou. 2025. DeepRAG: Thinking to Retrieve Step\nby Step for Large Language Models. arXiv preprint arXiv:2502.01142 (2025).\n[10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1:\nIncentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948 (2025).\n[11] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. Lightrag:\nSimple and fast retrieval-augmented generation. arXiv preprint arXiv:2410.05779\n(2024).\n[12] Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025.\nFrom rag to memory: Non-parametric continual learning for large language\nmodels. arXiv preprint arXiv:2502.14802 (2025).\n[13] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Ma-\nhantesh Halappanavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, et al.\n2024. Retrieval-augmented generation with graphs (graphrag). arXiv preprint\narXiv:2501.00309 (2024).\n[14] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun,\nXavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented gen-\neration for textual graph understanding and question answering. Advances in\nNeural Information Processing Systems 37 (2024), 132876–132907.\n[15] Chadi Helwe, Chloé Clavel, and Fabian Suchanek. 2021.\nReasoning with\ntransformer-based models: Deep learning, but shallow reasoning. In 2021 In-\nternational Conference on Automated Knowledge Base Construction (AKBC).\n[16] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.\nConstructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reason-\ning Steps. In Proceedings of the 28th International Conference on Computational\nLinguistics. 6609–6625.\n[17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\nModels. (2021).\n[18] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky,\nAiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024.\nOpenai o1 system card. arXiv preprint arXiv:2412.16720 (2024).\n[19] Julie Jiang and Emilio Ferrara. 2023. Social-llm: Modeling user behavior at scale\nusing language models and social network data. arXiv preprint arXiv:2401.00893\n(2023).\n[20] Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su.\n2024. Hipporag: Neurobiologically inspired long-term memory for large language\nmodels. Advances in Neural Information Processing Systems 37 (2024), 59532–\n59569.\n[21] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang,\nHamed Zamani, and Jiawei Han. 2025.\nSearch-r1: Training llms to reason\nand leverage search engines with reinforcement learning.\narXiv preprint\narXiv:2503.09516 (2025).\n[22] Dawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun, Joseph Lee, Aaron Chacko,\nBojian Hou, Duy Duong-Tran, Ying Ding, et al. 2024. DALK: Dynamic Co-\nAugmentation of LLMs and KG to answer Alzheimer’s Disease Questions with\nScientific Literature. In Findings of the Association for Computational Linguistics:\nEMNLP 2024. 2187–2205.\n[23] Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu,\nXingwei Qu, Yangguang Li, Wanli Ouyang, et al. 2024. GraphReader: Building\nGraph-based Agent to Enhance Long-Context Abilities of Large Language Models.\nIn Findings of the Association for Computational Linguistics: EMNLP 2024. 12758–\n12786.\n[24] Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King,\nand Jia Li. 2025. G-refer: Graph retrieval-augmented large language model for\nexplainable recommendation. In Proceedings of the ACM on Web Conference 2025.\n240–251.\n[25] Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin\nMao, and Jian Guo. 2024. Think-on-graph 2.0: Deep and faithful large language\nmodel reasoning with knowledge-guided retrieval augmented generation. arXiv\npreprint arXiv:2407.10805 (2024).\n[26] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and\nHannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating\nEffectiveness of Parametric and Non-Parametric Memories. In Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). 9802–9822.\n[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin neural information processing systems 35 (2022), 27730–27744.\n[28] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\n2024. Unifying large language models and knowledge graphs: A roadmap. IEEE\nTransactions on Knowledge and Data Engineering 36, 7 (2024), 3580–3599.\n[29] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan\nZhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: A survey.\narXiv preprint arXiv:2408.08921 (2024).\n[30] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,\nNicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard,\n\n\n--- Page 10 ---\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nYu et al.\net al. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In\nProceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies. 2523–2544.\n[31] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and\nChristopher D Manning. [n. d.]. Raptor: Recursive abstractive processing for\ntree-organized retrieval. In The Twelfth International Conference on Learning\nRepresentations.\n[32] Sheetal S Sonawane and Parag A Kulkarni. 2014. Graph based representation\nand analysis of text document: A survey of techniques. International Journal of\ncomputer applications 96, 19 (2014), 1–8.\n[33] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin\nZhao, Lei Fang, and Ji-Rong Wen. 2025. R1-searcher: Incentivizing the search\ncapability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592\n(2025).\n[34] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi\nYuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop\noverthinking: A survey on efficient reasoning for large language models. arXiv\npreprint arXiv:2503.16419 (2025).\n[35] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun\nGong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. [n. d.]. Think-on-Graph:\nDeep and Responsible Reasoning of Large Language Model on Knowledge Graph.\nIn The Twelfth International Conference on Learning Representations.\n[36] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Com-\nmonsenseQA: A Question Answering Challenge Targeting Commonsense Knowl-\nedge. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers). 4149–4158.\n[37] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen,\nCheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5:\nScaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 (2025).\n[38] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna\nGurevych. [n. d.]. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation\nof Information Retrieval Models. ([n. d.]).\n[39] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n2022. MuSiQue: Multihop Questions via Single-hop Question Composition.\nTransactions of the Association for Computational Linguistics (2022).\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[41] Jingjin Wang. 2025. PropRAG: Guiding Retrieval with Beam Search over Propo-\nsition Paths. arXiv preprint arXiv:2504.18070 (2025).\n[42] Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong,\nShikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, et al. 2021. Ernie 3.0\ntitan: Exploring larger-scale knowledge enhanced pre-training for language\nunderstanding and generation. arXiv preprint arXiv:2112.12731 (2021).\n[43] Yaozu WANG, Qing LI, Zhangjie DAI, and Yue XU. 2024. Current status and\ntrends in large language modeling research. Chinese Journal of Engineering 46, 8\n(2024), 1411–1425.\n[44] Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr.\n2024. Knowledge graph prompting for multi-document question answering. In\nProceedings of the AAAI conference on artificial intelligence, Vol. 38. 19206–19214.\n[45] Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong,\nXiao Huang, and Jinsong Su. 2025. When to use graphs in rag: A compre-\nhensive analysis for graph retrieval-augmented generation.\narXiv preprint\narXiv:2506.05690 (2025).\n[46] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai,\nJoey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm\nreasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768\n(2025).\n[47] Guanming Xiong, Haochen Li, and Wen Zhao. 2025.\nMCTS-KBQA: Monte\nCarlo Tree Search for Knowledge Base Question Answering. arXiv preprint\narXiv:2502.13428 (2025).\n[48] Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yuhe Ke, Wanxin\nLi, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, et al. 2024. KG-\nRank: Enhancing Large Language Models for Medical QA with Knowledge Graphs\nand Ranking Techniques. In Proceedings of the 23rd Workshop on Biomedical\nNatural Language Processing. 155–166.\n[49] Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel.\n2024. Do Large Language Models Latently Perform Multi-Hop Reasoning?. In\nProceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 10210–10229.\n[50] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan\nSalakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for\nDiverse, Explainable Multi-hop Question Answering. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\n[51] Jingying Zeng, Richard Huang, Waleed Malik, Langxuan Yin, Bojan Babic, Danny\nShacham, Xiao Yan, Jaewon Yang, and Qi He. 2024. Large language models\nfor social networks: Applications, challenges, and solutions. arXiv preprint\narXiv:2401.02575 (2024).\n[52] Jingyan Zhang, Dawei Feng, and Bo Ding. 2025. CoRAG: Enhancing Retrieval-\nAugmented Generation with Coreference Resolution. In 2025 5th International\nConference on Artificial Intelligence and Industrial Technology Applications (AIITA).\nIEEE, 2116–2123.\n[53] Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou,\nZijin Hong, Junnan Dong, Hao Chen, Yi Chang, and Xiao Huang. 2025. A survey\nof graph retrieval-augmented generation for customized large language models.\narXiv preprint arXiv:2501.13958 (2025).\n[54] Xishi Zhu, Xiaoming Guo, Shengting Cao, Shenglin Li, and Jiaqi Gong. 2024. Struc-\ntuGraphRAG: Structured document-informed knowledge graphs for retrieval-\naugmented generation. In Proceedings of the AAAI Symposium Series, Vol. 4.\n242–251.\nA\nMore Experiments\nA.1\nMore Details about Datasets\nWe adopt four widely used benchmark datasets, including Hot-\npotQA, MuSiQue, and 2Wiki from HippoRAG [20]1 and one simple\nQA dataset PopQA from PropRAG [41]2. Detailed dataset descrip-\ntions are as follows.\n• HotpotQA [50]: A Wikipedia-based dataset where answer-\ning each question requires reasoning over multiple supporting\ndocuments.\n• MuSiQue [39]: A challenging dataset consisting of questions\ncomposed of 2 to 4 single-hop questions, requiring multi-step\nreasoning.\n• 2Wiki [16]: Another Wikipedia-based dataset where each ques-\ntion requires information from two articles to be correctly an-\nswered.\n• PopQA [50]: A dataset of entity-centric QA pairs, where each\nquestion is generated by converting a knowledge tuple retrieved\nfrom Wikidata.\nThe statistics of these datasets are shown in Table 8.\nA.2\nExperiments with LLaMA-3\nIn this subsection, we investigate the effectiveness of GraphRAG-\nR1 across different base models. We adopt another representative\nopen-sourced LLM, Llama-3-8B, as the backbone LLM. The experi-\nmental setting is similar to using Qwen-2.5-7B, except we add PRA\nreward in the third training stage to encourage the model to invoke\nretrieval, which enhances performance. The overall results are pre-\nsented in Table 5 and the results of integrating with other retrievals\nare shown in Table 6. Collectively, these experiments demonstrate\nthat our method is effective and outperforms baselines when using\nLLaMA-3-8B as the backbone LLM, showing that our method is\ngeneralizable across different LLM models.\nA.3\nCost and Efficiency Analysis\nIn this subsection, we evaluate the performance and cost of different\nreasoning-enhanced retrieval methods. We compare our approach\nwith KGP and ToG, two reasoning-enhanced retrieval methods\nthat also support multiple retrievals. In addition to comparing the\nperformance measured by F1 scores, we also calculate the token\nconsumption throughout the retrieval and reasoning processes. As\nshown in Table 7, GraphRAG-R1 achieves significant improvements\nin both performance and computational cost by deeply integrating\n1https://github.com/OSU-NLP-Group/HippoRAG/releases/tag/v1.0.0\n2https://github.com/ReLink-Inc/PropRAG/tree/main/outputs/popqa\n\n\n--- Page 11 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 5: The results of different methods for multi-hop question answering using Llama-3-8B as the backbone LLM. The best\nresults are in bold and the second-best results are underlined.\nMethod\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nNaive RAG\n9.57\n27.00\n48.25\n1.17\n4.00\n40.11\n4.14\n10.00\n43.58\n8.14\n24.70\n46.59\nKGP\n3.61\n17.00\n46.41\n1.38\n3.50\n44.49\n8.23\n26.50\n48.48\n10.75\n59.00\n48.13\nToG\n8.97\n16.50\n49.83\n3.28\n10.00\n44.74\n7.91\n23.00\n48.36\n10.96\n19.40\n49.57\nLightRAG\n5.33\n12.50\n43.95\n1.83\n3.50\n41.51\n4.68\n12.00\n43.95\n6.71\n27.50\n45.83\nPropRAG\n12.47\n25.00\n49.03\n3.83\n6.50\n42.17\n5.06\n12.50\n43.06\n9.76\n27.50\n48.10\nG-retriever\n1.91\n23.00\n43.09\n1.06\n12.50\n42.77\n2.43\n18.00\n42.26\n0.25\n38.10\n39.32\nHippoRAG2\n6.89\n22.00\n46.06\n1.85\n4.00\n42.03\n5.61\n12.00\n43.51\n7.85\n26.10\n46.57\nGraphRAG-R1\n13.16\n26.50\n53.06\n6.32\n16.00\n48.96\n13.36\n25.00\n51.46\n11.64\n41.50\n50.42\nTable 6: The results of integrating GraphRAG-R1 using Llama-3-8B with other GraphRAG retrieval methods. Imp denotes the\naverage improvement across all datasets.\nMethod\nModel\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nImp\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nF1\nACCL\nSBERT\nNaive RAG\nOriginal\n9.57\n27.00\n48.25\n1.17\n4.00\n40.11\n4.14\n10.00\n43.58\n8.14\n24.70\n46.59\n+GraphRAG-R1 10.84\n26.00\n50.66\n4.62\n4.00\n45.93\n10.92\n14.00\n47.96\n9.06\n22.70\n48.39\n+10.41%\nKGP\nOriginal\n3.61\n17.00\n46.41\n1.38\n3.50\n44.49\n8.23\n26.50\n48.48\n10.75\n59.00\n48.13\n+GraphRAG-R1 10.13\n15.00\n50.94\n4.26\n4.50\n47.15\n16.55\n22.00\n52.83\n8.41\n24.50\n46.56\n-4.61%\nToG\nOriginal\n8.97\n16.50\n49.83\n3.28\n10.00\n44.74\n7.91\n23.00\n48.36\n10.96\n19.40\n49.57\n+GraphRAG-R1 11.73\n14.50\n52.03\n5.05\n5.00\n47.62\n15.96\n18.50\n54.25\n8.13\n14.50\n48.93\n+1.26%\nLightRAG\nOriginal\n5.33\n12.50\n43.95\n1.83\n3.50\n41.51\n4.68\n12.00\n43.95\n6.71\n27.50\n45.83\n+GraphRAG-R1\n7.44\n12.50\n47.20\n4.58\n5.00\n45.76\n11.96\n16.00\n48.64\n4.79\n14.50\n45.01\n+5.65%\nPropRAG\nOriginal\n12.47\n25.00\n49.03\n3.83\n6.50\n42.17\n5.06\n12.50\n43.06\n9.76\n27.50\n48.10\n+GraphRAG-R1\n10.15\n18.50\n49.14\n5.31\n9.00\n46.24\n8.22\n14.50\n46.40\n8.31\n21.70\n47.79\n+0.01%\nG-retriever\nOriginal\n1.91\n23.00\n43.09\n1.06\n12.50\n42.77\n2.43\n18.00\n42.26\n0.25\n38.10\n39.32\n+GraphRAG-R1 24.28\n26.50\n60.42\n7.71\n5.00\n51.44\n18.35\n21.00\n54.87\n8.30\n14.00\n49.55\n+28.99%\nHippoRAG2 Original\n6.89\n22.00\n46.06\n1.85\n4.00\n42.03\n5.61\n12.00\n43.51\n7.85\n26.10\n46.57\n+GraphRAG-R1 13.16\n26.50\n53.06\n6.32\n16.00\n48.96\n13.36\n25.00\n51.46\n11.64\n41.50\n50.42\n+35.13%\nTable 7: Performance and cost comparison with KGP and ToG, two reasoning-enhanced GraphRAG methods\nMethod\nHotpotQA\nMuSiQue\n2Wiki\nPopQA\nF1\n#Token\nF1\n#Token\nF1\n#Token\nF1\n#Token\nKGP\n10.73\n2687.44\n4.61\n2858.23\n10.16\n2173.02\n21.01\n3182.02\nToG\n11.44\n3136.61\n5.02\n2873.56\n14.45\n2065.94\n29.21\n2838.49\nGraphRAG-R1\n38.00\n1674.47\n20.06\n2086.10\n32.24\n1883.21\n35.04\n1449.80\nthe intrinsic reasoning capabilities of LLMs. Compared to baseline\nmethods that also leverage LLM reasoning, our method demon-\nstrates substantially improved answer quality while requiring lower\ntoken overhead than the other two methods. This validates the ef-\nfectiveness and cost efficiency of our approach.\nA.4\nSystem Prompt in GraphRAG-R1\nIn this subsection we shown the detailed prompt of our model in\nFigure 4. This prompt is used to help the model gradually develop\nstructured reasoning abilities through reinforcement learning. It\nencourages breaking down complex questions into subproblems\nand identifying what information is needed at each step. Through\n\n\n--- Page 12 ---\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nYu et al.\nAlgorithm 1 GraphRAG-R1 Training Pipeline\nRequire: Base LLM policy 𝜋ref, Training Dataset 𝐷train, Exter-\nnal Retriever 𝑅, External LLM LLMgen, Hyper-parameters:\n𝛽, 𝑅0,𝑘,𝑎,𝑏\nEnsure: Optimized LLM policy 𝜋𝜃\n1: // Stage 1: Cold Start SFT (Format Following)\n2: // Generate dataset 𝐷SFT with desired format:\n3: for each (𝑞, docs) ∈𝐷train do\n4:\nConstruct prompt with desired format (Fig 5)\n5:\nGenerate 𝑜SFT with desired format using LLMgen\n6:\n𝐷SFT ←𝐷SFT ∪{(𝑞,𝑜SFT)}\n7: end for\n8: Train 𝜋𝜃with 𝐷SFT via supervised fine-tuning\n9: // Stage 2: Behavior Shaping (PRA + Format Reward)\n10: for each training epoch do\n11:\nfor each 𝑞∈𝐷train do\n12:\nSampleRollouts(𝜋𝜃,𝑞, 𝑅,𝐺)\n13:\nCalculate rewards using Eqs. (4)(5)\n14:\nUpdate policy via GRPO using Eqs. (2)(3)\n15:\nend for\n16: end for\n17: // Stage 3: Smartness Optimization (CAF + Format Re-\nward)\n18: for each training epoch do\n19:\nfor each 𝑞∈𝐷train do\n20:\nSampleRollouts(𝜋𝜃,𝑞, 𝑅,𝐺)\n21:\nCalculate rewards using Eqs. (4)(7)\n22:\nUpdate policy via GRPO Eqs. (2)(3)\n23:\nend for\n24: end for\n25: return 𝜋𝜃\n26: // Function of Rollout with Thinking\n27: function SampleRollouts(𝜋𝜃,𝑞, 𝑅,𝐺)\n28:\nfor 𝑗= 1 to 𝐺do\n29:\n𝑜𝑗←[ ]\n30:\nwhile not terminated do\n31:\nif model outputs ⟨|begin_of_query|⟩then\n32:\nquery ←DecodeQuery(𝑜𝑗)\n33:\ndocs ←𝑅(query)\n34:\nAppend ⟨|begin_of_documents|⟩and docs to 𝑜𝑗\n35:\nend if\n36:\n𝑜𝑗←𝑜𝑗⊕𝜋𝜃(next_token|𝑞,𝑜𝑗) // ⊕: concatenation\n37:\nend while\n38:\nend for\n39:\nreturn {𝑜1, . . . ,𝑜𝐺}\n40: end function\nTable 8: Dataset Statistics\nMetric\nMuSiQue 2Wiki HotpotQA PopQA\nGraph\n# Passages\n11,656\n6,119\n9,221\n8,676\n# Nodes\n91,729\n42,694\n82,157\n74,472\n# Triplets\n107,448\n50,671\n98,709\n139,273\nQuestions\nTotal\n1,000\n1,000\n1,000\n1,000\nTraining Set\n800\n800\n800\n0\nTesting Set\n200\n200\n200\n1,000\nYou are a helpful assistant. Given a question, you should answer \nit by first thinking about the reasoning process in the mind and \nthen providing the final answer.\nThe output format of reasoning process is enclosed within \n<think> </think>, and final answer is enclosed within \n<answer> </answer> tags, respectively, i.e., \"<think> \nreasoning process here </think><answer> final answer here \n</answer>\".\nDuring the thinking process, **you can perform searching for \nuncertain knowledge** if necessary with the format of \n\"<|begin_of_query|> search query <|end_of_query|>\". \nThen, the search system will provide you with the retrieval \ninformation with the format of \n\"<|begin_of_documents|> ...search results... \n<|end_of_documents|>\".\nSystem Prompt in GraphRAG-R1\nFigure 4: System Prompt in GraphRAG-R1.\nthis design, the model learns to combine retrieval calls with internal\nreasoning abilities.\nA.5\nSystem Prompt for Generating Data with\nDesired Format\nIn this subsection, we introduce the prompt for generating data\nwhich is used in the first training stage to address the cold-start\nproblem. Specifically, we utilize ERNIE-3.5-128K [42] model. The\nprompt design is shown in Figure 5. This structured prompt enables\nbatch generation of training data. For each instance, we modify\nonly the question and reference documents within the fixed tem-\nplate while retaining consistent few-shot examples. These examples\nexplicitly define the required output format and logical structure.\nFollowing generation, we perform rigorous post-processing: val-\nidating output labels, correcting inconsistencies, and conducting\nquality assessment (e.g., relevance scoring).\nB\nDetails of GraphRAG-R1\nB.1\nAlgorithm Details\nIn this section, we provide the pseudo-code of our training pipeline\nin Algorithm 1.\n\n\n--- Page 13 ---\n\nGraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nInstruction:\nI need you to help me generate some data to train models for SFT. This data is about a thinking and retrieval paradigm for multi-hop questions, \naiming to enhance RAG's effectiveness on multi-hop questions. The most important thing is to generate this way of solving problems. Next, I will \ngive an example. You need to use the question I give you, as well as multiple reference documents to complete a progressive thinking process \ndemonstration.\nInput:\nQuestion:\nReference document 1: \nReference document 2: \nOne-Shot Demonstration: \nThe following is the content you need to generate (because it's for generating training data, the format requirements are quite strict, please follow \nthis example exactly):\nYou are an advanced assistant who can analyze and break down complex questions to make multiple calls to information retrieval to \nultimately solve problems. <|User|>Who is the father of the scientist that discovered the theory of relativity? \n<|Assistant|><think>User asks who is the father of the scientist that discovered the theory of relativity. I need to break this down into \nsteps. First, I need to identify which scientist discovered the theory of relativity. Then I can find information about this scientist's father. \n<|begin_of_query|>Who discovered the theory of relativity? <|end_of_query|><begin_of_documents>Albert Einstein \ndeveloped the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). \n<|end_of_documents|>Now I know that Albert Einstein discovered the theory of relativity. Next, I need to find information about his \nfather. <|begin_of_query|>Who was Albert Einstein's father?<|end_of_query|><|begin_of_documents|>Hermann Einstein \n(30 August 1847 – 10 October 1902) was the father of Albert Einstein. He was an engineer and \nsalesman.<|end_of_documents|></think><answer>The father of the scientist who discovered the theory of relativity was Hermann \nEinstein. Albert Einstein developed the theory of relativity, and Hermann Einstein was Albert Einstein's father.</answer>\nThis is a complete example, where the reasoning process and the retrieved content process are both wrapped with tags in this example. Please \nimitate this example to generate the reasoning process based on the question and reference documents I gave you.\nSystem prompt for generating data with desired format\nFigure 5: System prompt for generating data with desired format used in the first training stage."
  }
}